,filename,cell_index,cell_type,slide_type,n_lines,cell_text
0,10_transformers.ipynb,0,code,-,1,from torch import nn
1,10_transformers.ipynb,1,markdown,slide,1,<center><h1>Transformers</h1></center>
2,10_transformers.ipynb,2,markdown,slide,6,"<center>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/gpt-3.png"">		<a href=""https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd"">link</a>		</center>"
3,10_transformers.ipynb,3,markdown,slide,8,# Outline	1. Sequence-to-Sequence (seq2seq) learning. Encoder-decoder architecture.	1. Motivation of Transformer.	1. Attention	1. Transformer block	1. Encoder	1. Decoder	1. BERT
4,10_transformers.ipynb,4,markdown,slide,1,# seq2seq. Encoder-decoder architecture
5,10_transformers.ipynb,5,markdown,slide,5,"<center>	RNN for classification	<br>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-classification.png"">	</center>"
6,10_transformers.ipynb,6,markdown,slide,5,"<center>	RNN for language modeling	<br>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-lm.png"">	</center>"
7,10_transformers.ipynb,7,markdown,slide,8,"In the demo [language model with RNN](https://colab.research.google.com/drive/1gg6RZqjz6d-1o6pguZaktwHUVAFyScoX#scrollTo=YCjdUfGrsqWr&line=11&uniqifier=1) we trained a <font color=""red"">conditioned</font> RNN:		$$	h_t = f(h_{t-1}, x_t, \color{red}{c})	$$	$$	y_t = g(h_t)	$$"
8,10_transformers.ipynb,8,markdown,slide,5,We need conditioning in many applications:	- Machine translation (English $\rightarrow$ Spanish)	- Text summarization (long text $\rightarrow$ short text)	- Dialogue / Chatbots / Question answering	- ...
9,10_transformers.ipynb,9,markdown,slide,5,**Idea**:	1. _Learn_ the condition from the first sequence (many-to-one RNN).	1. Use the learned condition to generate the second sequence (many-to-many language model RNN).		> This is called sequence-to-sequence (seq2seq) learning.
10,10_transformers.ipynb,10,markdown,slide,5,"<center>	RNN for seq2seq	<br>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-seq2seq.png"">	</center>"
11,10_transformers.ipynb,11,markdown,slide,7,"Definitions:	- The part that _learns_ the condition is called **encoder**.	- The part that generates the result based on the condition is called **decoder**.		**Intuition**:	- Encoder learns (""encodes"") the representation of the input sequence	- Decoder reconstructs (""decodes"") the text from the learned representation."
12,10_transformers.ipynb,12,markdown,slide,5,# Motivation of Transformer		**Problem**: RNN is sequential (= slow).		**Q**: But _why_ do we use it?
13,10_transformers.ipynb,13,markdown,slide,1,"**A**: RNN accepts texts of any length and should learn _long-term dependencies_ between words (at least, in theory)."
14,10_transformers.ipynb,14,markdown,slide,41,"This is how dependencies may look like:	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/word-dependencies.png"" style=""width:70%;"">	<table>	<tbody>	<tr>	<td></td>	<td><b>This</b></td>	<td><b>is</b></td>	<td><b>a</b></td>	<td><b>sentence</b></td>	</tr>	<tr>	<td><b>This</b></td>	<td>DET</td>	<td></td>	<td></td>	<td></td>	</tr>	<tr>	<td><b>is</b></td>	<td>nsubj</td>	<td>AUX</td>	<td></td>	<td>attr</td>	</tr>	<tr>	<td><b>a</b></td>	<td></td>	<td></td>	<td>DET</td>	<td></td>	</tr>	<tr>	<td><b>sentence</b></td>	<td></td>	<td></td>	<td>det</td>	<td>NOUN</td>	</tr>	</tbody>	</table>"
15,10_transformers.ipynb,15,markdown,slide,1,# Attention
16,10_transformers.ipynb,16,markdown,slide,7,"RNNs produce **word vectors** based on **dependencies** between words.		**Idea**: represent words as vectors and measure dependency using dot product.		**Our goal**: represent the word $q$ given the sequence $k_1, k_2, \dots$		> $q$ does not necessarily belong to the sequence!"
17,10_transformers.ipynb,17,markdown,slide,4,**Definitions**:	- $k_i$ is a **key** vector of size $d_k$.	- Each key $k_i$ has the associated **value** - vector $v_i$ of size $d_v$.	- $q$ is a **query** vector of size $\color{red}{d_k}$.
18,10_transformers.ipynb,18,markdown,slide,4,- Dependency between $q$ and $k_i$ is quantified by a dot product:	$$	\mathrm{softmax}(q \cdot k_i) = \dfrac{\exp(q \cdot k_i)}{\sum_j \exp(q \cdot k_j)}	$$
19,10_transformers.ipynb,19,markdown,slide,4,"- The vector for $q$ is the average value vector:	$$	A(q, k_1, v_1, k_2, v_2, \dots) = \sum_i v_i \cdot \mathrm{softmax}(q \cdot k_i)	$$"
20,10_transformers.ipynb,20,markdown,slide,4,"Put all keys into matrix $K$ and all values into matrix $V$:	$$	A(q, K, V) = \sum_i v_i \cdot \mathrm{softmax}(q \cdot k_i)	$$"
21,10_transformers.ipynb,21,markdown,slide,4,"If we have many queries, put them into matrix $Q$:	$$	A(Q, K, V) = \mathrm{softmax}(Q K^T) V	$$"
22,10_transformers.ipynb,22,markdown,slide,6,"**Problem**: for large $d_k$, dot products are large and softmax is ""peaked"".		**Solution**: normalize the dot product:	$$	A(Q, K, V) = \mathrm{softmax}\left(\dfrac{Q K^T}{\color{red}{\sqrt{d_k}}}\right) V	$$"
23,10_transformers.ipynb,23,markdown,slide,3,"<center>	<img src=""http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png"">	</center>"
24,10_transformers.ipynb,24,markdown,slide,1,"The function $A(Q, K, V)$ is called **Dot-Product Attention** function."
25,10_transformers.ipynb,25,markdown,slide,5,In the **encoder**: $Q = K = V$.		> In other words: the word vectors themselves select each other		We’ll see in the **decoder** why we separate them in the definition.
26,10_transformers.ipynb,26,markdown,slide,3,**Problem**: only one way for words to interact with one-another.		**Solution**: _multi-head_ attention
27,10_transformers.ipynb,27,markdown,slide,5,"- First map $Q, K, V$ into $h=8$ lower dimensional spaces via linear layers.	- Then apply attention.	- Then concatenate outputs and pipe through linear layer.		<img src=""http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png"">"
28,10_transformers.ipynb,28,markdown,slide,1,# Transformer block
29,10_transformers.ipynb,29,markdown,slide,3,**Transformer block** is a layer that has two “sublayers”	1. Multihead attention	2. 2-layer feed-forward neural network (with ReLU)
30,10_transformers.ipynb,30,markdown,slide,3,"<center>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/transformer-block.png"">	</center>"
31,10_transformers.ipynb,31,markdown,slide,6,	Techniques to speed up training:	- Residual connection	- Dropout	- Layer normalization	
32,10_transformers.ipynb,32,markdown,slide,1,## Residual connection
33,10_transformers.ipynb,33,markdown,slide,6,"For a layer $\mathcal{F}(\mathbf{x})$, the _residual connection_ is $\mathcal{F}(\mathbf{x}) + \mathbf{x}$.		<br>	<center>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/residual.JPEG"" style=""width:50%"">	</center>"
34,10_transformers.ipynb,34,markdown,slide,4,**Idea**: it helps to propagate the gradient	$$	\dfrac{\partial (\mathcal{F}(\mathbf{x}) + \mathbf{x})}{\partial \mathbf{x}} = \dfrac{\partial \mathcal{F}(\mathbf{x})}{\partial \mathbf{x}} \color{red}{+ 1}	$$
35,10_transformers.ipynb,35,markdown,slide,4,"## Dropout	Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training.		![dropout](https://raw.githubusercontent.com/horoshenkih/harbour-space-ds210/master/pic/dropout.png)"
36,10_transformers.ipynb,36,markdown,slide,3,**Intuition**: voting over $2^N$ thinned networks with shared weights		![thinned networks](https://raw.githubusercontent.com/horoshenkih/harbour-space-ds210/master/pic/thinned-networks.png)
37,10_transformers.ipynb,37,markdown,slide,1,## Layer normalization
38,10_transformers.ipynb,38,markdown,slide,2,"	Layer normalization changes input to have mean 0 and variance 1, per layer and per training point (and adds two more parameters)"
39,10_transformers.ipynb,39,code,slide,12,"class LayerNorm(nn.Module):	""Construct a layernorm module.""	def __init__(self, features, eps=1e-6):	super(LayerNorm, self).__init__()	self.a_2 = nn.Parameter(torch.ones(features))	self.b_2 = nn.Parameter(torch.zeros(features))	self.eps = eps		def forward(self, x):	mean = x.mean(-1, keepdim=True)	std = x.std(-1, keepdim=True)	return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
40,10_transformers.ipynb,40,markdown,slide,4,"	From the [original paper](https://arxiv.org/pdf/1607.06450.pdf):		> Layer normalization is very effective at **stabilizing** the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially **reduce the training time** compared with previously published techniques."
41,10_transformers.ipynb,41,markdown,slide,3,Back to the **transformer block**: 2 sublayers	1. Multihead attention	2. 2-layer feed-forward neural network (with ReLU)
42,10_transformers.ipynb,42,markdown,slide,4,"To speed up training, the actual output of each $\mathrm{Sublayer}(x)$ is	$$	\mathrm{LayerNorm}(x + \mathrm{Dropout}(\mathrm{Sublayer}(x)))	$$"
43,10_transformers.ipynb,43,code,slide,13,"class SublayerConnection(nn.Module):	""""""	A residual connection followed by a layer norm.	Note for code simplicity the norm is first as opposed to last.	""""""	def __init__(self, size, dropout):	super(SublayerConnection, self).__init__()	self.norm = LayerNorm(size)	self.dropout = nn.Dropout(dropout)		def forward(self, x, sublayer):	""Apply residual connection to any sublayer with the same size.""	return x + self.dropout(sublayer(self.norm(x)))"
44,10_transformers.ipynb,44,markdown,slide,1,# Encoder
45,10_transformers.ipynb,45,markdown,slide,1,## Input: byte-pair encoding
46,10_transformers.ipynb,46,markdown,slide,4,"**Problem**: represent rare words (like ""athazagoraphobia"").		**Solution**: use subword units.	> In FastText, $n$-grams are used."
47,10_transformers.ipynb,47,markdown,slide,5,"**Q**: which $n$ shall we use?		**A**: don't fix $n$, extract frequent subwords instead.		> This is inspired by the data compression technique called Byte Pair Encoding."
48,10_transformers.ipynb,48,markdown,slide,1,![](https://miro.medium.com/max/1400/1*x1Y_n3sXGygUPSdfXTm9pQ.gif)
49,10_transformers.ipynb,49,markdown,slide,3,"In Tex Mining, BPE is slightly modified in its implementation: the frequently occurring subword pairs are **merged together instead of being replaced** by another byte to enable compression.		> This would basically lead the rare word `athazagoraphobia` to be split up into more frequent subwords such as `['▁ath', 'az', 'agor', 'aphobia']`."
50,10_transformers.ipynb,50,markdown,slide,5,"1. Initialize vocabulary.	2. Represent each word in the corpus as a combination of the characters along with the special end of word token `</w>`.	2. Iteratively count character pairs in all tokens of the vocabulary.	4. Merge every occurrence of the most frequent pair, add the new character n-gram to the vocabulary.	5. Repeat step 4 until the desired number of merge operations are completed or the desired vocabulary size is achieved (which is a hyperparameter)."
51,10_transformers.ipynb,51,markdown,slide,1,## Input: positional encoding
52,10_transformers.ipynb,52,markdown,slide,1,"**Problem**: we don't want the model to be sequential, but word order is important."
53,10_transformers.ipynb,53,markdown,slide,1,**Solution**: represent the _position_ as a vector.
54,10_transformers.ipynb,54,markdown,slide,15,"The exact formula:	$$	PE(pos) = \begin{pmatrix}	\cos\left(\dfrac{pos}{1}\right) \\	\sin\left(\dfrac{pos}{1}\right) \\	\cos\left(\dfrac{pos}{10000^{\frac{2}{d}}}\right) \\	\sin\left(\dfrac{pos}{10000^{\frac{2}{d}}}\right) \\	\dots \\	\cos\left(\dfrac{pos}{10000}\right) \\	\sin\left(\dfrac{pos}{10000}\right) \\	\end{pmatrix}	$$		From the [original paper](https://arxiv.org/pdf/1706.03762.pdf):	> it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE(pos+k)$ can be represented as a linear function of $PE(pos)$."
55,10_transformers.ipynb,55,markdown,slide,5,"## Complete encoder	<center>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/transformer-encoder.png"">	</center>	Blocks are repeated 6 times (in vertical stack)."
56,10_transformers.ipynb,56,markdown,slide,1,![](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/attention-1.png)
57,10_transformers.ipynb,57,markdown,slide,1,![](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/attention-2.png)
58,10_transformers.ipynb,58,markdown,slide,1,# Decoder
59,10_transformers.ipynb,59,markdown,slide,5,"## Masked self-attention	$$	A(q, k_1, v_1, k_2, v_2, \dots) = \sum_{i \color{red}{ < t}} v_i \cdot \mathrm{softmax}(q \cdot k_i)	$$	where $q = k_t$ for some $t$."
60,10_transformers.ipynb,60,markdown,slide,4,"Why do we need masking?	- Suppose $k_t$ attends to some future word $k_{t + \delta}$.	- The word $k_{t + \delta}$ may attend the word $k_t$ **in one of the previous transformer blocks**.	- So $k_t$ may effectively ""see itself""."
61,10_transformers.ipynb,61,markdown,slide,3,## Encoder-Decoder Attention	- queries come from previous decoder layer	- keys and values come from output of encoder
62,10_transformers.ipynb,62,markdown,slide,3,"<center>	<img src=""http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png"">	</center>"
63,10_transformers.ipynb,63,markdown,slide,1,# BERT (Bidirectional Encoder Representations from Transformers)
64,10_transformers.ipynb,64,markdown,slide,1,"**Problem**: Transformer uses _masking_ so that words cannot ""see themselves"" - but _language understanding_ is bidirectional!"
65,10_transformers.ipynb,65,markdown,slide,3,"Remember this example?	1. ""Students opened their ___ as the proctor started the clock.""	1. ""Students opened their ___ and started coding."""
66,10_transformers.ipynb,66,markdown,slide,2,"**Solution**: Mask out 15% of the input words, and then predict the masked words.	> the man went to the [MASK] to buy a [MASK] of milk"
67,10_transformers.ipynb,67,markdown,slide,3,"<center>	<img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/elmo-gpt-bert.png"">	</center>"
68,10_transformers.ipynb,68,markdown,slide,1,# Demo: [transformer notebooks](https://huggingface.co/transformers/notebooks.html)
69,10_transformers.ipynb,69,markdown,slide,13,"# Summary	1. Sequence-to-Sequence (seq2seq): learn the condition with _encoder_, then generate text with conditioning with _decoder.	1. Motivation of Transformer: learn long-range dependencies faster.	1. Attention: measure dependency between words using dot product of vectors.	1. Transformer block:	- multi-head attention + FFNN	- residual connections, dropout, layer normalization	1. Encoder	- input: BPE, positional encoding	1. Decoder	- masked self-attention, encoder-decoder attention	1. BERT	- bidirectional model with masking"
70,10_transformers.ipynb,70,markdown,slide,7,"# Recommended resources	- [CS224n Lecture 13: Contextual Word Representations	and Pretraining](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture13-contextual-representations.pdf)	- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)	- [Transformer Notebooks](https://huggingface.co/transformers/notebooks.html)	- [Byte Pair Encoding — The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)	- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
71,11_news_aggregator.ipynb,0,code,-,19,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.utils import (	display_cv_results,	display_token_importance,	)		import numpy as np	from collections import Counter	from math import exp	from tabulate import tabulate	from tqdm.notebook import tqdm	from IPython.display import HTML, display	!pip install fasttext"
72,11_news_aggregator.ipynb,1,markdown,slide,1,<center><h1>Case study: Telegram News Aggregator </h1></center>
73,11_news_aggregator.ipynb,2,markdown,slide,15,# So far	Concepts:	1. TF-IDF	1. Language Models	1. Text Classification	1. Text Clustering	1. Topic Modeling	1. Word Vectors		Algorithms:	1. $n$-gram language models	1. Logistic Regression	1. $k$-means	1. LDA	1. Neural Networks
74,11_news_aggregator.ipynb,3,markdown,slide,9,# Telegram News Aggregator Contest		1. Isolate articles in English and Russian.	1. Isolate news articles.	1. Group news articles by category.	1. Group similar news into threads.	1. Sort threads by their relative importance.		[link](https://contest.com/docs/data_clustering)
75,11_news_aggregator.ipynb,4,markdown,slide,5,"## 1. Isolate articles in English and Russian.		Your algorithm must sort articles by language, filtering English and Russian articles. Articles in other languages are not relevant for this stage of the contest and may be discarded.		**Q: how to do it?**"
76,11_news_aggregator.ipynb,5,markdown,slide,5,> Text Mining problem: Language Identification.		Language Identification can be solved using _subword information_: for often different combinations of tokens occur:	- $n$-gram or RNN language model for characters	- Pre-trained FastText model for language isentification: [link](https://fasttext.cc/docs/en/language-identification.html)
77,11_news_aggregator.ipynb,6,markdown,slide,4,## 2. Isolate news articles.	Your algorithm must discard everything except for news articles.		**Q: how to do it?**
78,11_news_aggregator.ipynb,7,markdown,slide,1,> Text Mining problem: Classification.
79,11_news_aggregator.ipynb,8,markdown,slide,13,"## 3. Group news articles by category.	Your algorithm must place news articles into the following 7 categories:		1. Society (includes Politics, Elections, Legislation, Incidents, Crime)	1. Economy (includes Markets, Finance, Business)	1. Technology (includes Gadgets, Auto, Apps, Internet services)	1. Sports (includes E-Sports)	1. Entertainment (includes Movies, Music, Games, Books, Arts)	1. Science (includes Health, Biology, Physics, Genetics)	1. Other (news articles that don't fall into any of the above categories)		**Q: how to do it?**	"
80,11_news_aggregator.ipynb,9,markdown,slide,3,"> Text Mining problem: Classification.		Instead of 2 classification problems, we can add one more category that corresponds to articles which are not news articles."
81,11_news_aggregator.ipynb,10,markdown,slide,4,"## 4. Group similar news into threads.	Your algorithm must identify news articles about the same event and group them together into threads, selecting a relevant title for each thread. News articles inside each thread must be sorted according to their relevance (most relevant at the top).		**Q: how to do it?**"
82,11_news_aggregator.ipynb,11,markdown,slide,7,> Text Mining problem: Clustering.		Subproblems:	1. Texts to vectors	1. Vectors to clusters	1. Relevant title	1. Ranking
83,11_news_aggregator.ipynb,12,markdown,slide,4,"## 5. Sort threads by their relative importance.	Your algorithm must sort news threads in each of the categories based on perceived importance (important at the top). In addition, the algorithm must build a global list of threads, indepedent of category, sorted by perceived importance (important at the top).		**Q: how to do it?**"
84,11_news_aggregator.ipynb,13,markdown,slide,6,"> Not exactly a Text Mining problem, can be solved using Machine Learning (ranking).		Features:	- Number of documents in a thread.	- ""Freshness"" of documents if a thread.	- ""Authority"" of sources ([PageRank](https://en.wikipedia.org/wiki/PageRank))."
85,11_news_aggregator.ipynb,14,markdown,slide,1,# Colab demo: Get the data
86,11_news_aggregator.ipynb,15,code,-,4,# download one sample of data	!wget https://data-static.usercontent.dev/DataClusteringSample0107.tar.gz	!mkdir -p DataClustering	!tar -xvf DataClusteringSample0107.tar.gz -C DataClustering
87,11_news_aggregator.ipynb,16,code,-,5,"# get list of files using glob	import glob	from IPython.display import HTML, display	files = list(sorted(glob.glob(""DataClustering/*/*/*.html"")))	len(files)"
88,11_news_aggregator.ipynb,17,code,-,3,# data contains html files	with open(files[359]) as f:	print(f.read())
89,11_news_aggregator.ipynb,18,code,-,3,# there are samples in different languages	with open(files[10003]) as f:	display(HTML(f.read()))
90,11_news_aggregator.ipynb,19,code,-,3,# some of the samples are news	with open(files[359]) as f:	display(HTML(f.read()))
91,11_news_aggregator.ipynb,20,code,-,3,# some samples are not news	with open(files[24114]) as f:	display(HTML(f.read()))
92,11_news_aggregator.ipynb,21,markdown,-,1,Parsing
93,11_news_aggregator.ipynb,22,code,-,28,"from dateutil.parser import parse	from collections import namedtuple		# store structured information about the document	Sample = namedtuple(""Sample"", ""text title published_time short_text"")		def parse_html(html):	from bs4 import BeautifulSoup	soup = BeautifulSoup(html, 'html.parser')	title_tag = soup.find(""meta"", property=""og:title"")	if title_tag:	title = title_tag[""content""].strip()	else:	title = """"	description_tag = soup.find(""meta"", property=""og:description"")	if description_tag:	description = description_tag[""content""].strip()	else:	description = """"	h1 = [_.text.strip() for _ in soup.find_all('h1')]	h2 = [_.text.strip() for _ in soup.find_all('h2')]	paragraphs = [_.text.strip() for _ in soup.find_all('p')]		text = ""\t"".join([title, description] + h1 + h2 + paragraphs).replace(""\n"", "" "")	short_text = ""\t"".join([title, description])	published_time_tag = soup.find(""meta"", property=""article:published_time"")	published_time = parse(published_time_tag[""content""].strip())	return Sample(text=text, title=title, published_time=published_time, short_text=short_text)"
94,11_news_aggregator.ipynb,23,markdown,-,1,Look at the parsed data
95,11_news_aggregator.ipynb,24,code,-,7,from pprint import pprint	with open(files[100]) as f:	sample = parse_html(f.read())	print(sample.published_time)	print(sample.title)	print(sample.short_text)	print(sample.text)
96,11_news_aggregator.ipynb,25,code,-,7,# parse all	from tqdm.notebook import tqdm		samples = []	for p in tqdm(files):	with open(p) as f:	samples.append(parse_html(f.read()))
97,11_news_aggregator.ipynb,26,markdown,slide,3,# Colab demo: Language identification		We will isolate only English articles.
98,11_news_aggregator.ipynb,27,code,-,2,# get FastText model for language isentification	!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz
99,11_news_aggregator.ipynb,28,code,-,15,"from collections import Counter	from pprint import pprint	import fasttext		li_model = fasttext.load_model(""lid.176.ftz"")		samples_en = []	language_counter = Counter()	for sample in tqdm(samples):	text = sample.text	predicted_language = li_model.predict(text)[0][0][len(""__label__""):]	language_counter[predicted_language] += 1	if predicted_language == ""en"":	samples_en.append(sample)	pprint(language_counter, compact=True)"
100,11_news_aggregator.ipynb,29,markdown,slide,3,"# Classification		**Problem**: In the contest, data is provided without labels. How to train classification?"
101,11_news_aggregator.ipynb,30,markdown,slide,5,Possible solutions:	- Classify training data manually.	- Use crowdsourcing ([Yandex.Toloka](https://toloka.yandex.com) or [Amazon Mechanical Turk](https://www.mturk.com/)).	- Find similar labelled dataset.	- Use [Google Dataset Search](https://datasetsearch.research.google.com/)
102,11_news_aggregator.ipynb,31,markdown,slide,5,"## Colab demo: [BBC articles dataset](https://www.kaggle.com/yufengdev/bbc-fulltext-and-category)		**Advantages**: full texts.		**Disadvantages**: small dataset, not enough categories."
103,11_news_aggregator.ipynb,32,code,-,3,"import pandas as pd	df_bbc = pd.read_csv(""harbour-space-text-mining-course/datasets/bbc/bbc-text.csv"")	df_bbc.head()"
104,11_news_aggregator.ipynb,33,markdown,-,1,"The external dataset has categories different from the contest, so we need to find the mapping between them."
105,11_news_aggregator.ipynb,34,code,-,1,df_bbc.category.unique()
106,11_news_aggregator.ipynb,35,code,-,10,"df_bbc[""ContestCategory""] = df_bbc.category.map(	{	""tech"": ""Technology"",	""business"": ""Economy"",	""sport"": ""Sports"",	""entertainment"": ""Entertainment"",	""politics"": ""Society""	}	)	df_bbc.head()"
107,11_news_aggregator.ipynb,36,markdown,slide,5,"## Colab demo: [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)		**Advantages**: large dataset, many categories.		**Disadvantages**: no full articles, only short descriptions."
108,11_news_aggregator.ipynb,37,code,-,2,"df_news = pd.read_csv(""harbour-space-text-mining-course/datasets/news_category_dataset/category_headline_description.csv"")	df_news.head()"
109,11_news_aggregator.ipynb,38,code,-,9,"# get texts	def row2text(r):	import re	# the same format as for BBC: lowercase without punctuation	text = str(r[""headline""]) + "" "" + str(r[""short_description""])	return re.sub(r'[^\w\s]', '', text.lower())		df_news[""text""] = df_news.fillna(""."").apply(row2text, axis=1)#[""text""]	df_news.head()"
110,11_news_aggregator.ipynb,39,code,-,1,df_news.category.unique()
111,11_news_aggregator.ipynb,40,code,-,46,"# convert categories	categories_mapping = {	""CRIME"": ""Society"",	""ENTERTAINMENT"": ""Entertainment"",	""WORLD NEWS"": ""Society"",	""IMPACT"": ""NotNews"",	""POLITICS"": ""Society"",	""WEIRD NEWS"": ""Other"",	""BLACK VOICES"": ""Society"",	""WOMEN"": ""NotNews"",	""COMEDY"": ""Entertainment"",	""QUEER VOICES"": ""Society"",	""SPORTS"": ""Sports"",	""BUSINESS"": ""Economy"",	""TRAVEL"": ""NotNews"",	""MEDIA"": ""Society"",	""TECH"": ""Technology"",	""RELIGION"": ""Society"",	""SCIENCE"": ""Science"",	""LATINO VOICES"": ""Society"",	""EDUCATION"": ""Society"",	""COLLEGE"": ""Society"",	""PARENTS"": ""NotNews"",	""ARTS & CULTURE"": ""Society"",	""STYLE"": ""NotNews"",	""GREEN"": ""Society"",	""TASTE"": ""NotNews"",	""HEALTHY LIVING"": ""NotNews"",	""THE WORLDPOST"": ""Society"",	""GOOD NEWS"": ""Other"",	""WORLDPOST"": ""Society"",	""FIFTY"": ""NotNews"",	""ARTS"": ""NotNews"",	""WELLNESS"": ""NotNews"",	""PARENTING"": ""NotNews"",	""HOME & LIVING"": ""NotNews"",	""STYLE & BEAUTY"": ""NotNews"",	""DIVORCE"": ""NotNews"",	""WEDDINGS"": ""NotNews"",	""FOOD & DRINK"": ""NotNews"",	""MONEY"": ""NotNews"",	""ENVIRONMENT"": ""Other"",	""CULTURE & ARTS"": ""NotNews"",	}	df_news[""ContestCategory""] = df_news.category.map(categories_mapping)	df_news.head(10)"
112,11_news_aggregator.ipynb,41,markdown,slide,3,"## Combine datasets		As we see, datasets are different:"
113,11_news_aggregator.ipynb,42,code,slide,7,"def describe_dataset(df):	total_tokens = sum([len(s.split()) for s in df.text])	print(f""\tsamples: {df.shape[0]}, avg tokens: {total_tokens / df.shape[0]}, total tokens: {total_tokens}"")	print(""BBC:"")	describe_dataset(df_bbc)	print(""News Category Dataset"")	describe_dataset(df_news)"
114,11_news_aggregator.ipynb,43,markdown,slide,1,**Q: how to train a classifier with two datasets?**
115,11_news_aggregator.ipynb,44,markdown,slide,1,## Colab demo: Train the classifier
116,11_news_aggregator.ipynb,45,markdown,-,1,Try the following approach: assign larger weight to samples with full texts.
117,11_news_aggregator.ipynb,46,code,-,2,"df_bbc[""weight""] = 1.0	df_news[""weight""] = 0.1"
118,11_news_aggregator.ipynb,47,markdown,-,1,Combine and shuffle
119,11_news_aggregator.ipynb,48,code,-,9,"from sklearn.utils import shuffle	df = pd.concat(	[	df_bbc[[""text"", ""ContestCategory"", ""weight""]],	df_news[[""text"", ""ContestCategory"", ""weight""]]	]	)	df = shuffle(df)	df.head()"
120,11_news_aggregator.ipynb,49,markdown,-,1,Train the classifier
121,11_news_aggregator.ipynb,50,code,-,24,"from sklearn.linear_model import SGDClassifier	from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.pipeline import Pipeline	from sklearn.model_selection import GridSearchCV	from sklearn.preprocessing import LabelEncoder		le = LabelEncoder()	y = le.fit_transform(df.ContestCategory)		vec = TfidfVectorizer(stop_words=""english"")	clf = SGDClassifier(max_iter=50, loss=""log"", random_state=0)	pipeline = Pipeline([	(""vec"", vec),	(""clf"", clf),	])		param_grid = {	""clf__alpha"": [1e-6, 1e-7, 1e-8],	}		pipeline_grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, refit=True)	pipeline_grid_search.fit(df.text, y, clf__sample_weight=df.weight)		display_cv_results(pipeline_grid_search)"
122,11_news_aggregator.ipynb,51,markdown,-,1,Apply on the contest data
123,11_news_aggregator.ipynb,52,code,-,6,"texts = [s.text for s in samples_en]	predictions = le.inverse_transform(pipeline_grid_search.predict(texts))	for p, s in zip(predictions[:10], samples_en[:10]):	print(p)	print(s.title)	print(""---"")"
124,11_news_aggregator.ipynb,53,markdown,slide,7,"# Clustering		**Problems**:	1. In $k$-means algotithm, $k$ depends on the number of samples.	2. We cannot find centroids using one dataset and apply them on another dataset (news are time-dependent).	3. How do we understand if the number of clusters is too small?	4. How do we understand if the number of clusters is too large?"
125,11_news_aggregator.ipynb,54,markdown,slide,5,"## Agglomerative clustering		<center>	<img src=""https://upload.wikimedia.org/wikipedia/commons/a/ad/Hierarchical_clustering_simple_diagram.svg"">	</center>"
126,11_news_aggregator.ipynb,55,markdown,slide,3,"Complexity $O(n^2)$.	- OK for our dataset (clusterize each category)	- In general, can split the data: news are _time-dependent_, so split by 10000 with overlap 2000"
127,11_news_aggregator.ipynb,56,markdown,slide,1,**Q: How to choose the distance threshold?**
128,11_news_aggregator.ipynb,57,markdown,slide,2,- Measure the max in-cluster distance.	- Use the time of publishing! News should be close in time.
129,11_news_aggregator.ipynb,58,markdown,slide,1,## Colab demo: find clusters using agglomerative clustering
130,11_news_aggregator.ipynb,59,code,-,46,"def find_clusters(samples, vectorizer, distance_threshold=1.):	# vectors	print(""Find vectors"")	texts = [s.text for s in samples]	tfidf_vectors = vectorizer.transform(texts)	# sparse -> dense	from sklearn.decomposition import TruncatedSVD	tfidf_vectors_svd = TruncatedSVD(n_components=300).fit_transform(tfidf_vectors)		# clusters	print(""Find clusters"")	from sklearn.cluster import AgglomerativeClustering	clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold)	clusters = clustering.fit_predict(tfidf_vectors_svd)		from itertools import combinations	from scipy.spatial.distance import euclidean	from collections import defaultdict	import numpy as np		print(""Group into clusters"")	cluster2samples = defaultdict(list)	cluster2vectors = defaultdict(list)		for c, s, v in zip(clusters, samples, tfidf_vectors_svd):	cluster2samples[c].append(s)	cluster2vectors[c].append(v)		rv = []	for c in sorted(cluster2samples.keys()):	# the most relevant title is the closest to the average vector	# titles are ranked by the distance to the average vector	avg_vec = sum(cluster2vectors[c]) / len(cluster2vectors[c])	distances = [euclidean(avg_vec, v) for v in cluster2vectors[c]]	samples_order = np.argsort(distances)	best_title = cluster2samples[c][samples_order[0]].title	cluster_samples = [cluster2samples[c][i] for i in samples_order]	best_title = cluster2samples[c][0].title		cluster_published_times = [s.published_time for s in cluster2samples[c]]	# max difference between published time	published_time_diff = max(cluster_published_times) - min(cluster_published_times)		rv.append((cluster_samples, best_title, published_time_diff, avg_vec))		return rv"
131,11_news_aggregator.ipynb,60,code,-,2,"from sklearn.feature_extraction.text import TfidfVectorizer	tfidf_vectorizer = TfidfVectorizer(stop_words=""english"").fit([s.text for s in samples_en])"
132,11_news_aggregator.ipynb,61,code,-,6,"# select samples with ""Society"" category	samples_en_society = []	for s, p in zip(samples_en, predictions):	if p == ""Society"":	samples_en_society.append(s)	len(samples_en_society)"
133,11_news_aggregator.ipynb,62,code,-,1,"clusters_en_society = find_clusters(samples_en_society, tfidf_vectorizer, distance_threshold=1)"
134,11_news_aggregator.ipynb,63,code,-,1,"print(f""num samples: {len(samples_en_society)}, num clusters: {len(clusters_en_society)}"")"
135,11_news_aggregator.ipynb,64,code,-,7,"# time publishing difference distribution	import matplotlib.pyplot as plt	import numpy as np	time_publishing_difference_hours = [x[2].seconds/3600 for x in clusters_en_society]	print(f""avg time publishing difference: {np.mean(time_publishing_difference_hours)} hours"")	plt.hist(time_publishing_difference_hours, bins=100)	plt.show()"
136,11_news_aggregator.ipynb,65,markdown,-,1,Find largest clusters
137,11_news_aggregator.ipynb,66,code,-,6,"largest_cluster_idxs = list(sorted(	range(len(clusters_en_society)),	key=lambda i: len(set([_.title for _ in clusters_en_society[i][0]])),	reverse=True	))	largest_cluster_idxs[:5]"
138,11_news_aggregator.ipynb,67,markdown,-,1,Print the largest cluster
139,11_news_aggregator.ipynb,68,code,-,1,[s.title for s in clusters_en_society[largest_cluster_idxs[0]][0]]
140,11_news_aggregator.ipynb,69,markdown,-,1,Find similar clusters
141,11_news_aggregator.ipynb,70,code,-,16,"from sklearn.neighbors import NearestNeighbors	import numpy as np		cluster_vectors = np.vstack([x[3] for x in clusters_en_society])	nbrs = NearestNeighbors().fit(cluster_vectors)		from pprint import pprint	for cluster_idx in range(0, len(clusters_en_society), 1000):  # random clusters	# for cluster_idx in largest_cluster_idxs[:10]:  # largest clusters	distances, indices = nbrs.kneighbors(cluster_vectors[cluster_idx, :].reshape(1, -1))	nearest_cluster_idx = indices[0][1]	cluster_samples_titles = [s.title for s in clusters_en_society[cluster_idx][0]]	nearest_cluster_samples_titles = [s.title for s in clusters_en_society[nearest_cluster_idx][0]]	pprint(cluster_samples_titles)	pprint(nearest_cluster_samples_titles)	print(""-"" * 50)"
142,11_news_aggregator.ipynb,71,markdown,slide,5,	# Recommended resources	- https://contest.com/docs/data_clustering	- https://contest.com/docs/data_clustering2	- https://github.com/IlyaGusev/tgcontest
143,1_text_analysis_tools.ipynb,0,code,-,27,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.utils import (	calendar_table,	visualize_term_counter,	enable_mathjax_in_cell,	)	from tmcourse.ipyquiz import Quiz, Function	from tmcourse.quiz import (	quiz_count_tokens,	quiz_count_lemmas,	quiz_ner,	quiz_tfidf,	quiz_vectorizer_shape,	quiz_vector_distance,	)		from collections import Counter	from math import exp	from tabulate import tabulate	from tqdm.notebook import tqdm	from IPython.display import HTML, display"
144,1_text_analysis_tools.ipynb,1,markdown,slide,1,<h1><center>Introduction. Tools for text analysis. TF-IDF.</center></h1>
145,1_text_analysis_tools.ipynb,2,markdown,-,1,# Overview of the course
146,1_text_analysis_tools.ipynb,3,markdown,slide,12,"## The team		### Your teacher	- Sergey Khoroshenkikh	- Senior software engineer at [Yandex](https://yandex.com/company/)	- ~6 years of experience in data analysis and engineering	- Taught Machine Learning at Harbour.Space in 2019: [DS210](https://in.harbour.space/data-science/machine-learning-sergey-khoroshenkikh/)	- <img src=""https://www.freepnglogos.com/uploads/logo-gmail-png/logo-gmail-png-image-gmail-logo-gta-wiki-the-grand-theft-auto-4.png"" alt=""email"" width=""15""> horoshenkih91@gmail.com, <img src=""https://upload.wikimedia.org/wikipedia/commons/8/82/Telegram_logo.svg"" alt=""tg"" width=""15""> @khoroshenkikh		### Teaching assistant	- Catalina Sagan	- <img src=""https://www.freepnglogos.com/uploads/logo-gmail-png/logo-gmail-png-image-gmail-logo-gta-wiki-the-grand-theft-auto-4.png"" alt=""email"" width=""15""> csagan.cs@gmail.com"
147,1_text_analysis_tools.ipynb,4,markdown,slide,7,"## Learning objectives	1. Learn the main concepts	- TF-IDF, language models, classification, topic modeling, distributional semantics, neural networks, ...	2. Understand the algorithms	- n-gram language models, linear classifiers, LDA, word2vec, backpropagation, RNN, LSTM, ...	3. Practice the tools **on real datasets**	- spaCy, sklearn, gensim, fasttext, pytorch, ..."
148,1_text_analysis_tools.ipynb,5,markdown,slide,12,	## Core ideas of the course	1. Remote-first	- Chunked content	- Ungraded quizzes during the lecture	- Coding sessions in Zoom breakout rooms	2. Hands-on	- Coding sessions	- Final projects	3. Code-first	- The most important content is written in Python code (slides are somewhat optional)	- Math is supported by code
149,1_text_analysis_tools.ipynb,6,markdown,slide,11,## Grading policy	- 30 points: graded quiz assignments in Google Classroom (**class code: wo7vv5r**)	- 70+ points: final project	- Apply learned Text Mining techniques on real dataset	- Report your results at the end of the course	- **Receive additional points for making a small MVP**	- There are many ways to get 100 points	- Apply more Text Mining techniques	- Create an MVP		See more details in our Google Classroom.
150,1_text_analysis_tools.ipynb,7,markdown,slide,15,"## Technical details	- The course is taught in Google Colab	- Colab notebooks are the primary source of information	- ⬅️ Table of contents is on the left	- All the slides are created from Colab notebooks	- Ungraded quizzes in Colab notebooks:	- Always run the first cell in the notebook to make it work	- I implemented the Python library for quizzes on my own, so bugs are possible	- You can copy Colab notebooks and run your copy	- You can modify your copy	- Notes: `> ...`	> The example of a note	- $\LaTeX$ equations: `$...$` or `$$...$$`	$$E=mc^2$$	- A short overview of Colab features: [link](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)"
151,1_text_analysis_tools.ipynb,8,markdown,slide,11,"## Remote learning details	- You are muted by default. To ask a question	- Press the ""raise hand"" button in Zoom	- Or write your question to the Zoom chat	- Ungraded quizzes	- After you've finished the quiz, press the **""raise hand""**	- If you have a question, write it to the Zoom chat	- Coding sessions	- You work in individual breakout rooms	- Turn on screen sharing	- To ask a question, press the ""raise hand"" button"
152,1_text_analysis_tools.ipynb,9,markdown,-,11,"<!--slideshow slide-->	# Plan for today	1. Look at the collection of Donald Trump's tweets	2. Get familiar with `spacy`: Python library for text analysis	3. Learn about tokenization, lemmatization and NLP pipeline in general	4. Coding session 1: work with `spacy`	5. Learn about TF-IDF	6. ""Word of a day"": find specific words in tweets	7. Learn how co compute TF-IDF in `sklearn`	8. Find similar texts using TF-IDF	9. Coding session 2: work with `sklearn`"
153,1_text_analysis_tools.ipynb,10,markdown,slide,5,"# Dataset: TrumpTwitterArchive.com		In this lesson, we will study [Trump's tweets](http://trumptwitterarchive.com/archive) stored in a JSON file in [the course repo on GitHub](https://github.com/horoshenkih/harbour-space-text-mining-course).		> You can also store the data for your final projects on GitHub"
154,1_text_analysis_tools.ipynb,11,code,slide,4,"# NB! Run the first cell of the notebook to checkout the repo	import json	with open(""harbour-space-text-mining-course/datasets/trump_twitter_archive/tweets.json"") as f:	tweets = json.load(f)"
155,1_text_analysis_tools.ipynb,12,markdown,slide,1,Tweets are stored in a list of dictionaries
156,1_text_analysis_tools.ipynb,13,code,slide,5,"from pprint import pprint	print(f""type: {type(tweets)}"")	pprint(tweets[0])	print(f""Text: {tweets[0]['text']}"")	print(f""Created at: '{tweets[0]['created_at']}'"")"
157,1_text_analysis_tools.ipynb,14,markdown,slide,2,## Preliminary analysis	Let's look at the meaningful characteristics of the dataset
158,1_text_analysis_tools.ipynb,15,code,slide,15,"print(f""Number of tweets: {len(tweets)}"")	# compute the number of unique characters	char_set = set()	for t in tweets:	char_set.update(list(t['text']))	print(f""Number of unique characters: {len(char_set)}"")	# compute the distribution of text sizes	tweet_len_characters = [len(t['text']) for t in tweets]	avg_text_len = sum(tweet_len_characters) / len(tweet_len_characters)	print(f""Average tweet length: {avg_text_len}"")	import matplotlib.pyplot as plt	# print the distribution of tweet sizes	plt.figure(figsize=(10, 6))	plt.hist(tweet_len_characters, bins=100)	plt.show()"
159,1_text_analysis_tools.ipynb,16,markdown,slide,7,"# Tokenization		How do we read texts?		- Technically speaking, a text is a sequence of *characters*.	- But we don't read texts character-by-character.	- We perceive the text as the sequence of words, phrases, punctuation and other *meaningful* pieces."
160,1_text_analysis_tools.ipynb,17,markdown,slide,7,"**Definition**: *token* is a meaningful substring (for example, a word or a punctuation sign).		The exact definition depends on your application!	- You may need to keep emojis for *sentiment analysis* and ignore them for *topic modeling*.	- You may remove punctuation for *text classification* and keep it for *language modeling*.		For example, the substring `"":-)""` may be a single token, three tokens or it may be ignored (0 tokens)."
161,1_text_analysis_tools.ipynb,18,markdown,slide,3,The easiest to tokenize the text is to split it by whitespace		> This is what the method `.split()` does.
162,1_text_analysis_tools.ipynb,19,code,slide,5,"text = tweets[222][""text""]	print(""Text:"")	pprint(text)	print(""Tokens:"")	pprint(text.split(), compact=True)"
163,1_text_analysis_tools.ipynb,20,markdown,slide,6,"We see that dealing with punctuation is tricky.	> We need `['Poll', ':']` instead of `'Poll:'`		But we cannot split by punctuation.		> We don't need to split `""DIDN'T""` into `['DIDN', 'T']`"
164,1_text_analysis_tools.ipynb,21,markdown,slide,3,## spaCy		It's time to get familiar with `spaCy` - a Python library for Natural Language Processing (NLP).
165,1_text_analysis_tools.ipynb,22,code,-,1,from pprint import pprint
166,1_text_analysis_tools.ipynb,23,code,slide,17,"# text to be tokenized	print(""Text:"")	pprint(tweets[222][""text""])		from spacy.lang.en import English		# create text analyzer that includes rules of tokenization	# rules are language-specific, so we load the rules for English	nlp = English()		# spaCy has functional API: you just call nlp(text)	analyzed_text = nlp(tweets[222][""text""])		# the resulting object (`analyzed_text`) is iterable, and tokens can be extracted by iteration	print(""Tokens:"")	pprint([token.text for token in analyzed_text], compact=True)	print([token.text for token in analyzed_text])"
167,1_text_analysis_tools.ipynb,24,markdown,slide,1,Let's find the most frequent tokens
168,1_text_analysis_tools.ipynb,25,code,slide,10,"from spacy.lang.en import English		nlp = English()	token_counter = Counter()		for tweet in tweets[:10000]:	doc = nlp(tweet[""text""])	for token in doc:	token_counter[token.text] += 1	pprint(token_counter.most_common()[:20], compact=True)"
169,1_text_analysis_tools.ipynb,26,markdown,slide,2,"- Note that tokens ""The"" and ""the"" are treated as different words, but have the same meaning.	- ""is"" and ""are"" are two forms of the same verb."
170,1_text_analysis_tools.ipynb,27,markdown,slide,13,"## Colab quiz 1	This is the first in-class code quiz in the course.	Its purpose is to check your understanding as the class progresses.	**All Colab quizzes are ungraded.**		1. Make sure to run the first cell of this Colab notebook. It imports all the necessary libraries to run the quiz.	2. Your task is to implement a function according to the problem statement. In the example below, the problem statement is	> Compute the number of tokens in an input string using spaCy.		The function you need to implement is called `solution()`.	The signature (i.e. the input and output types) and the first line are already implemented; your task is to complete the implementation.	3. `check_solution()` is the function that prints out the problem statement and checks your solution. You can see the results of some (but not all) tests. In this quiz, there are 5 tests but only 2 are shown to you.	4. After you've finished, press the ""Raise hand"" button in Zoom. If you have a question, send it to the Zoom chat or ask me in the chat to unmute you."
171,1_text_analysis_tools.ipynb,28,code,-,7,def solution(s: str) -> int:	from spacy.lang.en import English	nlp = English()		# the function `quiz_count_tokens()` returns a function that checks your solituon	check_solution = quiz_count_tokens()	check_solution(solution)
172,1_text_analysis_tools.ipynb,29,markdown,slide,6,"# Lemmatization		It is often convenient to use only one form of a word	- ""The"", ""the"" $\rightarrow$ ""the""	- ""is"", ""are"" $\rightarrow$ ""be""	"
173,1_text_analysis_tools.ipynb,30,markdown,slide,2,"**Definition**: *lemma* is the canonical form of a word.	> For example, dictionaries contain lemmas instead of all possible word forms."
174,1_text_analysis_tools.ipynb,31,markdown,slide,2,	Let's see how to find lemmas in spaCy.
175,1_text_analysis_tools.ipynb,32,code,slide,14,"import spacy	# note that not we create `nlp` in a different way	# because we need to load dictionaries for lemmatization	nlp = spacy.load(""en"")		token_lemma_hash = []	for token in nlp(""Don't mess with Donald Trump.""):	# `lemma_` attribute contains the text of the token's lemma	lemma = token.lemma_	# `lemma` (without underscore) contains the hashed lemma (the internal spaCy prepresentation)	hashed_lemma = token.lemma	token_lemma_hash.append((token, lemma, hashed_lemma))		print(tabulate(token_lemma_hash, headers=[""Token"", ""Lemma"", ""Hashed lemma""]))"
176,1_text_analysis_tools.ipynb,33,markdown,slide,3,"Note the translations	- `[""Do"", ""n't""]` $\rightarrow$ `[""do"", not""]`	- `[""Donald"", ""Trump""]` $\rightarrow$ `[""Donald"", ""Trump""]` (not `[""donald"", ""trump""]`)"
177,1_text_analysis_tools.ipynb,34,markdown,slide,1,"How does it work? Why `""Do""` $\rightarrow$ `""do""` and not `""Trump""` $\rightarrow$ `""trump""`?"
178,1_text_analysis_tools.ipynb,35,markdown,slide,3,"spaCy determines the lemma using not only spelling. It also predicts part of speech (POS) of the token.		> Part of speech is _predicted with some probability_, because it is impossible to infer it from spelling."
179,1_text_analysis_tools.ipynb,36,markdown,slide,3,This process (predict part of speech) is called POS-tagging.		Let's see how to perform POS-tagging with spaCy.
180,1_text_analysis_tools.ipynb,37,code,slide,9,"token_tag_description = []	for text in (""he had played a trump"", ""trump is the us president""):	for token in nlp(text):	# `tag_` attribute contains the text of the token's predicted part-of-speech tag	tag = token.tag_	# `spacy.explain` makes tag_ human-readable	tag_description = spacy.explain(token.tag_)	token_tag_description.append((token, tag, tag_description))	print(tabulate(token_tag_description, headers=(""Token"", ""Tag"", ""Tag description"")))"
181,1_text_analysis_tools.ipynb,38,markdown,slide,1,## Colab quiz 2
182,1_text_analysis_tools.ipynb,39,code,-,7,"def solution(s: str) -> int:	import spacy	nlp = spacy.load(""en"")	# YOUR CODE HERE		check_solution = quiz_count_lemmas()	check_solution(solution)"
183,1_text_analysis_tools.ipynb,40,markdown,slide,10,# NLP pipeline		`spaCy` extracts a lot of information from the text.		It is performed as a sequence of steps combined into so-called NLP pipeline.	> NLP = Natural Language Processing		![NLP pipeline](https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)		Read more: https://spacy.io/usage/processing-pipelines
184,1_text_analysis_tools.ipynb,41,code,-,5,"#@sldeshow slide	import spacy	nlp = spacy.load(""en"")	# `nlp` object has the attribute `pipeline`	nlp.pipeline"
185,1_text_analysis_tools.ipynb,42,markdown,slide,2,`tagger` performs POS-tagging.	Let's briefly look at `parser` and `ner`.
186,1_text_analysis_tools.ipynb,43,markdown,slide,2,## parser	The `parser` component extracts dependencies between words.
187,1_text_analysis_tools.ipynb,44,code,slide,5,"doc = nlp(""This is a sentence."")		# display the relations using spacy.displacy.render	html = spacy.displacy.render(doc, style=""dep"")	display(HTML(html))"
188,1_text_analysis_tools.ipynb,45,markdown,slide,4,"## ner	The `ner` component recognizes named entities. (NER stands for ""Named Entity Recognition"").		> Named entites correspond to some objects in real world (persons, countries, organizations, etc.)"
189,1_text_analysis_tools.ipynb,46,markdown,slide,2,Let's find named entities in tweets.	Pay attention to mistakes that `ner` makes.
190,1_text_analysis_tools.ipynb,47,code,slide,17,"doc = nlp(tweets[86][""text""])	# highlight names entities in the text using spacy	html = spacy.displacy.render(doc, style=""ent"")	display(HTML(html))		entity_label_description = []	for ent in doc.ents:	# the text recognized as a named entity	entity_text = ent.text	# each entity is assigned a label, stored in the `label_` attribute	entity_label = ent.label_	# spacy.explain() works with entity labels	explained_entity_label = spacy.explain(ent.label_)		entity_label_description.append((entity_text, entity_label, explained_entity_label))		print(tabulate(entity_label_description, headers=(""Entity text"", ""Entity label"", ""Entity description"")))"
191,1_text_analysis_tools.ipynb,48,markdown,slide,4,"## Customizing pipelines	- By default, spaCy includes a lot of components in NLP pipeline.	- However, each component delays text processing.	- It is recommended to keep only necessary parts of the pipeline."
192,1_text_analysis_tools.ipynb,49,code,slide,1,"nlp_all = spacy.load(""en"")  # load tagger, parser, ner"
193,1_text_analysis_tools.ipynb,50,code,slide,3,"%%timeit	for tweet in tweets[:100]:	nlp_all(tweet[""text""])"
194,1_text_analysis_tools.ipynb,51,code,slide,1,"nlp_tagger = spacy.load(""en"", disable=[""parser"", ""ner""])  # load tagger only"
195,1_text_analysis_tools.ipynb,52,code,slide,3,"%%timeit	for tweet in tweets[:100]:	nlp_tagger(tweet[""text""])"
196,1_text_analysis_tools.ipynb,53,code,slide,1,"nlp_none = spacy.load(""en"", disable=[""parser"", ""ner"", ""tagger""])  # disable everything"
197,1_text_analysis_tools.ipynb,54,code,slide,3,"%%timeit	for tweet in tweets[:100]:	nlp_none(tweet[""text""])"
198,1_text_analysis_tools.ipynb,55,markdown,slide,1,## Colab quiz 3
199,1_text_analysis_tools.ipynb,56,code,-,7,"def solution(s: str) -> set:	import spacy	nlp = spacy.load(""en"")	# YOUR CODE HERE		check_solution = quiz_ner()	check_solution(solution)"
200,1_text_analysis_tools.ipynb,57,markdown,slide,3,# Coding session 1: `spacy`		
201,1_text_analysis_tools.ipynb,58,markdown,-,7,"## Exercise 1		Compute the average number of tokens in Trump's tweets, the number of unique tokens, and draw the distrubution of the number of tokens (like we did at the beginning for characters).		**Hints**	- Disable `tagger`, `parser` and `ner` to speed up computations.	- Use `tqdm` to show progress bar (instead `for x in lst` just write `for x in tqdm(lst)`)."
202,1_text_analysis_tools.ipynb,59,code,-,18,"import spacy	from tqdm.notebook import tqdm  # progress bar		nlp = spacy.load(""en"", disable=[""tagger"", ""parser"", ""ner""])	tweet_len = []	uniq_tokens = set()	for tweet in tqdm(tweets):	doc = nlp(tweet[""text""])	tokens = [token.text for token in doc]	tweet_len.append(len(tokens))	uniq_tokens |= set(tokens)  # uniq_tokens.update(tokens)		print(""Average len:"", sum(tweet_len) / len(tweet_len))	print(""num uniq tokens:"", len(uniq_tokens))		import matplotlib.pyplot as plt	plt.hist(tweet_len, bins=100)	plt.show()"
203,1_text_analysis_tools.ipynb,60,markdown,-,11,"## Exercise 2		Find 10 lemmas that have the largest number of **unique** forms.		For example, the text “Is is are are” contains _three_ distinct forms of the lemma “be”: “Is”, “is”, “are”.		**Hints**	- Disable `parser` and `ner` to speed up computations.	- Use `tqdm` to show progress bar.	- Debug on a small subset of tweets, then compute on the full dataset.	- Lemma is stored in `.lemma_` attribute of spaCy token, form is the token text in the attribute `.text`."
204,1_text_analysis_tools.ipynb,61,code,-,11,"from collections import defaultdict		# types in Python: int, float, list, dict, ...	# dct = defaultdict(int)	# dct[""a""] += 1  # for ordinary dict, you need to check if ""a"" exists	# dct[""a""]		dct = defaultdict(set)	dct[""b""].add(42)	dct[""b""].add(""Z"")	dct, dct[""b""]"
205,1_text_analysis_tools.ipynb,62,code,-,18,"from collections import defaultdict	from tqdm.notebook import tqdm		nlp = spacy.load(""en"", disable=[""parser"", ""ner""])	lemma_forms = defaultdict(set)		for tweet in tqdm(tweets):	for token in nlp(tweet[""text""]):	lemma_forms[token.lemma_].add(token.text)		from pprint import pprint	pprint(	list(sorted(	lemma_forms,	key=lambda l: len(lemma_forms[l]),	reverse=True	))[:10]	)"
206,1_text_analysis_tools.ipynb,63,markdown,-,9,"## Exercise 3		1. How many tweets contain named entities?	2. What are top-5 most popular entity labels?		**Hints**:	- Disable `parser` and `tagger` to speed up computations.	- Use `tqdm` to show progress bar.	- Debug on a small subset of tweets, then compute on the full dataset."
207,1_text_analysis_tools.ipynb,64,code,-,15,"import spacy	from collections import Counter	from tqdm.notebook import tqdm		nlp = spacy.load(""en"", disable=[""parser"", ""tagger""])		tweets_with_ne = 0	ne_labels = Counter()	for tweet in tqdm(tweets[:1000]):	doc = nlp(tweet[""text""])	if doc.ents:	tweets_with_ne += 1	for ent in doc.ents:	ne_labels[ent.label_] += 1	tweets_with_ne, ne_labels.most_common(5)"
208,1_text_analysis_tools.ipynb,65,markdown,-,1,# Python refresher: `collections.counter`
209,1_text_analysis_tools.ipynb,66,code,-,10,"# collections.Counter is a subclass of dict that has default value 0 for all keys	from collections import Counter		cnt = Counter()	cnt[""a""] += 1	cnt[""b""] += 2	print(cnt)	print(cnt[""a""])	print(cnt[""b""])	print(cnt[""c""])  # default value is 0"
210,1_text_analysis_tools.ipynb,67,markdown,slide,5,"# ""Word of a day""		**Problem statement**	- For each day, find the word which is ""specific"" for this day in Trump's tweets.	- It may be a name, a country, or some event."
211,1_text_analysis_tools.ipynb,68,markdown,slide,4,"**Definitions and notation**:	- *Term* (notation: $t$) is a ""meaningful"" piece of text (lemma in our case).	- *Document* (notation: $d$) is the collection of tweets created on a given day.	- $D$ is the set of all documents (not tweets!)."
212,1_text_analysis_tools.ipynb,69,markdown,slide,1,The first attempt: find the most frequent term for each document.
213,1_text_analysis_tools.ipynb,70,code,slide,17,"# preprocess the data: create documents from tweets	from pprint import pprint	from dateutil.parser import parse as parse_datetime  # convert a string in any format to a datetime object	# preprocess the data: convert each tweet into tuple (date, lemmas_list)	import spacy	nlp = spacy.load(""en"", disable=[""parser"", ""ner"", ""tagger""])	preprocessed_tweets = []	for tweet in tqdm(tweets):	if tweet.get(""is_retweet""):	# ignore retweets	continue	tweet_date = parse_datetime(tweet[""created_at""]).date()	tweet_lemmas = [token.lemma_ for token in nlp(tweet[""text""])]	preprocessed_tweets.append((tweet_date, tweet_lemmas))		# this is how the preprocessed data looks like	pprint(preprocessed_tweets[100], compact=True)"
214,1_text_analysis_tools.ipynb,71,code,-,1,"from collections import Counter, defaultdict"
215,1_text_analysis_tools.ipynb,72,code,slide,7,"# term count for each document	document_term_counter = defaultdict(Counter)	for tweet_date, tweet_lemmas in preprocessed_tweets:	for lemma in tweet_lemmas:	# tweet_date is the document	# lemma is the term	document_term_counter[tweet_date][lemma] += 1"
216,1_text_analysis_tools.ipynb,73,code,slide,16,"# visualize		documents = []	terms = []	weights = []		for d in sorted(document_term_counter):	documents.append(d)	# get the most frequent term and its count	term, count = document_term_counter[d].most_common(1)[0]	terms.append(term)	weights.append(count)		from datetime import date	html = calendar_table(documents, terms, weights, from_datetime=date(2020, 3, 30))	display(HTML(html))"
217,1_text_analysis_tools.ipynb,74,markdown,slide,3,"- As expected, the most frequent terms overall are the most frequent in each document.	- The most frequent terms are not informative	- Does informativity depend on frequency?"
218,1_text_analysis_tools.ipynb,75,code,-,5,"# count all terms	term_counter = Counter()	for tweet_date, tweet_lemmas in preprocessed_tweets:	for term in tweet_lemmas:	term_counter[term] += 1"
219,1_text_analysis_tools.ipynb,76,code,slide,1,"visualize_term_counter(term_counter, 0, ""Top 10 terms: frequent, not informative"", use_ggplot=False)"
220,1_text_analysis_tools.ipynb,77,code,slide,1,"visualize_term_counter(term_counter, 1200, ""Terms from 1200 to 1210: not frequent, possibly informative"", use_ggplot=False)"
221,1_text_analysis_tools.ipynb,78,code,slide,1,"visualize_term_counter(term_counter, 35000, ""Terms from 35000 to 35010: rare, not informative"", use_ggplot=False)"
222,1_text_analysis_tools.ipynb,79,markdown,slide,4,# TF-IDF		Try the following idea:	> The term $t$ is specific (or informative) for the document $d$ if it occurs relatively rarely in $D$ but occurs often in $d$.
223,1_text_analysis_tools.ipynb,80,markdown,slide,7,"	The measure of ""informativity"" of term $t$ for document $d$ should satisfy two properties:	1. The higher the frequency of $t$ in $d$, the _higher_ the informativity.	2. The higher the frequency of $t$ in the collection $D$, the _lower_ the informativity.			"
224,1_text_analysis_tools.ipynb,81,markdown,slide,2,"It can be written as the product of two functions:	$$TFIDF(t, d, D) = TF(t, d) \cdot IDF(t, D)$$"
225,1_text_analysis_tools.ipynb,82,markdown,slide,3,"## Term frequency $TF(t, d)$		Term frequency is just the number of occurrences of the term $t$ in the document $d$."
226,1_text_analysis_tools.ipynb,83,markdown,slide,6,"## Inverse document frequency $IDF(t, D)$		- In practice, the range of ""informativity"" is wide.	- Informative terms may occur in 1%, 0.1% or even 0.01% of all documents $D$.	- That's why $IDF(t, D)$ depends on $t$'s frequency in $D$ _logarithmically_.	> If $t_1$ is 10 times more frequent than $t_2$, then $IDF(t_1, D) - IDF(t_2, D) = \mathrm{const}$"
227,1_text_analysis_tools.ipynb,84,markdown,slide,5,"$IDF(t, D)$ is computed in 3 steps:	1. Compute the number of documents that contain $t$.	2. Divide it by the total number of documents.	> The result is the probability to find term $t$ in a randomly chosen document.	3. Take the $\log$ of the _inverse_ quantity from the previous step."
228,1_text_analysis_tools.ipynb,85,markdown,slide,6,"Or, mathematically	$$	IDF(t, D) = \log\left(\dfrac{|D|}{|\{d \in D | t \in d\}|}\right)	$$	- $|D|$ is the total number of documents	- $|\{d \in D | t \in d\}|$ is the number of documents that contain $t$"
229,1_text_analysis_tools.ipynb,86,markdown,slide,12,"## TF-IDF example		Consider the collection $D$ containing 3 documents		1. $d_1$ = ""If you tell the truth you don’t have to remember anything.""	2. $d_2$ = ""If you don’t read the newspaper, you’re uninformed. If you read the newspaper, you’re misinformed.""	3. $d_3$ = ""A lie can travel half way around the world while the truth is putting on its shoes.""		Let us compute $TFIDF(\mathrm{''newspaper''}, d_2, D)$:	- $TF(\mathrm{''newspaper''}, d_2) = 2$	- $IDF(\mathrm{''newspaper''}, D) = \log\left(\dfrac{3}{1}\right) \approx 1.1$	- $TFIDF(\mathrm{''newspaper''}, d_2, D) \approx 2.2$"
230,1_text_analysis_tools.ipynb,87,markdown,slide,4,"## Colab qiuz 4	Compute $TFIDF(\mathrm{''If''}, d_1, D)$		"
231,1_text_analysis_tools.ipynb,88,code,-,2,enable_mathjax_in_cell()	quiz_tfidf()()
232,1_text_analysis_tools.ipynb,89,markdown,slide,1,## TF-IDF implementation
233,1_text_analysis_tools.ipynb,90,code,slide,26,"from collections import defaultdict, Counter	# 1. Compute IDF for all terms.	#    Remember that we've already computed term counts for all documents.	#    Document_term_counter[document][term] = term_count_in_document.		# For each term, compute the number of documents where the term occurs	term_counter = Counter()	for document in document_term_counter:	for term in document_term_counter[document]:	term_counter[term] += 1	# compute term IDFs from term counts	num_documents = len(document_term_counter)	from math import log	term_idf = {	term: log(num_documents / term_count)	for term, term_count in term_counter.items()	}		# 2. Compute TF-IDF for all terms and documents	document_term_tfidf = defaultdict(Counter)	for document in document_term_counter:	word_tfidf = {}	for term in document_term_counter[document]:	tf = document_term_counter[document][term]	idf = term_idf[term]	document_term_tfidf[document][term] = tf * idf"
234,1_text_analysis_tools.ipynb,91,code,-,20,"# visualize	documents = []	terms = []	weights = []		for d in sorted(document_term_tfidf):	documents.append(d)	# get the most frequent term and its count	term = max(document_term_tfidf[d], key=lambda t: document_term_tfidf[d][t])	terms.append(term)	weights.append(document_term_tfidf[d][term])		def print_tweets_by_date(tweets, d):	from pprint import pprint	for tweet in tweets:	if tweet.get(""is_retweet""):	continue	if parse_datetime(tweet[""created_at""]).date() == d:	pprint(tweet[""text""])	print(""---"")"
235,1_text_analysis_tools.ipynb,92,markdown,slide,1,## Colab demo: insights from daily TF-IDF
236,1_text_analysis_tools.ipynb,93,code,-,10,"# look at the period of Trump's impeachment	from datetime import date	html = calendar_table(	documents,	terms,	weights,	from_datetime=date(2019, 12, 1),	to_datetime=date(2020, 2, 1),	)	display(HTML(html))"
237,1_text_analysis_tools.ipynb,94,code,-,2,"# the first time ""impeachment"" becomes ""word of a day""	print_tweets_by_date(tweets, date(2019, 12, 2))"
238,1_text_analysis_tools.ipynb,95,code,-,2,"# the second time ""impeachment"" becomes ""word of a day""	print_tweets_by_date(tweets, date(2020, 1, 29))"
239,1_text_analysis_tools.ipynb,96,code,-,2,"# interestingly, Coronavirus becomes ""word of a day"" quite early	print_tweets_by_date(tweets, date(2020, 1, 30))"
240,1_text_analysis_tools.ipynb,97,code,-,10,"# look at the period of COVID-19 outbreak	from datetime import date	html = calendar_table(	documents,	terms,	weights,	from_datetime=date(2020, 3, 1),	to_datetime=date(2020, 5, 1),	)	display(HTML(html))"
241,1_text_analysis_tools.ipynb,98,markdown,slide,9,"## Remarks on TF-IDF	1. The idea of TF-IDF is very general and can be applied outside of Text Mining domain.	> For example, a sequence of websites visited by a user can be treated as a ""document"", and each individual website is a ""term"".		> It doesn't tell you much if the user visits [facebook.com](https://colab.research.google.com/drive/1HBWC4XPaYmnyF8nF6OpAKSy1hLasZxuV?usp=sharing) 10 times a day.		> But 10 visits of [stackoverflow.com](https://stackoverflow.com) per day may tell you something.		2. You can look at the probabilistic interpretation of TF-IDF in the **[OPTIONAL]** section."
242,1_text_analysis_tools.ipynb,99,markdown,-,1,# TF-IDF in sklearn
243,1_text_analysis_tools.ipynb,100,markdown,slide,4,"# Transform texts to vectors		In practice, it's convenient to have the finite number of terms.	> We can use, say, 10000 the most common words, ignoring all the other words."
244,1_text_analysis_tools.ipynb,101,code,slide,26,"n_frequent_terms = 10000	most_common_terms = set([x[0] for x in term_counter.most_common()][:n_frequent_terms])	import random	random.seed(42)		tweets_data = []	for random_tweet in random.choices(preprocessed_tweets, k=10):	original_tweet = "" "".join(random_tweet[1])	masked_tweet = []	for t in random_tweet[1]:	if t in most_common_terms:	masked_tweet.append(t)	else:	masked_tweet.append(""<UNK>"")	tweet_with_rare_words_ignored = "" "".join(masked_tweet)	tweets_data.append((original_tweet, tweet_with_rare_words_ignored))		from IPython.display import display, HTML	display(HTML(tabulate(	tweets_data,	headers=(	""Original tweets ({} terms)"".format(len(term_counter)),	""Keep {} frequent terms"".format(n_frequent_terms)	),	tablefmt=""html""	)))"
245,1_text_analysis_tools.ipynb,102,markdown,slide,5,"Our observations so far:	1. For each term $t$ in each document $d \in D$, we can compute $TFIDF(t, d, D)$.	2. We can have the finite number of terms.		Therefore, we can represent each document with the fixed set of TFIDFs."
246,1_text_analysis_tools.ipynb,103,markdown,slide,1,"In other words, we can convert a document into a **vector**."
247,1_text_analysis_tools.ipynb,104,markdown,slide,3,"But why?	- To compute the distance between two documents (for example, to find similar documents).	- Vectors are inputs for machine learning algorithms."
248,1_text_analysis_tools.ipynb,105,markdown,slide,14,"## Colab quiz 5	The cosine distance between vectors $\mathbb{v_1}$ and $\mathbb{v_2}$ is computed as	$$	d(\mathbb{v_1}, \mathbb{v_2}) = 1 - \frac{1}{2}\cos(\textrm{angle between }\mathbb{v_1}\textrm{ and }\mathbb{v_2}) =	$$	$$ =1 - \frac{1}{2}\left(\dfrac{\mathbb{v_1}}{||\mathbb{v_1}||}\right) \cdot \left(\dfrac{\mathbb{v_2}}{||\mathbb{v_2}||}\right)	$$		where $||\mathbb{v}|| = \sqrt{\sum_i v_i^2}$ is the Euclidean norm of $\mathbb{v}$, and $\mathbb{v} \cdot \mathbb{w} = \sum_i v_i w_i$ is the dot product of $\mathbb{v}$ and $\mathbb{w}$.		So the algorithm to compute $d(\mathbb{v_1}, \mathbb{v_2})$ is:	1. Normalize the vectors: $\mathbb{\tilde v_1} = \dfrac{\mathbb{v_1}}{||\mathbb{v_1}||}$,  $\mathbb{\tilde v_2} = \dfrac{\mathbb{v_2}}{||\mathbb{v_2}||}$	2. Compute the dot product between the normalized vectors $\mathbb{\tilde v_1} \cdot \mathbb{\tilde v_2}$	3. Compute the distance: $d(\mathbb{v_1},~\mathbb{v_2}) = 1 - \frac{1}{2}\mathbb{\tilde v_1}~\cdot~\mathbb{\tilde v_2}$"
249,1_text_analysis_tools.ipynb,106,code,-,2,enable_mathjax_in_cell()	quiz_vector_distance()()
250,1_text_analysis_tools.ipynb,107,markdown,slide,1,"scikit-learn implements so-called ""vectorizers"" that convert texts into vectors."
251,1_text_analysis_tools.ipynb,108,code,slide,26,"# TfidfVectorizer converts texts into vectors	from sklearn.feature_extraction.text import TfidfVectorizer		# the texts we will vectorize	data = [	""One, two, three."",	""Two, three."",	""Three.""	]		# Tfidf vectorizer has the ""smooth"" version of IDF	# Here is the difference:	#   idf = log(n_documents / count) + 1	#   smooth_idf = log((n_documents + 1) / (count + 1)) + 1		# Note that instead of	#   idf = log(n_documents / count)	# TfidfVectorizer computes	#   idf = log(n_documents / count) + 1	# It helps to distinguish 2 cases:	#   1. TF-IDF(t, d, D) = 0 because the term t is absent in the document d	#   2. TF-IDF(t, d, D) = 0 because the term t is present in each document in the collection D	vectorizer = TfidfVectorizer(smooth_idf=False)		# to compute TF-IDFs, use the method `.fit()`	vectorizer.fit(data)"
252,1_text_analysis_tools.ipynb,109,code,slide,16,"# vectorizer.vocabulary_ contains word -> index mapping	print(""vectorizer.vocabulary_:"", vectorizer.vocabulary_)	# vectorizer.idf_ contains the idf value for each index	print(""vectorizer.idf_:"", vectorizer.idf_)		# let's check that TfidfVectorizer uses the formula	#   idf = log(n_documents / count) + 1	# to do so, we restore word count from idf	vectorizer_internals = []	for word, idx in vectorizer.vocabulary_.items():	idf = vectorizer.idf_[idx]	n_documents = len(data)	count = n_documents / exp(idf - 1)	vectorizer_internals.append([word, idf, count])		print(tabulate(vectorizer_internals, headers=[""word"", ""idf"", ""count""]))"
253,1_text_analysis_tools.ipynb,110,code,slide,6,"# The trained vectorizer can be applied on new data	new_data = [	""Two, three, four"",	""Five"",	]	pprint(vectorizer.transform(new_data).todense())"
254,1_text_analysis_tools.ipynb,111,markdown,slide,8,"The `.transform()` method converts the input text array into matrix where:	- $i$-th row corresponds to the $i$-th input text	- $j$-th column corresponds to some term $t_j$ *from the training set*	> The mapping $t_j \rightarrow j$ is stored in the `.vocabulary_` attribute.		Note that:	1. New words (""four"", ""five"") are ignored.	1. Non-zero vectors are normalized (have length 1)."
255,1_text_analysis_tools.ipynb,112,markdown,slide,1,## Colab quiz 6
256,1_text_analysis_tools.ipynb,113,code,-,1,quiz_vectorizer_shape()()
257,1_text_analysis_tools.ipynb,114,markdown,slide,1,"By default, `TfidfVectorizer` performs simple tokenization: lowercasing, split by punctuation."
258,1_text_analysis_tools.ipynb,115,code,-,1,from pprint import pprint
259,1_text_analysis_tools.ipynb,116,code,slide,4,"text = tweets[99][""text""]	print(text)	vectorizer = TfidfVectorizer().fit([text])	pprint(vectorizer.vocabulary_)"
260,1_text_analysis_tools.ipynb,117,markdown,slide,1,But the tokenizer can be redefined:
261,1_text_analysis_tools.ipynb,118,code,slide,9,"nlp = spacy.load(""en"", disable=[""parser"", ""ner"", ""tagger""])		def spacy_tokenizer(text):	return [t.lemma_ for t in nlp(text)]		text = tweets[99][""text""]	print(text)	vectorizer_spacy = TfidfVectorizer(tokenizer=spacy_tokenizer).fit([text])	pprint(vectorizer_spacy.vocabulary_)"
262,1_text_analysis_tools.ipynb,119,markdown,slide,3,# Text similarity with TF-IDF: Quora Question Pairs		Now we will use TF-IDF vectors to find duplicates in Quora questions.
263,1_text_analysis_tools.ipynb,120,code,slide,8,"import pandas as pd		df = pd.read_csv(""harbour-space-text-mining-course/datasets/quora_question_pairs/train.csv"")	print(f""Total samples: {df.shape[0]}"")	# select a subset of rows to speed up the demonstration	df = df.head(20000)	# `is_duplicate` shows whether `question1` and `question2` are duplicates	df[[""question1"", ""question2"", ""is_duplicate""]].head(10)"
264,1_text_analysis_tools.ipynb,121,code,slide,13,"import numpy as np	from sklearn.model_selection import train_test_split		# split the data into two parts	# we will train TfidfVectorizer on the first (train) part and apply it on the second (test) part	df_train, df_test = train_test_split(df.fillna("".""), test_size=0.5, shuffle=True, random_state=0)		# we use spacy_tokenizer, but we could use the default one	quora_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)		# stack two columns: question1 first, question2 second	questions = np.hstack([df_train.question1.values, df_train.question2.values])	quora_vectorizer.fit(questions)"
265,1_text_analysis_tools.ipynb,122,code,slide,3,# extract TF-IDF vectors from the test data	question_1_vectors = quora_vectorizer.transform(df_test.question1)	question_2_vectors = quora_vectorizer.transform(df_test.question2)
266,1_text_analysis_tools.ipynb,123,code,slide,14,"from sklearn.metrics import roc_auc_score		# compute distances between TF-IDF vectors	# iterate over pairs using `zip()` generator	# vectors are normalized, so the similarity = 1 - distance = dot product	tf_idf_similarity = [	# v1, v2 are sparse matrices: convert their dot product into a dense matrix and then to a scalar	np.dot(v1, v2.T).todense().item()	for v1, v2 in zip(question_1_vectors, question_2_vectors)	]	# evaluate ROC AUC score:	#   - ROC AUC = 1 for perfect ranking	#   - ROC AUC = 0.5 for random ranking	print(""ROC AUC:"", roc_auc_score(df_test.is_duplicate, tf_idf_similarity))"
267,1_text_analysis_tools.ipynb,124,markdown,slide,7,"# Other sklearn vectorizers		There are other vectorizers that convert texts into vectors. All of them implement the same `fit/transform` interface.	1. `CountVectorizer`. Constructs the vector of term frequencies instead of TF-IDFs	1. `HashingVectorizer`. Performs so-called ""one-hot encoding"". Converts the document into binary vector: 1 if the term occurs in the document and 0 otherwise.		> Hashing vectorizer doesn't store the vocabulary - it  constructs vectors ""on the fly"" using hash function."
268,1_text_analysis_tools.ipynb,125,code,slide,21,"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer	data = [	""one"",	""one two"",	""one two three"",	]		tfidf_vectorizer = TfidfVectorizer().fit(data)	count_vectorizer = CountVectorizer().fit(data)	hashing_vectorizer = HashingVectorizer().fit(data)		new_data = [	""one two two four"",	""five"",	]	tfidf_transformed = tfidf_vectorizer.transform(new_data).todense()	count_transformed = count_vectorizer.transform(new_data).todense()	hashing_transformed = hashing_vectorizer.transform(new_data).todense()	print(""TfidfVectorizer\n"", tfidf_transformed)	print(""CountVectorizer\n"", count_transformed)	print(""HashingVectorizer shape:"", hashing_transformed.shape)"
269,1_text_analysis_tools.ipynb,126,markdown,slide,1,# Coding session 2: `sklearn`
270,1_text_analysis_tools.ipynb,127,markdown,-,5,## Exercise 1	Compare `TfidfVectorizer` and `CountVectorizer` on the Quora Question Pairs dataset. Which one gives higher ROC AUC?		**Hints**:	- You can use the same split into train and test.
271,1_text_analysis_tools.ipynb,128,code,-,1,# YOUR CODE HERE
272,1_text_analysis_tools.ipynb,129,markdown,-,6,## Exercise 2		Compare the default tokenizer in `TfIdfVectorizer` and spaCy tokenizer. Which one gives higher ROC AUC?		**Hints**:	- You can use the same split into train and test.
273,1_text_analysis_tools.ipynb,130,code,-,1,# YOUR CODE HERE
274,1_text_analysis_tools.ipynb,131,markdown,-,8,"## Exercise 3		`TfIdfVectorizer` has the parameter `ngram_range`. Set it to `(1, 2)` and compare with the default one. Which one gives higher ROC AUC?		**Hints**:	- You can use the same split into train and test.	- Use the default tokenizer, not the spaCy tokenizer.	- `ngram_range=(1, 2)` means that terms are not only words but also subsequent word pairs (so-called *bigrams*). We will cover it in more details in the next lecture."
275,1_text_analysis_tools.ipynb,132,code,-,1,# YOUR CODE HERE
276,1_text_analysis_tools.ipynb,133,markdown,-,8,"## Exercise 4		`TfIdfVectorizer` has the parameter `stop_words`. Set it to `""english""` and compare with the default one. Which one gives higher ROC AUC?		**Hints**:	- You can use the same split into train and test.	- Use the default tokenizer, not the spaCy tokenizer.	- `stop_words=""english""` means that the most frequent non-informative words are removed from the input data."
277,1_text_analysis_tools.ipynb,134,code,-,1,# YOUR CODE HERE
278,1_text_analysis_tools.ipynb,135,markdown,slide,11,"# Lecture summary	1. `spacy`: Python library for text analysis	- tokenization	- lemmatization	- named entity recognition	1. TF-IDF: measure of the word informativity in the documents	- The product of term frequency (TF) and inverse document frequency (IDF)	1. `TfidfVectorizer` in `sklearn`	- `fit()`, `transform()`	1. TF-IDF vectors for text similarity	- Quora Question Pairs dataset"
279,1_text_analysis_tools.ipynb,136,markdown,slide,10,"# Recommended resources	- [📖 Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)	- [📖 Natural Language Processing with Python, chapter 3	](http://www.nltk.org/book/ch03.html)	- [📖 Advanced NLP with spaCy, chapter 1](https://course.spacy.io/)	- [📖 Lemma (morphology)](https://en.wikipedia.org/wiki/Lemma_(morphology))	- [📖 Language Processing Pipelines with spaCy](https://spacy.io/usage/processing-pipelines)	- [📖 Named Entity Recognition 101 with spaCy](https://spacy.io/usage/linguistic-features#named-entities)	- [📖 TfidfVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)	- [📖 Quora Question Pairs dataset](https://www.kaggle.com/c/quora-question-pairs/overview)"
280,1_text_analysis_tools.ipynb,137,markdown,slide,19,"# [OPTIONAL] Probabilistic interpretation of TF-IDF		Given the collection of documents $D$, for each $d \in D$ we want to figure out which terms are the most specific for the document.		Let the number of terms is finite: $T$ distinct terms.		Let $p_t$ is the probability to find the term $t$ in a random document from $D$, and $f_t$ is the frequency of term $t$ in the document $d$ ($f_t$ may be 0).		Then the product $\prod_{t=1}^{T} p_i^{f_t}$ gives the probability that the document $d$ is generated randomly.		Thus, the inverse quality $L_d = \prod_{t=1}^{T} p_t^{-f_t}$ shows ""non-randomness"" of the document $d$		Take the $\log$:	$$	\log L_d = \sum_{t=1}^{T}f_t\log\left(\frac{1}{p_t}\right)	$$	The higher the $t$-th summand, the more the term $t$ makes the document ""non-random"".		Note that $t_i\log\left(\frac{1}{p_t}\right) = TFIDF(t, d)$."
281,2.1_language_models.ipynb,0,code,-,32,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.ipyquiz import Quiz		from tmcourse.utils import (	enable_mathjax_in_cell,	display_cv_results,	display_token_importance,	)	from tmcourse.quiz import (	quiz_conditional_probability,	quiz_chain_rule,	quiz_bigram_lm,	quiz_count_ngrams,	quiz_perplexity,	quiz_random_benchmark,	quiz_pipeline_parameter,	)	from tmcourse.demo import (	demo_generate_text_ngram,	)		from collections import Counter	from math import exp	from tabulate import tabulate	from tqdm.notebook import tqdm	from IPython.display import HTML, display"
282,2.1_language_models.ipynb,1,markdown,slide,1,# Language Models
283,2.1_language_models.ipynb,2,markdown,slide,9,# Last lesson's review	1. `spaCy`	- tokenization	- lemmatization	- NER	2. TF-IDF	- measures informativity of a term in a document	- the product of Term Frequency (TF) and Inverse Document Frequency (IDF)	3. Vectorizers in `sklearn`
284,2.1_language_models.ipynb,3,markdown,slide,7,# Plan for today	1. Probability refresher: conditional probability and chain rule	2. Definition of Language Model	3. Algorithm: $n$-gram Language Model	4. Generate text with Language Model	5. Evaluate quality of Language Model	6. Text classification with Language Models
285,2.1_language_models.ipynb,4,markdown,slide,1,# Probability refresher
286,2.1_language_models.ipynb,5,markdown,slide,10,"## Conditional probability	Probability of $B$ given $A$ is defined as	$$	\Pr(B|A) = \dfrac{\Pr(AB)}{\Pr(A)}	$$		**Interpretation**: instead of full probability space, consider the subspace where event $A$ occurs.		**Example**: Compute $\Pr(\textrm{''great''}|\textrm{''america''})$ in tweets of Donald Trump.	> How likely does Trump say ""great"" when he says ""america""?"
287,2.1_language_models.ipynb,6,code,-,3,"import json	with open(""harbour-space-text-mining-course/datasets/trump_twitter_archive/tweets.json"") as f:	tweets = json.load(f)"
288,2.1_language_models.ipynb,7,markdown,slide,1,Compute $\Pr(\textrm{''great''}|\textrm{''america''})$ using the definition of conditional probability.
289,2.1_language_models.ipynb,8,code,slide,22,"total_tweets = len(tweets)	tweets_with_america = sum([	""america"" in t[""text""].lower()	for t in tweets	])	tweets_with_america_great = sum([	""america"" in t[""text""].lower() and ""great"" in t[""text""].lower()	for t in tweets	])	# P(A)	P_america = tweets_with_america / total_tweets	# P(AB)	P_america_great = tweets_with_america_great / total_tweets	# P(B|A)	P_great_given_america = P_america_great / P_america		print(""Total tweets:"", total_tweets)	print('# times ""america"" occurs in tweets:', tweets_with_america)	print('# times ""america"" and \""great\"" occur in tweets:', tweets_with_america_great)	print('P(""america"") =', P_america)	print('P(""america"" ""great"") =', P_america_great)	print('P(""great"" | ""america"") =', P_great_given_america)"
290,2.1_language_models.ipynb,9,markdown,slide,1,Interpret the conditional probability $\Pr(B|A)$ as the probability of $B$ in the subspace where $A$ occurs
291,2.1_language_models.ipynb,10,code,slide,16,"# event A = tweet contains ""america""	# event B = tweet contains ""great""		N = 0	N_B = 0	for t in tweets:	tweet_text = t[""text""].lower()	if ""america"" not in tweet_text:	# ignore all the events where A do not occur	continue	# count total events in the subspace	N += 1	if ""great"" in tweet_text:	# count events B in the subspace	N_B += 1	print('P(""great"" | ""america"") =', N_B / N)"
292,2.1_language_models.ipynb,11,markdown,slide,1,## Colab quiz 1
293,2.1_language_models.ipynb,12,code,-,1,quiz_conditional_probability()()
294,2.1_language_models.ipynb,13,markdown,slide,2,## Chain rule	Chain rule is the successive application of the definition of conditional probability.
295,2.1_language_models.ipynb,14,markdown,slide,4,"For 3 events $A, B, C$	$$	\Pr(ABC) = \Pr(A) \cdot \Pr(BC|A)	$$"
296,2.1_language_models.ipynb,15,markdown,slide,3,"Denote $\Pr_A(\cdot) \equiv \Pr(\cdot|A)$. Again, the interpretation is: instead of full probability space, consider the subspace where $A$ occurs.		<center>$\Pr(BC|A) \equiv \Pr_A (BC) = \Pr_A(B) \cdot \Pr_A(C|B)$</center>"
297,2.1_language_models.ipynb,16,markdown,slide,4,Substituting to the first equation:	$$	\Pr(ABC) = \Pr(A) \cdot \Pr(BC|A) = \Pr(A) \cdot \Pr(B|A) \cdot \Pr(C|BA)	$$
298,2.1_language_models.ipynb,17,markdown,slide,4,"The event $A$ is not special, we can use $B$ instead:	$$	\Pr(ABC) = \Pr(B) \cdot \Pr(AC|B) = \Pr(B) \cdot \Pr(C|B) \cdot \Pr(A|BC)	$$"
299,2.1_language_models.ipynb,18,markdown,slide,4,General form of chain rule:	$$	\Pr(A_1 A_2 \dots A_n) = \Pr(A_1) \cdot \Pr(A_2 | A_1) \cdot \dots \cdot \Pr(A_n| A_{n-1} A_{n-2} \dots A_2 A_1)	$$
300,2.1_language_models.ipynb,19,markdown,slide,1,## Colab quiz 2
301,2.1_language_models.ipynb,20,code,-,1,quiz_chain_rule()()
302,2.1_language_models.ipynb,21,markdown,slide,1,# What is Language Model?
303,2.1_language_models.ipynb,22,markdown,slide,5,"**Definition 1**: a _language model_ is an algorithm that predicts (estimates) the probability of a text:		$$	\Pr(\textrm{""never gonna give you up""}) = ?	$$"
304,2.1_language_models.ipynb,23,markdown,slide,7,"**Definition 2**: a _language model_ is an algorithm that predicts (estimates) the conditional probability of the next word in a sequence:		$$	\Pr(\textrm{""up""} | \textrm{""never gonna give you""}) = ?	$$		> ""Predict the conditional probability of the next word"" is just a formal way to say ""Predict the next word"""
305,2.1_language_models.ipynb,24,markdown,slide,1,**These definitions are equivalent!**
306,2.1_language_models.ipynb,25,markdown,slide,10,"## Definition 1 implies Definition 2		We can get conditional probabilities of words from probabilities of texts, like this:	$$	\Pr(\textrm{""up""} | \textrm{""never gonna give you""}) = \dfrac{\Pr(\textrm{""never gonna give you up""})}{\Pr(\textrm{""never gonna give you""})}	$$	Or, in general	$$	\Pr(t_n | t_1 t_2 \dots t_{n-1}) = \dfrac{\Pr(t_1 t_2 \dots t_{n-1} t_n)}{\Pr(t_1 t_2 \dots t_{n-1})}	$$"
307,2.1_language_models.ipynb,26,markdown,slide,10,"## Definition 2 implies Definition 1		We can compute the probability of a sequence from conditional probabilities of words using chain rule, like this:	$$	\Pr(\textrm{""never gonna give you up""}) = \Pr(\textrm{""never""}) \cdot \Pr(\textrm{""gonna""} | \textrm{""never""}) \cdot \Pr(\textrm{""give""}| \textrm{""never gonna""}) \cdot \Pr(\textrm{""you""}| \textrm{""never gonna give""}) \cdot \Pr(\textrm{""up""} | \textrm{""never gonna give you""})	$$	Or, in general	$$	\Pr(t_1t_2\dots t_n) = \Pr(t_1) \cdot \Pr(t_2 | t_1) \cdot \Pr(t_3 | t_1 t_2) \cdot \dots \cdot \Pr(t_n | t_1 t_2 \dots t_{n-1})	$$"
308,2.1_language_models.ipynb,27,markdown,slide,1,**Q**: How to estimate the probability of a sequence?
309,2.1_language_models.ipynb,28,markdown,slide,1,**A**: Count how often the sequence occurs in data.
310,2.1_language_models.ipynb,29,markdown,slide,1,"**Q**: But the data is finite, and the number of sequences is infinite! Is that possible?"
311,2.1_language_models.ipynb,30,markdown,slide,1,"**A**: Well, we need an approximation."
312,2.1_language_models.ipynb,31,markdown,slide,8,"Assumption (Markov property): the probability of the next token depends only on $\color{red}k$ previous tokens.	$$	\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \Pr(t_i | t_{\color{red}{i-k}} t_{\color{red}{i-k+1}} \dots t_{i-1})	$$	Example for $k=3$	$$	\Pr(\textrm{""up""} | \textrm{""never gonna give you""}) = \Pr(\textrm{""up""} | \textrm{""gonna give you""})	$$"
313,2.1_language_models.ipynb,32,markdown,slide,6,**Definition**: $n$_-gram_ is a sequence of $n$ tokens.		Special cases:	- 1-gram is called _unigram_	- 2-gram is called _bigram_	- 3-gram is called _trigram_
314,2.1_language_models.ipynb,33,markdown,slide,1,**Definition**: _$n$-gram language model_ estimates the probability of a sequence assuming that each token depends on $n-1$ previous tokens.
315,2.1_language_models.ipynb,34,markdown,slide,3,Examples:	- A unigram (1-gram) language model assumes that all the tokens are independent (each token depends on 0 preceeding tokens).	- $\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \Pr(t_i)$
316,2.1_language_models.ipynb,35,markdown,slide,2,- A bigram (2-gram) language model assumes that the next token depends on the latest preceeding token.	- $\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \Pr(t_i| t_{i-1}) = \dfrac{\Pr(t_{i-1} t_i)}{\Pr(t_{i-1})}$
317,2.1_language_models.ipynb,36,markdown,slide,2,"- For a general $n$-gram language model, it is sufficient to know probabilities of $n$-grams and $n-1$-grams.	- $\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \Pr(t_i| t_{i-n + 1} \dots t_{i-1}) = \dfrac{\Pr(\overbrace{t_{i-n + 1} \dots t_i}^{n\textrm{ tokens}})}{\Pr(\underbrace{t_{i-n + 1} \dots t_{i-1}}_{n-1\textrm{ tokens}})}$"
318,2.1_language_models.ipynb,37,markdown,slide,1,## Colab quiz 3
319,2.1_language_models.ipynb,38,code,-,2,enable_mathjax_in_cell()	quiz_bigram_lm()()
320,2.1_language_models.ipynb,39,markdown,slide,1,# Algorithm: $n$-gram Language Model
321,2.1_language_models.ipynb,40,markdown,slide,3,"To build an $n$-gram language model, we need to compute probabilities of $n$-grams and $n-1$-grams.		To do so, we need to count $n$-grams and $n-1$-grams."
322,2.1_language_models.ipynb,41,code,slide,21,"def generate_n_grams(sequence, n):	# it is convenient to add padding to the beginnning and to the end of the sequence	# 'None' is the technical token	padding = [None for _ in range(n-1)]	# we extract n-grams from the padded sequence	padded_sequence = padding + sequence + padding	generated_ngrams = []	# sliding window of size n: iterate over first n-1 technical tokens, then len(sequence) ""real"" tokens	for i in range(n - 1 + len(sequence)):	# take the slice of size n starting with i-th token	generated_ngrams.append(tuple(padded_sequence[i:i+n]))		return generated_ngrams		# look at the examples	unigrams = generate_n_grams(list(""abcdef""), 1)	bigrams = generate_n_grams(list(""abcdef""), 2)	trigrams = generate_n_grams(list(""abcdef""), 3)	print(len(unigrams), ""unigrams:"", unigrams)	print(len(bigrams), ""bigrams:"", bigrams)	print(len(trigrams), ""trigrams:"", trigrams)"
323,2.1_language_models.ipynb,42,code,slide,3,"# usage:	from collections import Counter	print(Counter(generate_n_grams(list(""aabab""), 2)))"
324,2.1_language_models.ipynb,43,markdown,slide,4,Remember the formula for $n$-gram language model:	$$	\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \Pr(t_i| t_{i-n + 1} \dots t_{i-1}) = \dfrac{\Pr(\overbrace{t_{i-n + 1} \dots t_i}^{n\textrm{ tokens}})}{\Pr(\underbrace{t_{i-n + 1} \dots t_{i-1}}_{n-1\textrm{ tokens}})}	$$
325,2.1_language_models.ipynb,44,markdown,slide,3,We estimate probabilities by counts:	- $\Pr(t_{i-n} \dots t_{i-1}) = \dfrac{\textrm{count}(t_{i-n} \dots t_{i-1})}{\textrm{total # of }n\textrm{-grams}}$	- $\Pr(t_{i-n + 1} \dots t_{i-1}) = \dfrac{\textrm{count}(t_{i-n + 1} \dots t_{i-1})}{\textrm{total # of }n-1\textrm{-grams}}$
326,2.1_language_models.ipynb,45,markdown,slide,7,Since	$$\textrm{total # of }n\textrm{-grams} \approx \textrm{total # of }n-1\textrm{-grams}$$		we have	$$	\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \dfrac{\textrm{count}(t_{i-n} \dots t_{i-1})}{\textrm{count}(t_{i-n + 1} \dots t_{i-1})}	$$
327,2.1_language_models.ipynb,46,markdown,slide,6,$$	\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \dfrac{\textrm{count}(t_{i-n} \dots t_{i-1})}{\textrm{count}(t_{i-n + 1} \dots t_{i-1})}	$$	Two possible problems:	1. Zero counts in the numerator	1. Zero counts in the denominator
328,2.1_language_models.ipynb,47,markdown,slide,5,"Solution: add _smoothing_:	$$	\Pr(t_i | t_1 t_2 \dots t_{i-1}) = \dfrac{\textrm{count}(t_{i-n} \dots t_{i-1}) \color{red}{+\delta}}{\textrm{count}(t_{i-n + 1} \dots t_{i-1})\color{red}{+\delta \cdot |V|}}	$$	where $\delta$ is a small number, and $V$ is the _vocabulary_ (the set of all tokens)."
329,2.1_language_models.ipynb,48,markdown,slide,1,"**Intuition**: if we don't have enough data to estimate probabilities, any token is equiprobable."
330,2.1_language_models.ipynb,49,markdown,slide,1,## Colab demo: implementation of $n$-gram language model
331,2.1_language_models.ipynb,50,code,-,40,"from collections import Counter	from tqdm.notebook import tqdm		class NGramLanguageModel:	def __init__(self, n, delta=0.001, verbose=True):	""""""	n is the parameter of the model	Keep counters for n-grams and n-1-grams	""""""	self.n = n	self.delta = delta	self.n_grams_counter = Counter()  # store n-gram counts	self.nm1_grams_counter = Counter()  # store n-1-gram counts	self.vocab = {None}  # set of all tokens	self.verbose = verbose  # show progressbar		def fit(self, sequences):	""""""	Train the model	""""""	if self.verbose:	sequences = tqdm(sequences, desc=""fit"")	for sequence in sequences:	# update the n-grams counter	for n_gram in generate_n_grams(sequence, self.n):	self.n_grams_counter[n_gram] += 1	# update the n-1-grams counter	for nm1_gram in generate_n_grams(sequence, self.n - 1):	self.nm1_grams_counter[nm1_gram] += 1	# update the vocabulary	self.vocab |= set(sequence)		def predict_token_probability(self, sequence, token):	""""""	Return P(token | sequence)	""""""	padding = [None for i in range(self.n-1)]  # add padding	tail = (padding + sequence)[-(self.n-1):]  # get last n-1 tokens	# estimate the conditional probability using counts with smoothing	return (self.n_grams_counter[tuple(tail + [token])] + self.delta) / (self.nm1_grams_counter[tuple(tail)] + self.delta * len(self.vocab))"
332,2.1_language_models.ipynb,51,code,-,22,"# read the data	import json	with open(""harbour-space-text-mining-course/datasets/trump_twitter_archive/tweets.json"") as f:	tweets = json.load(f)		# prepare tokenizer	import spacy	nlp = spacy.load(""en"", disable=[""parser"", ""ner"", ""tagger""])		# Special tokenizer for Twitter:	#  - one token for all Twitter accounts	#  - one token for all URLs	def tokenize(text):	tokens = []	for t in nlp(text):	if t.text.startswith(""@""):	tokens.append(""<TWITTER_ACCOUNT>"")	elif t.text.startswith(""http""):	tokens.append(""<URL>"")	else:	tokens.append(t.text)	return tokens"
333,2.1_language_models.ipynb,52,code,-,5,"sequences = [	tokenize(tweet[""text""])	for tweet in tqdm(tweets, desc=""tokenize"")	if not tweet.get(""is_retweet"")	]"
334,2.1_language_models.ipynb,53,code,-,5,"lm = NGramLanguageModel(3, delta=1e-5)	lm.fit(sequences)	prefix = [""Make"", ""America""]	for token in [""great"", ""strong"", ""cool""]:	print("" "".join(prefix), token, "":"", lm.predict_token_probability(prefix, token))"
335,2.1_language_models.ipynb,54,markdown,slide,1,## Colab quiz 4
336,2.1_language_models.ipynb,55,code,-,2,enable_mathjax_in_cell()	quiz_count_ngrams()()
337,2.1_language_models.ipynb,56,markdown,slide,8,"# Generate text with Language Models		The algorithm of text generation with $n$-gram language model is recursive:	1. Input: some prefix $P$ (possibly empty). Add padding to the beginning if necessary.	1. Return $P$ if the last $n-1$ symbols of $P$ are padding symbols or $P$ is too long.	1. For each token $t \in V$, compute $\Pr(t|P)$.	1. Choose $t$ at random according to probability distribution $\Pr(t|P)$.	1. Append $t$ to $P$ and repeat."
338,2.1_language_models.ipynb,57,markdown,slide,1,## Colab demo: generate text with language model (implementation)
339,2.1_language_models.ipynb,58,code,-,29,"import numpy as np		def generate_text(language_model, prefix, seed=0, max_text_length=10):	n = language_model.n	generated_text = prefix[:]  # generated text starts with the given (possibly empty) prefix	np.random.seed(seed + len(generated_text))  # new seed for each generated word		# stopping criteria:	# - at least one word has been generated and the generated text ends with padding	# - or the generated text is too long	if (len(generated_text) >= 1 and generated_text[-1] is None) or len(generated_text) > max_text_length:	return generated_text	# the recursive step: sample a token and add it to the prefix	# tokens are stored in the .vocab attribute	all_tokens = list(language_model.vocab)	# get the probabilities for all tokens	all_token_probabilities = np.array([	language_model.predict_token_probability(generated_text, token)	for token in all_tokens	])	# sample	next_token = np.random.choice(all_tokens, size=1, p=all_token_probabilities)[0]	# generate using the updated text	return generate_text(	language_model,	generated_text + [next_token],	seed=seed,	max_text_length=max_text_length	)"
340,2.1_language_models.ipynb,59,code,-,8,"for n in (2, 3, 4, 5):	lm = NGramLanguageModel(n, delta=1e-5)	lm.fit(sequences)	print(""n={}"".format(n))	for seed in range(3):	generated_text = generate_text(lm, ""Make America"".split(), seed=seed)	print("" "".join([token for token in generated_text if token is not None]))	print(""-"" * 10)"
341,2.1_language_models.ipynb,60,markdown,slide,1,We can see that increasing $n$ doesn't help to generate good texts. Why?
342,2.1_language_models.ipynb,61,markdown,slide,1,"**Sparsity problem**: by increasing $n$ we also increase the total number of possible $n$-grams, so counts zero out."
343,2.1_language_models.ipynb,62,markdown,slide,1,## Colab demo: generate text with $n$-gram Language Model (internal details)
344,2.1_language_models.ipynb,63,code,-,1,"prefix = [""I"", ""promise""]"
345,2.1_language_models.ipynb,64,code,-,5,"# bigram model	# the text is grammatical, but incoherent	lm = NGramLanguageModel(2, delta=1e-5)	lm.fit(sequences)	demo_generate_text_ngram(lm, prefix, seed=0)"
346,2.1_language_models.ipynb,65,code,-,4,"# trigram model	lm = NGramLanguageModel(3, delta=1e-5)	lm.fit(sequences)	demo_generate_text_ngram(lm, prefix, seed=0)"
347,2.1_language_models.ipynb,66,code,-,5,"# 4-gram model	# observe what happens when count() reaches 0	lm = NGramLanguageModel(4, delta=1e-5)	lm.fit(sequences)	demo_generate_text_ngram(lm, prefix, seed=0)"
348,2.1_language_models.ipynb,67,markdown,slide,1,# Evaluate quality of Language Model
349,2.1_language_models.ipynb,68,markdown,slide,1,**Q**: Even the simplest Language Model has 2 parameters: $n$ and $\delta$. How to choose them?
350,2.1_language_models.ipynb,69,markdown,slide,1,**A**: Choose the parameters that give the best quality!
351,2.1_language_models.ipynb,70,markdown,slide,1,**Q**: How to measure quality?
352,2.1_language_models.ipynb,71,markdown,slide,6,"```	Вячэрняя прахалода.	Спякотлівы дзень, бывай.	Няхай адпачне прырода,	Бадзенеўскі родны край.	```"
353,2.1_language_models.ipynb,72,markdown,slide,1,"If you are surprised to see it on the slide, probably your inner Language Model for Belarusian language is not so good."
354,2.1_language_models.ipynb,73,markdown,slide,2,"	**Idea**: a language model is good for a given text if it is not ""surprised"" by it (it _assigns high probability_ to it)."
355,2.1_language_models.ipynb,74,markdown,slide,4,Here is the formula for probability of text $t_1 t_2 \dots t_l$ (chain rule):	$$	\Pr(t_1 t_2 \dots t_l) = \Pr(t_1) \cdot \Pr(t_2 | t_1) \cdot \dots \cdot \Pr(t_l|t_1 t_2 \dots t_{l-1})	$$
356,2.1_language_models.ipynb,75,markdown,slide,1,**Problem**: longer texts have lower probability because of larger number of terms in the product.
357,2.1_language_models.ipynb,76,markdown,slide,1,**Solution**: normalize by the number of tokens.
358,2.1_language_models.ipynb,77,markdown,slide,6,"**Definition**: _perplexity_ of text $t_1 t_2 \dots t_l$ is its inverse normalized probability:	$$	\textrm{Perplexity}(t_1 t_2 \dots t_l) = \dfrac{1}{\Pr(t_1 t_2 \dots t_l)^\frac{1}{l}}	$$		The lower the perplexity of the text, the better the Language Model is for the text."
359,2.1_language_models.ipynb,78,code,slide,9,"def perplexity(language_model, sequence):	import numpy as np		sum_logarithms = 0.0	for i in range(len(sequence)):	token = sequence[i]	prefix = sequence[:i]	sum_logarithms += np.log(language_model.predict_token_probability(prefix, token))	return np.exp(-sum_logarithms / len(sequence))"
360,2.1_language_models.ipynb,79,markdown,slide,1,## Colab quiz 5
361,2.1_language_models.ipynb,80,code,-,2,enable_mathjax_in_cell()	quiz_perplexity()()
362,2.1_language_models.ipynb,81,markdown,-,3,"Using perplexity, we can evaluate model's quality and choose the best model.		Perplexity is defined for a single text. To evaluate a model on a set of texts, we will average perplexity over this set."
363,2.1_language_models.ipynb,82,markdown,slide,1,## Colab demo: choose the best $n$-gram language model using perplexity
364,2.1_language_models.ipynb,83,code,-,27,"import random	from itertools import product		# convert tweets into sequences of tokens	sequences = [	tokenize(tweet[""text""])	for tweet in tqdm(tweets, desc=""tokenize"")	if not tweet.get(""is_retweet"")	]	random.seed(0)	random.shuffle(sequences)	# take 30000 texts to train language models	train = sequences[:30000]	# the remaining texts are left for validation	test = sequences[30000:]	results = []		# iterate over pairs of (n, delta)	for n, delta in tqdm(list(product((1, 2, 3), (1e-1, 1e-2, 1e-3, 1e-4)))):	# train language model	lm = NGramLanguageModel(n, delta=delta, verbose=False)	lm.fit(train)	# compute average perplexity on hold-out test dataset	avg_perplexity = sum(perplexity(lm, test_seq) for test_seq in test) / len(test)	results.append((n, delta, avg_perplexity))		print(tabulate(results, headers=(""n"", ""delta"", ""average perplexity""), floatfmt="".4f""))"
365,2.1_language_models.ipynb,84,markdown,-,3,"On average, bigram models beat trigram models because trigram models suffer from sparsity problem.		However, trigram models have lower perplexity on ""typical"" texts:"
366,2.1_language_models.ipynb,85,code,-,7,"typical_text = ""MAKE AMERICA GREAT AGAIN!""	best_bigram_lm = NGramLanguageModel(2, delta=0.01, verbose=False)	best_bigram_lm.fit(train)	print(""Best bigram model perplexity:"", perplexity(best_bigram_lm, tokenize(typical_text)))	best_trigram_lm = NGramLanguageModel(3, delta=0.001, verbose=False)	best_trigram_lm.fit(train)	print(""Best trigram model perplexity:"", perplexity(best_trigram_lm, tokenize(typical_text)))"
367,2.1_language_models.ipynb,86,markdown,slide,1,# Text classification with language models
368,2.1_language_models.ipynb,87,markdown,slide,4,**Classification problem (informally)**:	- We have a set of documents.	- Each document has a label (category).	- We need need to create an algorithm that predicts labels of **new similar** documents.
369,2.1_language_models.ipynb,88,markdown,slide,5,## 20 newsgroup dataset		[This is Usenet](https://en.wikipedia.org/wiki/Usenet_newsgroup)		![Usenet](https://upload.wikimedia.org/wikipedia/commons/f/f4/Usenet_servers_and_clients.svg)
370,2.1_language_models.ipynb,89,markdown,slide,14,"<center>	<b>Usenet categories</b>	<table border=""1"">	<tbody><tr>	<td>comp.graphics<br>comp.os.ms-windows.misc<br>comp.sys.ibm.pc.hardware<br>comp.sys.mac.hardware<br>comp.windows.x</td>	<td>rec.autos<br>rec.motorcycles<br>rec.sport.baseball<br>rec.sport.hockey</td>	<td>sci.crypt<br>sci.electronics<br>sci.med<br>sci.space</td>	</tr><tr>	<td>misc.forsale</td>	<td>talk.politics.misc<br>talk.politics.guns<br>talk.politics.mideast</td>	<td>talk.religion.misc<br>alt.atheism<br>soc.religion.christian</td>	</tr>	</tbody></table>	</center>"
371,2.1_language_models.ipynb,90,markdown,slide,1,## Colab demo: look at the data
372,2.1_language_models.ipynb,91,code,-,15,"from sklearn.datasets import fetch_20newsgroups		# select a subset of categories to speed up the demonstration	categories = (""sci.space"", ""rec.autos"", ""talk.politics.misc"", ""comp.graphics"")	# ignore metadata to avoid overfitting (for example, metadata may contain the name of target category)	remove = ('headers', 'footers', 'quotes')	dataset_train = fetch_20newsgroups(subset=""train"", remove=remove, categories=categories)	dataset_test = fetch_20newsgroups(subset=""test"", remove=remove, categories=categories)		# get texts and target	texts_train = dataset_train.data	y_train = dataset_train.target		texts_test = dataset_test.data	y_test = dataset_test.target"
373,2.1_language_models.ipynb,92,code,-,20,"# the easiest way to look at the data is to convert it to Pandas dataframe	import pandas as pd		# convert train	df_train = pd.DataFrame({	""text"": texts_train,	""label"": y_train,	""category"": [dataset_train.target_names[l] for l in y_train]	})		# convert test	df_test = pd.DataFrame({	""text"": texts_test,	""label"": y_test,	""category"": [dataset_test.target_names[l] for l in y_test]	})		# print the first 10 train examples	pd.set_option('display.max_colwidth', -1)  # display full texts	df_train.head(10)"
374,2.1_language_models.ipynb,93,code,-,6,"print(""train contains {} texts"".format(df_train.shape[0]))	print(""test contains {} texts"".format(df_test.shape[0]))	from collections import Counter	print()	print(""distribution of labels in train:"")	df_train.groupby([""category""])[""category""].count()"
375,2.1_language_models.ipynb,94,markdown,slide,7,## Classification with language models		**Idea**:	- train LM for each class	- prediction:	- for each language model compute perplexity	- choose the model (and the corresponding class) with the lowest perplexity
376,2.1_language_models.ipynb,95,markdown,slide,4,We will evaluate the quality of classification with accuracy score:	$$	\textrm{accuracy} = \dfrac{\textrm{number of correct predictions}}{\textrm{total number of predictions}}	$$
377,2.1_language_models.ipynb,96,code,-,1,from sklearn.metrics import accuracy_score
378,2.1_language_models.ipynb,97,markdown,slide,1,## Colab quiz 6
379,2.1_language_models.ipynb,98,code,-,1,quiz_random_benchmark()()
380,2.1_language_models.ipynb,99,markdown,slide,1,## Colab demo: classification with language models
381,2.1_language_models.ipynb,100,code,-,44,"from collections import defaultdict	from copy import deepcopy		class LanguageModelClassifier:	def __init__(self, n=4, delta=0.0001, pretrained_lm=None):	self._n = n	self._delta = delta	# optional pre-trained language model	self._pretrained_lm = pretrained_lm	# store language model for each class here	self._class2lm = dict()		def fit(self, texts, labels):	class2train = defaultdict(list)	# collect separate text datasets for each class	for text, label in zip(texts, labels):	class2train[label].append(text)	# fit language model for each class	for class_, train in class2train.items():	if self._pretrained_lm:	# if pre-trained language model is provided, copy n-gram counts from it	lm = deepcopy(self._pretrained_lm)	else:	# else train language model from scratch	lm = NGramLanguageModel(n=self._n, delta=self._delta)	lm.verbose = False	# fit and save language model	lm.fit(train)	self._class2lm[class_] = lm		def predict(self, texts):	predictions = []	for text in tqdm(texts, desc=""predict""):	class2perplexity = dict()	# compute perplexity for each language model	for class_, lm in self._class2lm.items():	class2perplexity[class_] = perplexity(lm, text)	# choose the class with the lowest perplexity	class_with_lowest_perplexity = min(	class2perplexity.items(),	key=lambda x: x[1]	)[0]	predictions.append(class_with_lowest_perplexity)	return predictions"
382,2.1_language_models.ipynb,101,code,-,5,"# tokenize the texts	# treat any alphanumeric sequence as a token	import re	tokenized_texts_train = [re.split(r'\W+', t.lower().strip()) for t in texts_train]	tokenized_texts_test = [re.split(r'\W+', t.lower().strip()) for t in texts_test]"
383,2.1_language_models.ipynb,102,code,-,4,"# without pretraining	clf = LanguageModelClassifier(n=2)	clf.fit(tokenized_texts_train, y_train)	print(accuracy_score(y_test, clf.predict(tokenized_texts_test)))"
384,2.1_language_models.ipynb,103,markdown,slide,2,"## Pre-trained Language Models	We know that language models suffer from sparsity problem. In out classification problem, there are only 2236 training examples, this may be a problem."
385,2.1_language_models.ipynb,104,markdown,slide,3,"**Idea**: we can ""initialize"" language model with pre-training on large unlabelled dataset.		> So we don't have to train each language model for each class from scratch, we will use some initial counts from pre-trained model."
386,2.1_language_models.ipynb,105,markdown,slide,1,## Colab demo: classification with pre-trained language model
387,2.1_language_models.ipynb,106,code,-,14,"# Let's try language model pre-trained on Reddit comments	# The code of pre-training is here: https://colab.research.google.com/drive/1DAW26wL5hxykxmLZ9W9ozk0_Otm3LpNI?usp=sharing	import pickle	with open(""harbour-space-text-mining-course/models/reddit_lm.pickle"", ""rb"") as f:	print(""load"")	pretrained_lm = pickle.load(f)		clf2 = LanguageModelClassifier(pretrained_lm=pretrained_lm)	print(""fit"")	# `fit` is relatively slow because of deepcopy	clf2.fit(tokenized_texts_train, y_train)		print(""predict"")	print(accuracy_score(y_test, clf2.predict(tokenized_texts_test)))"
388,2.1_language_models.ipynb,107,markdown,slide,17,"# Summary		1. Language Model predicts	- Probability of text	- Probability of the next word	1. $n$-gram Language Model	- The next word depends only on $n-1$ previous words	1. Generate text with Language Model	- Predict probabilities for all words	- Sample	1. Perplexity measures the quality of Language Model	- A good model is not ""surprised""	1. Text classification with Language Models:	- Train LM for each class	- Choose LM with smallest perplexity	1. **We can use pre-trained language models for classification.**	- This idea is used in ULMFiT algorithm (stay tuned)."
389,2.1_language_models.ipynb,108,markdown,slide,4,# Recommended resources	- [📖 Language Models (nlpforhackers.io)](https://nlpforhackers.io/language-models/)	- [📖 Home page for 20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/)	- [📖 20 newsgroups dataset in sklearn](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)
390,2.1_language_models.ipynb,109,markdown,slide,20,"# [OPTIONAL] Perplexity as loss function	Take the logarithm of the probability of text:	$$	\log \Pr(t_1 t_2 \dots t_l) = \log \Pr(t_1) + \log \Pr(t_2 | t_1) + \dots + \log \Pr(t_l|t_1 t_2 \dots t_{l-1})	$$	Take average instead of sum:	$$	\dfrac{\log \Pr(t_1 t_2 \dots t_l)}{l} = \dfrac{\log \Pr(t_1) + \log \Pr(t_2 | t_1) + \dots + \log \Pr(t_l|t_1 t_2 \dots t_{l-1})}{l}	$$	The expression above can be interpreted as loss function (it is similar to cross entropy loss).		We are interested in minimization of the loss function, so take the previous expression with the opposite sign:	$$	\textrm{LossFunction}(t_1 t_2 \dots t_l) = -\frac{1}{l}(\log \Pr(t_1) + \log \Pr(t_2 | t_1) + \dots + \log \Pr(t_l|t_1 t_2 \dots t_{l-1}))	$$		$\exp\left(\textrm{LossFunction}(t_1 t_2 \dots t_l)\right)$ is called perplexity:	$$	\textrm{Perplexity}(t_1 t_2 \dots t_l) = \dfrac{1}{\Pr(t_1 t_2 \dots t_l)^\frac{1}{l}}	$$"
391,2.2_text_classification.ipynb,0,code,-,27,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.ipyquiz import Quiz		from tmcourse.utils import (	enable_mathjax_in_cell,	display_cv_results,	display_token_importance,	)	from tmcourse.quiz import (	quiz_pipeline_parameter,	)		from tmcourse.demo import (	demo_gradient_descent,	)	import numpy as np	from collections import Counter	from math import exp	from tabulate import tabulate	from tqdm.notebook import tqdm	from IPython.display import HTML, display"
392,2.2_text_classification.ipynb,1,markdown,slide,1,<center><h1>Text classification</h1></center>
393,2.2_text_classification.ipynb,2,markdown,slide,1,# Machine Learning basics refresher
394,2.2_text_classification.ipynb,3,markdown,slide,2,## What is Machine Learning?	![Machine Learning vs. Computer Science](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ml_vs_cs.png)
395,2.2_text_classification.ipynb,4,markdown,slide,5,"## Definitions	- We have _data_ about $N$ _objects_ ($1, 2, \dots, N$).	- Each object $n$ has _features_ $\mathbf{x}_n$: a list of numbers.	- Each object $n$ has _label_ $y_n$: a number (integer or real number).	"
396,2.2_text_classification.ipynb,5,markdown,slide,4,In the context of Text Mining:	- Object: a text.	- Features: the vector of TF-IDFs.	- Label: the category of the text (integer).
397,2.2_text_classification.ipynb,6,markdown,slide,3,Notation:	- We store features $\mathbf{x}_n$ in a matrix $\mathbf{X}$.	- We store labels $y_n$ in a vector $\mathbf{y}$.
398,2.2_text_classification.ipynb,7,markdown,slide,6,"## Machine Learning Algorithm (informally)		- Input: data $\mathbf{X}, \mathbf{y}$ about objects $1, 2, \dots, N$.	- Output: _rules_ that predict answers on **new similar** objects.		> Usually, rules are called **models**."
399,2.2_text_classification.ipynb,8,markdown,slide,5,"Example: least squares.		Assumption: $y_n \approx a x_n + b$ for some $a, b$.		The rule (the model): for given $x$, muptiply by $a$ and add $b$."
400,2.2_text_classification.ipynb,9,code,slide,21,"import numpy as np	import matplotlib.pyplot as plt		plt.rcParams.update({'font.size': 15})		np.random.seed(0)	x = np.linspace(-1, 2, 20)	y_true = np.cosh(x)	y = y_true + np.random.randn(x.shape[0]) * 0.2	linear_approximation = np.poly1d(np.polyfit(x, y, 1))	plt.figure(figsize=(10, 6))	plt.plot(	x, y_true, '--',	x, y, 'o',	x, linear_approximation(x), '-'	)	plt.legend([	r""True dependency"",	r""Observations $y_n$"",	r""Linear model: $y_n \approx a x_n + b$""])	plt.show()"
401,2.2_text_classification.ipynb,10,markdown,slide,3,## Linear models		A _linear model_ is the Machine Learning algorithm that predicts $y_i$ using a linear function of features $w_0 + \sum_j w_j x_{ij}$.
402,2.2_text_classification.ipynb,11,markdown,-,3,"	<!--@slideshow fragment-->	Coefficients $w_0, w_1, \dots$ are called _parameters_. They can be written as a vector $\mathbf{w} = (w_0, w_1, \dots)$."
403,2.2_text_classification.ipynb,12,markdown,-,5,"	<!--@slideshow fragment-->	Even for one-dimensional data $x$, we can have many features: $x, x^2, x^3, \dots, x^d$ and the corresponding weights.		The number $d$ (the degree of polynomial) is a _hyperparameter_. It should be set before training."
404,2.2_text_classification.ipynb,13,code,slide,20,"np.random.seed(0)	x_min = -1	x_max = 2	x = np.linspace(x_min, x_max, 20)	y_true = np.cosh(x)	y = y_true + np.random.randn(x.shape[0]) * 0.2	approximation_2 = np.poly1d(np.polyfit(x, y, 2))	x_precise = np.linspace(x_min, x_max, 1000)	plt.figure(figsize=(10, 6))	plt.plot(	x, y_true, '--',	x, y, 'o',	x_precise, approximation_2(x_precise), '-'	)	plt.legend([	r""True dependency"",	r""Observations $y_n$"",	r""Model: $y_n \approx w_0 + w_1 x_n + w_2 x_n^2$""	])	plt.show()"
405,2.2_text_classification.ipynb,14,markdown,slide,1,## Loss functions and gradient descent
406,2.2_text_classification.ipynb,15,markdown,slide,1,"How to choose parameters $w_0, w_1, \dots$ of the linear model $h(x)$?"
407,2.2_text_classification.ipynb,16,markdown,slide,1,We need to measure how well the model $h(x)$ describes the data.
408,2.2_text_classification.ipynb,17,markdown,slide,4,"We can compute the error for each object. For example	$$	\ell(x_n, y_n) = \frac{1}{2}(y_n-h(x_n))^2	$$"
409,2.2_text_classification.ipynb,18,markdown,slide,4,"The average error is called _loss function_. For example, this is Mean Squared Error:	$$	L(\mathbf{X}, \mathbf{y}, h) = \dfrac{1}{N}\sum_{1 \leq n \leq N}\frac{1}{2}(y_n-h(x_n))^2	$$"
410,2.2_text_classification.ipynb,19,code,slide,34,"	plt.rcParams.update({'font.size': 10})	x_smooth = np.linspace(-1, 2, 500)	h1 = linear_approximation		plt.figure(figsize=(15, 8))	plt.plot(x, y, 'o', x_smooth, h1(x_smooth))	losses = []	for i, item in enumerate(zip(x, y)):	n = i+1	xi, yi = item	l = 0.5 * (yi - h1(xi))**2	losses.append(l)	plt.plot([xi, xi], [yi, h1(xi)], c=""r"")	plt.annotate(r""$y_{{{}}} = {:.2f}$"".format(n, yi), (xi, yi), ha='center')	plt.annotate(r""$h(x_{{{}}}) = {:.2f}$"".format(n, h1(xi)), (xi, h1(xi)), ha='center')	plt.annotate(""{:.2f}"".format(l), (xi, 0.5*(yi + h1(xi))), c=""r"")		plt.rcParams.update({'font.size': 15})	plt.legend(	[	r""Observations $y_i$"",	r""Model: $y_i \approx w_0 + w_1 x_i = h(x_i)$""	]	)		plt.show()		enable_mathjax_in_cell()	display(	HTML(	r""$L(\mathbf{X}, \mathbf{y}, h) = \dfrac{1}{N}\sum_{1 \leq n \leq N}\frac{1}{2}(y_n-h(x_n))^2="" + r""{:.2f}$"".format(np.mean(losses))	)	)"
411,2.2_text_classification.ipynb,20,markdown,slide,3,"Loss function $L(\mathbf{X}, \mathbf{y}, h)$ depends on parameters of the linear model $\mathbf{w} = w_0, w_1, \dots$.		So we can minimize it w.r.t. $\mathbf{w}$."
412,2.2_text_classification.ipynb,21,markdown,slide,11,"We can do it using _gradient descent_.		**Idea**: if we slightly change the parameters by $\Delta \mathbf{w} = (\Delta w_0, \Delta w_1, \dots)$, then	$$	\Delta L \approx \dfrac{\partial L}{\partial w_0}\Delta w_0 + \dfrac{\partial L}{\partial w_1}\Delta w_1 + \dots = \nabla L \cdot \Delta \mathbf{w}	$$	where $\nabla L$ is the _gradient_.		We want $\Delta L < 0$ (since we are minimizing $L$), so $\nabla L \cdot \Delta \mathbf{w} < 0$.		It means that we should take the step $\Delta \mathbf{w}$ in the direction _opposite_ to the gradient (hence the name ""gradient _descent_"")."
413,2.2_text_classification.ipynb,22,markdown,slide,3,There are two versions of gradient descent:	- _Full gradent descent_: minimization of the total sum $\dfrac{1}{N}\sum_{1 \leq n \leq N}\frac{1}{2}(y_n-h(x_n))^2$.	- _Stochastic gradient descent_: iterate over objects randomly and for each iteration minimize $\frac{1}{2}(y_n-h(x_n))^2$.
414,2.2_text_classification.ipynb,23,markdown,slide,1,## Colab demo: gradient descent
415,2.2_text_classification.ipynb,24,code,-,1,"demo_gradient_descent(lambda x: x**4 - x**3 - x**2 + 1, theta_0=-1, learning_rate=0.5)  # local minima"
416,2.2_text_classification.ipynb,25,code,-,1,"demo_gradient_descent(lambda x: x**2, theta_0=-1, learning_rate=0.5)  # divergence"
417,2.2_text_classification.ipynb,26,markdown,slide,3,"## Overfitting and cross-validation		A Machine Learning Algorithm may fit the training data too closely, like this polynomial of degree 15:"
418,2.2_text_classification.ipynb,27,code,slide,22,"import numpy as np	import matplotlib.pyplot as plt		np.random.seed(0)	x_min = -1	x_max = 2	x = np.linspace(x_min, x_max, 20)	y_true = np.cosh(x)	y = y_true + np.random.randn(x.shape[0]) * 0.2	approximation = np.poly1d(np.polyfit(x, y, 15))	x_precise = np.linspace(x_min, x_max, 1000)	plt.figure(figsize=(10, 6))	plt.plot(	x, y_true, '--',	x, y, 'o',	x_precise, approximation(x_precise), '-'	)	plt.legend([	r""True dependency"",	r""Observations $y_i$"",	r""Approximation: $y_i \approx w_0 + w_1 x_i + w_2 x_i^2 + \dots + w_{15} x_i^{15}$""])	plt.show()"
419,2.2_text_classification.ipynb,28,markdown,slide,1,Such an algorithm will unlikely predict well on new data.
420,2.2_text_classification.ipynb,29,markdown,slide,9,"**Idea**: pretend that we actually have ""new"" data.	- Split the data into two parts: training and validation.	- Train on the training part	- Compute the loss function $L$ on the validation part.	- Repeat $k$ times for different _folds_ (see the picture).		This is called _$k$-fold cross-validation_.		![alt text](https://upload.wikimedia.org/wikipedia/commons/b/b5/K-fold_cross_validation_EN.svg)"
421,2.2_text_classification.ipynb,30,markdown,slide,3,## Logistic regression		So far we discussed real-valued labels (regression). What if labels are discrete (classification)?
422,2.2_text_classification.ipynb,31,markdown,slide,4,	Example: binary classification (classes 0 and 1).		Predict probability of class 1: $\Pr(1 | \mathbf{x}_n) = \sigma(w_0 + w_1 x_{n1} + w_2 x_{n2} + \dots)$
423,2.2_text_classification.ipynb,32,code,slide,8,"import numpy as np	import matplotlib.pyplot as plt		x = np.linspace(-5, 5, 100)	plt.plot(x, 1 / (1 + np.exp(-x)))		plt.legend([r""$\sigma(x) = \dfrac{1}{1 + \exp(-x)}$"",])	plt.show()"
424,2.2_text_classification.ipynb,33,markdown,slide,6,"Loss function: negative log-likelihood (maximize likelihood $\Longleftrightarrow$ minimize negative log-likelihood)	$$	L(\mathbf{X}, \mathbf{y}, h) = -\dfrac{1}{N}\sum_{1 \leq n \leq N} \log\left( \Pr(1|\mathbf{x}_n)^{y_n} \cdot (1 - \Pr(1|\mathbf{x}_n))^{1 - y_n} \right)	$$		Minimize the loss function using _stochastic gradient descent_."
425,2.2_text_classification.ipynb,34,markdown,slide,1,# Colab demo: 20 newsgroups
426,2.2_text_classification.ipynb,35,markdown,-,3,## Get the data	- We work with 4 categories instead of 20 to speed up the demonstration	- We remove metadata because it may contain true labels
427,2.2_text_classification.ipynb,36,code,-,16,"from sklearn.datasets import fetch_20newsgroups		dataset = fetch_20newsgroups(	subset=""all"",	remove=('headers', 'footers', 'quotes'),	categories=(""sci.space"", ""rec.autos"", ""talk.politics.misc"", ""comp.graphics"")	)		texts = dataset.data	y = dataset.target		# look at the data	from pprint import pprint	pprint(texts[0])	print(""Label:"", y[0])	print(""Category:"", dataset.target_names[y[0]])"
428,2.2_text_classification.ipynb,37,markdown,-,7,"## Split into train and test		To split the data into training and testing parts, use `sklearn.model_selection.train_test_split`.	The important parameters:	- `shuffle`: is `True` by default.	- `random_state` (integer): always set it to have reproducible results!	- `test_size`: ratio of samples in the testing part."
429,2.2_text_classification.ipynb,38,code,-,10,"# the example of train_test_split	# it works with any number of arrays of any types	from sklearn.model_selection import train_test_split	data1 = [1, 2, 3, 4, 5, 6]	data2 = list(""abcdef"")	data3 = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]	train_data1, test_data1, train_data2, test_data2, train_data3, test_data3 = \	train_test_split(data1, data2, data3, test_size=0.5, random_state=1)	print(""Training parts:"", train_data1, train_data2, train_data3)	print(""Testing parts:"", test_data1, test_data2, test_data3)"
430,2.2_text_classification.ipynb,39,code,-,3,"# split texts and labels into train and test	texts_train, texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.5, random_state=1)	len(texts_train), len(texts_test)"
431,2.2_text_classification.ipynb,40,markdown,-,4,## Convert texts to vectors	Use `TfidfVectorizer` to convert texts into vectors	- First `fit()` it on the training data.	- Then `transform()` both training and testing data.
432,2.2_text_classification.ipynb,41,code,-,5,from sklearn.feature_extraction.text import TfidfVectorizer	vec = TfidfVectorizer()	vec.fit(texts_train)	X_train = vec.transform(texts_train)	X_test = vec.transform(texts_test)
433,2.2_text_classification.ipynb,42,code,-,1,"X_train.shape, X_test.shape"
434,2.2_text_classification.ipynb,43,markdown,-,6,"## Train the classifier	We use `SGDClassifier`:	- `loss=""log""` means that we train Logistic Regression	- `alpha=0.0001` is the learning rate of stochastic gradient descent	- `max_iter=50` is the maximum number of passes over the training data	- `random_state=0` is the random seed for shuffling"
435,2.2_text_classification.ipynb,44,code,-,6,"from sklearn.linear_model import SGDClassifier  # SGD == Stochastic Gradient Descent	clf = SGDClassifier(alpha=.0001, max_iter=50, loss=""log"", random_state=0)		clf.fit(X_train, y_train)	from sklearn.metrics import accuracy_score	print(""Accuracy on train:"", accuracy_score(y_train, clf.predict(X_train)))"
436,2.2_text_classification.ipynb,45,markdown,-,5,"## Estimate accuracy using cross-validation	To perform cross-validation, we use `sklearn.model_selection.cross_val_score`.	- It accepts the classifier object, the matrix of features $X$ and the vector of labels $y$.	- The parameter `cv=3` means that we use 3-fold cross-validation.	- The `cross_val_score` function automatically split the data (both $X$ and $y$) into folds, trains the classifier and computes its quality. By default, for classifiers accuracy is computed."
437,2.2_text_classification.ipynb,46,code,-,5,"from sklearn.model_selection import cross_val_score	cv_scores = cross_val_score(clf, X_train, y_train, cv=3)	import numpy as np	print(f""cv_scores: {cv_scores}"")	print(""Average accuracy on cross-validaion:"", np.mean(cv_scores))"
438,2.2_text_classification.ipynb,47,markdown,-,8,"## Combine everything into pipeline	For now, we have to train the classifier in two steps:	- First, vectorize the texts and get the matrix $X$.	- After that, use this matrix for classification.		It is convenient to combine these steps into so-called _Pipeline_ (not to be confused with NLP pipeline from the Lecture 1!).		`sklearn.pipeline.Pipeline` allows to create a chain of vectorizers and classifiers."
439,2.2_text_classification.ipynb,48,code,-,18,"from sklearn.pipeline import Pipeline		# a Pipeline is created from the list of 2-tuples	# each tuple has the name (""vec"" or ""clf"") and the corresponding class	pipeline = Pipeline([	(""vec"", vec),	(""clf"", clf),	])		# pipeline.fit(texts_train, y_train) performs the following computations:	# 1) vec.fit(texts_train)	# 2) X_train = vec.transform(texts_train)	# 3) clf.fit(X_train, y_train)	pipeline.fit(texts_train, y_train)	print(""Accuracy on train:"", accuracy_score(y_train, pipeline.predict(texts_train)))		cv_scores = cross_val_score(pipeline, texts_train, y_train, cv=3)	print(""Average accuracy on cross-validaion:"", np.mean(cv_scores))"
440,2.2_text_classification.ipynb,49,markdown,-,2,## Choose the best parameters for pipeline	The advantage of pipelines is that we can select best parameters (using cross-validation) for both vectorizer and classifier.
441,2.2_text_classification.ipynb,50,code,-,20,"from sklearn.model_selection import GridSearchCV		# This is where we need the names of stages in the pipeline: ""vec"" and ""clf""	# We will try different parameters for both vectorizer and classifier	param_grid = {	""vec__stop_words"": [None, ""english""],	""vec__ngram_range"": [(1, 1), (1, 2)],	""clf__alpha"": [1e-4, 1e-3, 1e-2],	}		# GridSearchCV does the following	#  1) It computes all possible combinations of parameters from `param_grid`	#  2) For each combination of parameters, perform k-fold cross-validation (k=3 in our case)	#  3) Choose parameters that give the highest accuracy on cross-validation	pipeline_grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3)	pipeline_grid_search.fit(texts_train, y_train)		# The function `display_cv_results` is not a sklearn function	# I implemented it for your convenience	display_cv_results(pipeline_grid_search)"
442,2.2_text_classification.ipynb,51,markdown,-,7,"## Evaluate accuracy on test		**After** we have chosen the best parameters for both vectorizer and classifier, we can compute the accuracy on the testing data.		**NB!** If you compute accuracy on the testing data _many_ times, you overfit on the testing data. **Always make decisions based on cross-validation.**		![alt text](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
443,2.2_text_classification.ipynb,52,code,-,1,"accuracy_score(y_test, pipeline_grid_search.predict(texts_test))"
444,2.2_text_classification.ipynb,53,markdown,-,1,"As we can see, the final accuracy on test is close to the accuracy on cross-validation. It is a good sign: no overfitting, and the accuracy is quite high."
445,2.2_text_classification.ipynb,54,markdown,-,3,"## Find important tokens		**Idea**: a token is important, if its _removal_ significantly changes predictions."
446,2.2_text_classification.ipynb,55,code,-,32,"def compute_token_importance(clf, text):	from scipy.linalg import norm	# a simple tokenizer: split by whitespace	# not necessarily the same tokenizer that the vectorizer uses!	import re	tokens = re.split(r'\W+', text.strip())	initial_text = "" "".join(tokens)	# predict the probabilities of all classes for the initial text	initial_class_distribution = clf.predict_proba([initial_text])[0]	token_importances = []	for i, token in enumerate(tokens):	masked_tokens = tokens[:]	# mask (remove) one token	masked_tokens[i] = ""<UNKNOWN>""	masked_text = "" "".join(masked_tokens)	# classify the text with masked token	masked_class_distribution = clf.predict_proba([masked_text])[0]	# token importance is the distance between	# the initial distribution of classes	# and the distribution after removal of the token	token_importance = norm(initial_class_distribution - masked_class_distribution)	token_importances.append((token, token_importance))	return token_importances		# remember the initial categories in the dataset	print(""categories:"", "", "".join(dataset.target_names))	for text, label in zip(texts_train[:20], y_train[:20]):	display(HTML(""<hr>""))	display(HTML(dataset.target_names[label]))  # uncomment this line to see the true label	token_importances = compute_token_importance(pipeline_grid_search, text)	display_token_importance(token_importances)	display(HTML(""<hr>""))"
447,2.2_text_classification.ipynb,56,markdown,-,1,# Coding session
448,2.2_text_classification.ipynb,57,markdown,-,1,## Get the data
449,2.2_text_classification.ipynb,58,code,-,2,"import pandas as pd	df = pd.read_csv(""harbour-space-text-mining-course/datasets/content_classification.csv"", header=0, names=[""text"", ""label""])"
450,2.2_text_classification.ipynb,59,markdown,-,3,## Exercise 1. Preliminary analysis	1. How many texts?	2. How many distinct labels?
451,2.2_text_classification.ipynb,60,code,-,1,# YOUR CODE HERE
452,2.2_text_classification.ipynb,61,markdown,-,3,"## Exercise 2. Get numeric labels		In this dataset, each text has a human-readable label instead of numeric label. You need to convert the labels into a vector of numbers (the vector $y$ in our notation). Use `sklearn.preprocessing.LabelEncoder` (see the example below)."
453,2.2_text_classification.ipynb,62,code,-,7,"# example of LabelEncoder usage	from sklearn.preprocessing import LabelEncoder	le = LabelEncoder()	le.fit([""paris"", ""paris"", ""tokyo"", ""amsterdam""])	print(f""le.classes_: {le.classes_}"")	print(le.transform([""tokyo"", ""tokyo"", ""paris""]))	print(list(le.inverse_transform([2, 2, 1])))"
454,2.2_text_classification.ipynb,63,code,-,5,from sklearn.preprocessing import LabelEncoder		# YOUR CODE HERE: create label encoder	# YOUR CODE HERE: fit label encoder	y = # YOUR CODE HERE: transform the labels
455,2.2_text_classification.ipynb,64,markdown,-,2,## Exercise 3. Split into train and test	Use `sklearn.model_selection.train_test_split` with parameters `random_state=1` and `test_size=0.25`.
456,2.2_text_classification.ipynb,65,code,-,2,"from sklearn.model_selection import train_test_split	texts_train, texts_test, y_train, y_test = # YOUR CODE HERE: split into train/test"
457,2.2_text_classification.ipynb,66,markdown,-,5,"## Exercise 4. Train and evaluate sklearn.pipeline.Pipeline		- Combine `TfidfVectorizer` and `SGDClassifier` into `Pipeline`.	- For `SGDClassifier`, use `alpha=.001, max_iter=50, loss=""log"", random_state=0`	- Use `cv=3` for cross-val score"
458,2.2_text_classification.ipynb,67,code,-,10,"from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.linear_model import SGDClassifier	from sklearn.pipeline import Pipeline	from sklearn.model_selection import cross_val_score		pipeline = # YOUR CODE HERE: create Pipeline		cv_scores = # YOUR CODE HERE: compute cv_scores using cross_val_score	import numpy as np	print(""Average accuracy on cross-validaion:"", np.mean(cv_scores))"
459,2.2_text_classification.ipynb,68,markdown,-,6,"## Exercise 5. Choose the best parameters of the pipeline	- Use `cv=3` for `GridSearchCV`	- For the vectorizer, try `stop_words=None` and `stop_words=""english""`.	- For the vectorizer, try `ngram_range=(1, 1)` and `ngram_range=(1, 2)`.	- For the classifier, try `alpha=1e-4`, `alpha=1e-3` and `alpha=1e-2`	- Compute `cross_val_score` for the pipeline found by `GridSearchCV`. Is it different from the `cross_val_score` from the Exercise 4?"
460,2.2_text_classification.ipynb,69,code,-,9,"from sklearn.model_selection import GridSearchCV		param_grid = # YOUR CODE HERE: define the grid for all the parameters	pipeline_grid_search = # YOUR CODE HERE: choose the best pipeline using GridSearchCV	# YOUR CODE HERE: fit the pipeline		display_cv_results(pipeline_grid_search)	cv_scores = cross_val_score(pipeline_grid_search, texts_train, y_train, cv=3)	print(""Average accuracy on cross-validaion:"", np.mean(cv_scores))"
461,2.2_text_classification.ipynb,70,markdown,-,2,## Exercise 6. Evaluate accuracy on test	Compare the accuracy on test with the accuracy estimated by `cross_val_score` in Exercise 5. Are they close to each other? Is it good?
462,2.2_text_classification.ipynb,71,code,-,1,# YOUR CODE HERE: compute the accuracy on test
463,2.2_text_classification.ipynb,72,markdown,-,3,## Exercise 7. Find important tokens	- Look at the important tokens for the first 10-20 texts of the test datasets. Do they correspond to the true labels?	- Use the method `.inverse_transform()` of `LabelEncoder` to get label name from the numeric value
464,2.2_text_classification.ipynb,73,code,-,7,"for text, label in zip(texts_test[:20], y_test[:20]):	display(HTML(""<hr>""))	true_label = # YOUR CODE HERE: get the human-readable label	display(HTML(true_label))	token_importances = # YOUR CODE HERE: compute token inportances	display_token_importance(token_importances)	display(HTML(""<hr>""))"
465,2_SUPPL_train_lm_reddit.ipynb,0,markdown,-,1,# Build a language model from reddit comments
466,2_SUPPL_train_lm_reddit.ipynb,1,code,-,52,"# the implementation of LM from the lecture	from collections import Counter	from tqdm.notebook import tqdm		class NGramLanguageModel:	def __init__(self, n, delta=0.001, verbose=True):	""""""	n is the parameter of the model	Keep counters for n-grams and n-1-grams	""""""	self.n = n	self.delta = delta	self.n_grams_counter = Counter()  # store n-gram counts	self.nm1_grams_counter = Counter()  # store n-1-gram counts	self.vocab = {None}  # set of all tokens	self.verbose = verbose  # show progressbar		def fit(self, sequences):	""""""	Train the model	""""""	if self.verbose:	sequences = tqdm(sequences, desc=""fit"")	for sequence in sequences:	# update the n-grams counter	for n_gram in generate_n_grams(sequence, self.n):	self.n_grams_counter[n_gram] += 1	# update the n-1-grams counter	for nm1_gram in generate_n_grams(sequence, self.n - 1):	self.nm1_grams_counter[nm1_gram] += 1	# update the vocabulary	self.vocab |= set(sequence)		def predict_token_probability(self, sequence, token):	""""""	Return P(token | sequence)	""""""	padding = [None for i in range(self.n-1)]  # add padding	tail = (padding + sequence)[-(self.n-1):]  # get last n-1 tokens	# estimate the conditional probability using counts with smoothing	return (self.n_grams_counter[tuple(tail + [token])] + self.delta) / (self.nm1_grams_counter[tuple(tail)] + self.delta * len(self.vocab))		def generate_n_grams(sequence, n):	padding = [None for _ in range(n-1)]  # 'None' is a technical token	padded_sequence = padding + sequence + padding  # we will extract n-grams from the padded sequence	generated_ngrams = []	# sliding window of size n: iterate over first n-1 technical tokens, then len(sequence) ""real"" tokens	for i in range(n - 1 + len(sequence)):	# take the slice of size n starting with i-th token	generated_ngrams.append(tuple(padded_sequence[i:i+n]))		return generated_ngrams"
467,2_SUPPL_train_lm_reddit.ipynb,2,code,-,2,# get the data	!git clone https://github.com/linanqiu/reddit-dataset.git
468,2_SUPPL_train_lm_reddit.ipynb,3,code,-,9,"# read the data	import pandas as pd	from glob import glob		comments = []	for csv_file in tqdm(glob(""./reddit-dataset/*.csv"")):	subreddit_comments = [c.split() for c in pd.read_csv(csv_file)[""0""] if type(c) == str]	comments += subreddit_comments	print(""Total comments:"", len(comments))"
469,2_SUPPL_train_lm_reddit.ipynb,4,code,-,7,"# train and save a bigram LM	reddit_comments_lm = NGramLanguageModel(2, delta=1e-5)	reddit_comments_lm.fit(comments)		import pickle	with open(""reddit_lm.pickle"", ""wb"") as f:	pickle.dump(reddit_comments_lm, f)"
470,3_SUPPL_choose_best_num_topics.ipynb,0,code,-,5,!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')	from tqdm.notebook import tqdm
471,3_SUPPL_choose_best_num_topics.ipynb,1,code,-,10,"import json	with open(""harbour-space-text-mining-course/datasets/trump_twitter_archive/tweets.json"") as f:	tweets = json.load(f)	import spacy		nlp = spacy.load(""en"", disable=['parser', 'ner'])	def tokens(s):	return [t.lemma_ for t in nlp(s) if t.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']]		tweet_documents = [tokens(t[""text""]) for t in tqdm(tweets)]"
472,3_SUPPL_choose_best_num_topics.ipynb,2,code,-,7,"from gensim.corpora import Dictionary	from gensim.models import LdaModel, CoherenceModel		tweet_dictionary = Dictionary(tweet_documents)	tweet_dictionary.filter_extremes(no_below=10, no_above=0.5)	tweet_dictionary[0]  # hack to initialize tweet_dictionary	tweet_corpus = [tweet_dictionary.doc2bow(tweet) for tweet in tweet_documents]"
473,3_SUPPL_choose_best_num_topics.ipynb,3,code,-,11,"import matplotlib.pyplot as plt	num_topics_grid = list(range(2, 21, 2))	coherences = []	for num_topics in tqdm(num_topics_grid):	lda = LdaModel(tweet_corpus, id2word=tweet_dictionary.id2token, num_topics=num_topics, passes=20)	# evaluate coherence	cm = CoherenceModel(model=lda, texts=tweet_documents, dictionary=tweet_dictionary, coherence='c_v', topn=10)	coherences.append(cm.get_coherence())		plt.plot(num_topics_grid, coherences)	plt.show()"
474,3_topic_modeling.ipynb,0,code,-,28,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.ipyquiz import Quiz		from tmcourse.utils import (	enable_mathjax_in_cell,	plot_confusion_matrix,	show_vectorizer_largest_components,	)	from tmcourse.quiz import (	quiz_kmeans,	quiz_estimate_clustering_quality,	quiz_nmf,	quiz_coherence,	)	from tmcourse.demo import (	demo_kmeans,	)		from tqdm.notebook import tqdm	from tabulate import tabulate	import matplotlib.pyplot as plt	import numpy as np"
475,3_topic_modeling.ipynb,1,markdown,slide,1,<center><h1>Topic Modeling</h1></center>
476,3_topic_modeling.ipynb,2,markdown,slide,13,# Last lesson's review	1. Machine Learning basics	- Data + Answers $\rightarrow$ Rules (Model)	- Rules predict answers on **new similar** objects	- Linear models	- Loss function	- Gradient descent	- Overfitting	- Cross-validation	2. Text classification	- `TfidfVectorizer`	- `Pipeline`	- Important tokens
477,3_topic_modeling.ipynb,3,markdown,slide,15,"# Plan for today	1. Motivation: automatically find groups of related documents	1. Clustering. $k$-means algorithm.	1. Clustering of TF-IDF vectors. Topics.	1. **Coding session 1**: $k$-means.	1. Non-negative matrix factorization (NMF).	1. **Coding session 2**: NMF.	1. Probability refresher: total probability, conditional independency.	1. Version of NMF: Probabilistic Latent Semantic Allocation (PLSA).	1. PLSA algorithm.	1. Latent Dirichlet Allocation (LDA): ""smooth"" version of PLSA.	1. Python libraries: gensim, pyLDAvis	1. Evaluate quality of topics.	1. Apply LDA model.	1. **Coding session 3**: LDA."
478,3_topic_modeling.ipynb,4,markdown,slide,5,# Motivation		- We know how to solve text classification problem using Machine Learning.	- But what if we don't know the labels?	- Can we somehow extract labels from the data?
479,3_topic_modeling.ipynb,5,markdown,slide,1,# Clustering
480,3_topic_modeling.ipynb,6,markdown,slide,1,How many classes are on the picture?
481,3_topic_modeling.ipynb,7,code,slide,7,"from sklearn.datasets import make_blobs	X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=0.6)	import matplotlib.pyplot as plt	plt.figure(figsize=(7, 7))	plt.axis(""equal"")	plt.scatter(X[:, 0], X[:, 1])	plt.show()"
482,3_topic_modeling.ipynb,8,markdown,slide,1,Can we find them automatically?
483,3_topic_modeling.ipynb,9,markdown,slide,8,"# $k$-means algorithm		$k$-means algorithm computes $k$ cluster centers using so-called **expectation-maximization** approach.		1. Start with initial (random) cluster centers.	2. Update cluster centers, repeating 2 steps:	1. Assign points to the nearest cluster center (**expectation** step).	2. Set the centers to the average of points belongint to the cluster (**maximization** step)."
484,3_topic_modeling.ipynb,10,markdown,slide,1,## Colab demo: $k$-means
485,3_topic_modeling.ipynb,11,code,-,1,demo_kmeans(k=4)
486,3_topic_modeling.ipynb,12,code,-,2,# try wrong number of cluster	demo_kmeans(k=5)
487,3_topic_modeling.ipynb,13,markdown,slide,2,## Colab quiz 1	One step of $k$-means algorithm
488,3_topic_modeling.ipynb,14,code,-,2,enable_mathjax_in_cell()	quiz_kmeans()()
489,3_topic_modeling.ipynb,15,markdown,slide,2,"Since we know true labels of texts, **in this lesson** we will evaluate quality of clustering using true labels.	"
490,3_topic_modeling.ipynb,16,code,slide,17,"def estimate_clustering_quality(target, prediction):	from itertools import combinations	N = len(target)	total_pairs = 0	# we will count the ratio of ""good"" pairs of points	good_pairs = 0	for pair in combinations(range(N), 2):	# NB: O(n**2) complexity!	total_pairs += 1	i, j = pair	if (target[i] == target[j]) == (prediction[i] == prediction[j]):	# a pair of points is ""good"" if and only if	#   - both points belong to the same cluster and have the same label	#   - or points belong to different clusters and have different labels	good_pairs += 1		return good_pairs / total_pairs"
491,3_topic_modeling.ipynb,17,markdown,slide,3,## Colab quiz 2		Evaluate quality of clustering.
492,3_topic_modeling.ipynb,18,code,-,2,enable_mathjax_in_cell()	quiz_estimate_clustering_quality()()
493,3_topic_modeling.ipynb,19,markdown,slide,7,# Clustering of TF-IDF vectors. Topics.		Now we know how	- Find clusters of vectors	- Extract vectors from texts		So we can find clusters of texts using `sklearn.cluster.KMeans`.
494,3_topic_modeling.ipynb,20,markdown,slide,1,## Colab demo: $k$-means in `sklearn`
495,3_topic_modeling.ipynb,21,code,-,17,"from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.datasets import fetch_20newsgroups	import numpy as np		categories = (""sci.space"", ""rec.sport.hockey"", ""comp.graphics"")	fetch_params = dict(	shuffle=True, random_state=1,	remove=('headers', 'footers', 'quotes'),	categories=categories	)	train_dataset = fetch_20newsgroups(subset=""train"", **fetch_params)	test_dataset = fetch_20newsgroups(subset=""test"", **fetch_params)		vec = TfidfVectorizer(stop_words=""english"")	vec.fit(train_dataset.data)	X_train = vec.transform(train_dataset.data)	X_test = vec.transform(test_dataset.data)"
496,3_topic_modeling.ipynb,22,code,-,2,"# for this dataset we know the true number of classes, but in general we don't	n_clusters = len(categories)"
497,3_topic_modeling.ipynb,23,code,-,7,"# KMeans has standard fit/predict interface	from sklearn.cluster import KMeans	kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=20)	kmeans.fit(X_train)	predictions = kmeans.predict(X_test)	plot_confusion_matrix(test_dataset.target, predictions)	estimate_clustering_quality(test_dataset.target, predictions)"
498,3_topic_modeling.ipynb,24,markdown,-,1,Compare clustering quality with random benchmark.
499,3_topic_modeling.ipynb,25,code,-,5,"import random	random.seed(0)	random_predictions = [random.randint(0, n_clusters - 1) for _ in test_dataset.target]	plot_confusion_matrix(test_dataset.target, random_predictions)	estimate_clustering_quality(test_dataset.target, random_predictions)"
500,3_topic_modeling.ipynb,26,markdown,-,1,Compare with classification.
501,3_topic_modeling.ipynb,27,code,-,5,"from sklearn.linear_model import SGDClassifier	clf = SGDClassifier(alpha=.0001, max_iter=50, penalty=""l2"", loss=""log"", random_state=0)	clf.fit(X_train, train_dataset.target)	plot_confusion_matrix(test_dataset.target, clf.predict(X_test))	estimate_clustering_quality(test_dataset.target, clf.predict(X_test))"
502,3_topic_modeling.ipynb,28,markdown,slide,3,Let's do an experiment.		What are the largest components (words) for each centroid?
503,3_topic_modeling.ipynb,29,code,slide,1,"show_vectorizer_largest_components(vec, kmeans.cluster_centers_)"
504,3_topic_modeling.ipynb,30,markdown,slide,1,"Is there the correspondence between largest components and true categories? Remember that the categories are (""sci.space"", ""rec.sport.hockey"", ""comp.graphics"")."
505,3_topic_modeling.ipynb,31,markdown,slide,3,"It is important that we found centroids (and their components) **without any information about true categories**.		We somehow ""inferred"" categories. Let's call them _topics_."
506,3_topic_modeling.ipynb,32,markdown,slide,8,"# Coding session 1		Apply $k$-means on tf-idf vectors of the dataset `harbour-space-text-mining-course/datasets/content_classification.csv`.	The dataset has 7 categories: leisure, affection, bonding, enjoy_the_moment, achievement, nature, exercise. Can you find the cluster that corresponds to the category ""achievement""?		**Hints**:	- Use parameters `n_clusters=7`, `n_init=10`, `max_iter=10`, `random_state=0` for `KMeans`	- Use the function `show_vectorizer_largest_components` to see the words corresponding to the largest components."
507,3_topic_modeling.ipynb,33,code,-,7,"import pandas as pd	from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.cluster import KMeans		df = pd.read_csv(""harbour-space-text-mining-course/datasets/content_classification.csv"", header=0, names=[""text"", ""label""])		# YOUR CODE HERE"
508,3_topic_modeling.ipynb,34,markdown,slide,1,# Non-negative matrix factorization (NMF)
509,3_topic_modeling.ipynb,35,markdown,slide,1,"So, we accidentally discovered topics. Now let's try to formalize this discovery."
510,3_topic_modeling.ipynb,36,markdown,slide,1,**Q**: Can a document have many topics?
511,3_topic_modeling.ipynb,37,markdown,slide,4,**A**: Why not?	> Systems as diverse as genetic networks or the world wide web are best described as	networks with complex topology. A common property of many large networks is that the	vertex connectivities follow a scale-free power-law distribution.
512,3_topic_modeling.ipynb,38,markdown,slide,1,**Q**: Can a word represent many topics?
513,3_topic_modeling.ipynb,39,markdown,slide,1,**A**: Sure! https://en.wikipedia.org/wiki/Matrix
514,3_topic_modeling.ipynb,40,markdown,slide,1,So in general we cannot assign one topic to a document or to a word. A document or a word may have all possible topics in some (probably small) proportions.
515,3_topic_modeling.ipynb,41,markdown,slide,4,"More formally:	- Suppose we want to find $T$ topics	- For each document and for each word, assign a non-negative vector with $T$ elements.	- The larger the $i$-th component, the higher the affinity of the document/word with the $i$-th topic."
516,3_topic_modeling.ipynb,42,markdown,slide,1,**Q**: How to find such vectors?
517,3_topic_modeling.ipynb,43,markdown,slide,1,"**A**: Since we have a tfidf-matrix, let's try to extract topics from it. By the way, there is a suitable matrix method."
518,3_topic_modeling.ipynb,44,markdown,slide,3,What we want:		![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/doc-topic-term-topic.png)
519,3_topic_modeling.ipynb,45,markdown,slide,3,What we have:		![doc-term](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/doc-term.png)
520,3_topic_modeling.ipynb,46,markdown,slide,3,What we will try:		![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/product.png)
521,3_topic_modeling.ipynb,47,markdown,slide,1,"**Definition**: non-negative matrix factorization of the matrix $X$ is a pair of matrices $D, W$ such that $X \approx D \cdot W$."
522,3_topic_modeling.ipynb,48,markdown,slide,10,"##  Non-negative number factorization		We will examine the simplest version of non-negative matrix factorization $X \approx D\cdot W$, where $X, D, W$ are just numbers ($1 \times 1$ matrices).	<!--@slideshow fragment-->	The algorithm of factorization is called ALS (Alternating Least Squares). The idea is to minimize the difference between $X$ and $D \cdot W$:	$$	L_X(D, W) = \frac{1}{2}(X - D\cdot W)^2	$$	<!--@slideshow fragment-->	The function $L_X$ depends on variables $D, W$, and we need to minimize $L_X$."
523,3_topic_modeling.ipynb,49,code,slide,12,"# this is how L_X(D, W) depends on D for fixed X and W		X = 4	def L_X(D, W):	return 0.5 * (X - D*W)**2		D_range = np.linspace(0, 5, 30)	plt.figure(figsize=(10, 6))	plt.plot(D_range, L_X(D_range, 2))	plt.xlabel(r""$D$"")	plt.title(r""$L_4(D, 2)$"")	plt.show()"
524,3_topic_modeling.ipynb,50,markdown,slide,5,"Suppose some $W_0$ is fixed. Then $L_X(D, W_0)$ depends only on $D$. Denote this function $f(D) \equiv L_X(W_0, D)$.	We can compute the derivative:	$$	\dfrac{\mathrm{d}f}{\mathrm{d}D} = (D\cdot W - X) \cdot W	$$"
525,3_topic_modeling.ipynb,51,markdown,slide,1,"Recall the interpretation of the derivative: if we change $D$ by some small amount $\Delta D$, then the function $f(D)$ will change by $\Delta f(D) \approx \dfrac{\mathrm{d}f}{\mathrm{d}D} \Delta D$."
526,3_topic_modeling.ipynb,52,markdown,slide,5,"Our goal is to minimize the function $f$. Since we know the derivative $\dfrac{\mathrm{d}f}{\mathrm{d}D}$, we can take a small step $\Delta D$ such that $\Delta f(D) < 0$ and decrease $f$ a little.	- If $\dfrac{\mathrm{d}f}{\mathrm{d}D} > 0$, take step $\Delta D < 0$	- If $\dfrac{\mathrm{d}f}{\mathrm{d}D} < 0$, take step $\Delta D > 0$		In both cases, we take the step in the direction opposite to $\dfrac{\mathrm{d}f}{\mathrm{d}D}$."
527,3_topic_modeling.ipynb,53,markdown,slide,1,"Clearly, we can do the same thing with $W$, having $D$ fixed."
528,3_topic_modeling.ipynb,54,markdown,slide,8,"So this ALS (Alternating least squares):	0. Choose _learning rate_ $\lambda$ (some positive small number).	1. Initialize $D^{(0)}$, $W^{(0)}$ randomly.	2. Repeat for $t = 0, 1, 2, \dots$ until convergence:	1. Change $D$ by $\Delta D$:	$$D^{(t+1)} = D^{(t)} - \lambda \cdot (D^{(t)}W^{(t)} - X) W^{(t)}$$	2. Change $W$ by $\Delta W$:	$$W^{(t+1)} = W^{(t)} - \lambda \cdot (D^{\color{red}{(t+1)}}W^{(t)} - X) D^{\color{red}{(t+1)}}$$"
529,3_topic_modeling.ipynb,55,code,-,24,"#@slideshow	def non_negative_number_factorization(	X,  # number to factorize	D0=None,  # optional initial value for D	W0=None,  # optional initial value for W	tolerance=1e-4,  # stop iterations if |X - WD| < tolerance	learning_rate=1e-5,  # lambda in the formula	seed=0,	):	import numpy as np	# initialize W and D with W0, D0 or randomly	np.random.seed(seed)	D = D0 or np.random.random() * X  # random from 0 to X	W = W0 or np.random.random() * X  # random from 0 to X		# repeat alternating steps until convergence	while np.abs(X - D*W) > tolerance:	D_step = -learning_rate * (W*D - X) * W	D += D_step	assert D > 0	W_step = -learning_rate * (W*D - X) * D	W += W_step	assert W > 0	return D, W"
530,3_topic_modeling.ipynb,56,code,slide,3,"# factorize X = 4	D, W = non_negative_number_factorization(4)	print(D, W, D*W)"
531,3_topic_modeling.ipynb,57,markdown,slide,1,## Colab demo: NMF in sklearn
532,3_topic_modeling.ipynb,58,code,-,5,"from sklearn.feature_extraction.text import TfidfVectorizer	trf = TfidfVectorizer(stop_words=""english"")	trf.fit(train_dataset.data)	X_train = trf.transform(train_dataset.data)	X_test = trf.transform(test_dataset.data)"
533,3_topic_modeling.ipynb,59,code,-,13,"# NMF has standard fit/transform interface	from sklearn.decomposition import NMF	nmf = NMF(	n_components=3,	random_state=0,	solver=""cd"",  # coordinate descent == ALS	init=""random""	)	nmf.fit(X_train)	X_train_nmf = nmf.transform(X_train)		# to transform the new input data, factorize the input matrix with fixed self.components_	X_test_nmf = nmf.transform(X_test)"
534,3_topic_modeling.ipynb,60,code,-,1,"show_vectorizer_largest_components(trf, nmf.components_)"
535,3_topic_modeling.ipynb,61,code,-,4,"dominant_topics = np.argmax(X_test_nmf, axis=1)		plot_confusion_matrix(test_dataset.target, dominant_topics)	estimate_clustering_quality(test_dataset.target, dominant_topics)"
536,3_topic_modeling.ipynb,62,markdown,slide,3,## Colab quiz 3		Choose the correct NMF
537,3_topic_modeling.ipynb,63,code,-,2,enable_mathjax_in_cell()	quiz_nmf()()
538,3_topic_modeling.ipynb,64,markdown,slide,8,"# Coding session 2		Apply `NMF` on tf-idf vectors of the dataset `harbour-space-text-mining-course/datasets/content_classification.csv`.	The dataset has 7 categories: leisure, affection, bonding, enjoy_the_moment, achievement, nature, exercise. Can you find the cluster that corresponds to the category ""achievement""?		**Hints**:	- Use parameters `n_components=7`, `solver=""cd""`, `init=""random""`, `random_state=0` for `NMF`	- Use the function `show_vectorizer_largest_components` to see the words corresponding to the largest components."
539,3_topic_modeling.ipynb,65,code,-,7,"import pandas as pd	from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.decomposition import NMF		df = pd.read_csv(""harbour-space-text-mining-course/datasets/content_classification.csv"", header=0, names=[""text"", ""label""])		# YOUR CODE HERE"
540,3_topic_modeling.ipynb,66,markdown,slide,1,# Probability refresher
541,3_topic_modeling.ipynb,67,markdown,slide,7,## Law of total probability		![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/total-probability.png)		$$	\Pr(B) =  \sum\limits_{i=1}^N\Pr(A_i \cap B) = \sum\limits_{i=1}^N\Pr(B|A_i) \Pr(A_i)	$$
542,3_topic_modeling.ipynb,68,markdown,slide,4,"## Conditional independency		By definition, $B$ is independent of $A$ if	$$\Pr(B|A) = \Pr(B) \Leftrightarrow \Pr(AB) = \Pr(A) \Pr(B)$$"
543,3_topic_modeling.ipynb,69,markdown,slide,1,# Version of NMF: Probabilistic Latent Semantic Allocation (PLSA).
544,3_topic_modeling.ipynb,70,markdown,slide,1,Now we know that NMF works. But how to interpret it?
545,3_topic_modeling.ipynb,71,markdown,slide,3,"Suppose we have the matrix of conditional probabilities $P = \lbrace\Pr(w_j|d_i)\rbrace_{ij}$	- First, compute the matrix of term frequencies (using `CountVectorizer`)	- Then, normalize rows: $\Pr(w_j|d_i) = \dfrac{\mathrm{TF}(w_j, d_i)}{\sum\limits_w\mathrm{TF}(w, d_i)}$, where $\mathrm{TF}$ is term frequency."
546,3_topic_modeling.ipynb,72,markdown,slide,2,"Apply the law of total probability:	$$\Pr(w|d) = \sum_t\limits\Pr(w|t,d)\Pr(t|d)$$"
547,3_topic_modeling.ipynb,73,markdown,slide,3,"**Assumption**:	$$\Pr(w|t,d) = \Pr(w|t)$$	> Suppose that the document $d$ has topic $t$. Then the probability to find the word $w$ in the document $d$ for the given $t$ does not depend on the document!"
548,3_topic_modeling.ipynb,74,markdown,slide,1,So we have $$\Pr(w|d) = \sum_t\limits\Pr(t|d)\Pr(w|t)$$
549,3_topic_modeling.ipynb,75,markdown,slide,1,"Denote $$\Theta = \lbrace \Pr(t_k|d_i) \rbrace_{ik}, \Phi = \lbrace \Pr(w_j|t_k) \rbrace_{kj}$$"
550,3_topic_modeling.ipynb,76,markdown,slide,2,We can write $P = \Theta\Phi$	> $P_{ij} = \Pr(w_j|d_i) = \sum\limits_t \Pr(t_k|d_i) \Pr(w_j|t_k) = \sum\limits_t \Theta_{ik} \Phi_{kj} = (\Theta \Phi)_{ij}$
551,3_topic_modeling.ipynb,77,markdown,slide,4,# PLSA algorithm	How do we find $\Theta$ and $\Phi$?		Apply Expextation-Maximization algorithm similar to $k$-means.
552,3_topic_modeling.ipynb,78,markdown,slide,2,## Expectation step	$$H_{dwt} =\Pr(t | d w) = \dfrac{\Pr(w|t) \Pr(t|d)}{\Pr(w|d)} = \dfrac{\Phi_{tw} \Theta_{dt}}{\sum\limits_s \Phi_{sw} \Theta_{ds}}$$
553,3_topic_modeling.ipynb,79,markdown,slide,6,"## Maximization step		How many times the word $w$ in document $d$ belongs to the topic $t$: $$\hat{n}_{dwt} = \mathrm{TF}(w, d) \cdot H_{dwt}$$ ($\mathrm{TF}$ is term frequency).			"
554,3_topic_modeling.ipynb,80,markdown,slide,1,"Using $\hat{n}_{dwt}$, we can compute new approximations for $\Theta$ and $\Phi$."
555,3_topic_modeling.ipynb,81,markdown,slide,4,- New approximations for $\Theta$:	- $\hat{n}_{dt} = \sum\limits_w \hat{n}_{dwt}$	- $\hat{n}_{d} = \sum\limits_t \hat{n}_{dt}$	- $\Theta_{dt} = \dfrac{\hat{n}_{dt}}{\hat{n}_{d}}$
556,3_topic_modeling.ipynb,82,markdown,slide,4,- New approximations for $\Phi$:	- $\hat{n}_{wt} = \sum\limits_d \hat{n}_{dwt}$	- $\hat{n}_{t} = \sum\limits_w \hat{n}_{wt}$	- $\Phi_{tw} = \dfrac{\hat{n}_{wt}}{\hat{n}_{t}}$
557,3_topic_modeling.ipynb,83,markdown,slide,5,"As usual, we may add something to numerators and denominators to smooth probability estimations:		$$\Theta_{dt} = \dfrac{\hat{n}_{dt} \color{red}{+ \alpha_t}}{\hat{n}_{d}\color{red}{+ \alpha_0}}, \Phi_{tw} = \dfrac{\hat{n}_{wt} \color{red}{+ \beta_w}}{\hat{n}_{t}\color{red}{+ \beta_0}}$$		This version is called LDA (Latent Dirichlet Allocation)"
558,3_topic_modeling.ipynb,84,markdown,slide,1,## Colab demo: PLSA implementation
559,3_topic_modeling.ipynb,85,code,-,104,"import numpy as np	import scipy.sparse		def plsa_decomposition(	X, n_topics, n_iterations=10,	alpha_t=0, alpha_0=0, beta_w=0, beta_0=0,	phi=None, theta=None, update_phi=True,	random_state=0	):	""""""	param X: (document, word) matrix with word counts	(the output of CountVectorizer)	return: (theta, phi) such that theta * phi ≈ X	""""""	n_documents, n_words = X.shape		# theta is the matrix of topics for documents	theta_shape = (n_documents, n_topics)	np.random.seed(random_state)	if theta is None:	# initialize theta with random normalized rows	theta = np.random.random(theta_shape)	theta /= np.sum(theta, axis=1)[:, None]  # normalize rows	else:	assert theta.shape == theta_shape, ""theta has wrong shape, expected {}, got {}"".format(	theta_shape, theta.shape)		# phi is the matrix of topics for words	phi_shape = (n_topics, n_words)	if phi is None:	# initialize phi with random normalized columns	phi = np.random.random(phi_shape)	phi /= np.sum(phi, axis=0)	else:	assert phi.shape == phi_shape, ""phi has wrong shape, expected {}, got {}"".format(	phi_shape, phi.shape)		# iterate over nonzero items of the input matrix	D, W, word_counts = scipy.sparse.find(X)	# in this implementation, the number of iterations is fixed	# an alternative way is to iterate until convergence of theta and phi	for _ in tqdm(range(n_iterations), desc=""iterations""):	# initialize ""n-hat"" variables	n_hat_td = np.zeros(theta.shape)	n_hat_wt = np.zeros(phi.shape)	n_hat_t = np.zeros(n_topics)	n_hat_d = np.zeros(n_documents)		# calculate all the normalization factors with one matrix multiplication	# Z[d, t] = \sum_{t} theta_[d, t] * phi_[t, w]	Z = theta @ phi		for d, w, word_count in zip(D, W, word_counts):	for t in range(n_topics):	delta = word_count * theta[d, t] * phi[t, w] / Z[d, w]	n_hat_td[d, t] += delta	n_hat_wt[t, w] += delta	n_hat_t[t] += delta	n_hat_d[d] += delta		# update	# broadcasting: (n_doculents, n_topics) / (n_documents,)	theta = (n_hat_td + alpha_t) / (n_hat_d[:, None] + alpha_0)	if update_phi:	# broadcasting: (n_topics, n_words) / (n_topics,)	phi = (n_hat_wt + beta_w) / (n_hat_t[:, None] + beta_0)		return theta, phi			from sklearn.base import BaseEstimator		class PLSA(BaseEstimator):	def __init__(	self, n_topics=10, n_iterations=10,	alpha_t=0, alpha_0=0, beta_w=0, beta_0=0,	random_state=0	):	self.n_topics = n_topics	self.n_iterations = n_iterations	self.alpha_t = alpha_t	self.alpha_0 = alpha_0	self.beta_w = beta_w	self.beta_0 = beta_0	self.random_state = random_state		def fit_transform(self, X, y=None):	theta, phi = plsa_decomposition(	X, n_topics=self.n_topics, n_iterations=self.n_iterations,	alpha_t=self.alpha_t, alpha_0=self.alpha_0, beta_w=self.beta_w, beta_0=self.beta_0,	random_state=self.random_state	)	self.components_ = phi	return theta		def transform(self, X):	theta, _ = plsa_decomposition(	X, n_topics=self.n_topics, n_iterations=self.n_iterations,	update_phi=False,  # do not update the word topic matrix	phi=self.components_,  # use the matrix obtained in the preceeding `fit_transform`	alpha_t=self.alpha_t, alpha_0=self.alpha_0, beta_w=self.beta_w, beta_0=self.beta_0,	random_state=self.random_state	)	return theta"
560,3_topic_modeling.ipynb,86,code,-,5,"from sklearn.feature_extraction.text import CountVectorizer	trf = CountVectorizer(stop_words=""english"")	trf.fit(train_dataset.data)	X_train = trf.transform(train_dataset.data)	X_test = trf.transform(test_dataset.data)"
561,3_topic_modeling.ipynb,87,markdown,-,1,Without smoothing (PLSA)
562,3_topic_modeling.ipynb,88,code,-,3,"plsa = PLSA(n_topics=3, n_iterations=30)	X_train_plsa = plsa.fit_transform(X_train)	X_test_plsa = plsa.transform(X_test)"
563,3_topic_modeling.ipynb,89,code,-,1,"show_vectorizer_largest_components(trf, plsa.components_)"
564,3_topic_modeling.ipynb,90,code,-,5,"dominant_topics = np.argmax(X_test_plsa, axis=1)		plot_confusion_matrix(test_dataset.target, dominant_topics)	# 100 iterations give the quality 0.8785407989730233	estimate_clustering_quality(test_dataset.target, dominant_topics)"
565,3_topic_modeling.ipynb,91,markdown,-,1,With smoothing (LDA)
566,3_topic_modeling.ipynb,92,code,-,6,"lda = PLSA(	n_topics=3, n_iterations=30,	alpha_t=0.1/3, alpha_0=0.1, beta_w=0.1/len(trf.vocabulary_), beta_0=0.1	)	X_train_lda = lda.fit_transform(X_train)	X_test_lda = lda.transform(X_test)"
567,3_topic_modeling.ipynb,93,code,-,1,"show_vectorizer_largest_components(trf, lda.components_)"
568,3_topic_modeling.ipynb,94,code,-,5,"dominant_topics = np.argmax(X_test_lda, axis=1)		plot_confusion_matrix(test_dataset.target, dominant_topics)	# 100 iterations give the quality 0.8821097151600854	estimate_clustering_quality(test_dataset.target, dominant_topics)"
569,3_topic_modeling.ipynb,95,markdown,slide,3,# Colab demo: gensim		`gensim` is a Python library that implements LDA algorithm.
570,3_topic_modeling.ipynb,96,code,-,10,"import spacy		# extract tokens (lemmas) with spaCy	# we are interested in interpretable topics, so keep only nouns, adjectives, verbs and adverbs	nlp = spacy.load(""en"", disable=['parser', 'ner'])	def tokens(s):	return [t.lemma_ for t in nlp(s) if t.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']]		train_documents = [tokens(t) for t in tqdm(train_dataset.data, desc=""tokenize"")]	print(train_documents[0])"
571,3_topic_modeling.ipynb,97,code,-,9,"# Remove rare and common tokens.	from gensim.corpora import Dictionary		# Create a dictionary representation of the documents.	dictionary = Dictionary(train_documents)		# Filter out words that occur less than in 5 documents, or more than in 50% of the documents.	dictionary.filter_extremes(no_below=5, no_above=0.5)	print(""dictionary word with index 10:"", dictionary[10])"
572,3_topic_modeling.ipynb,98,code,-,5,"# Convert tokenized documents into bag-of-words (BOW) representation: (word index, word count)	corpus = [dictionary.doc2bow(doc) for doc in train_documents]	print(""Example of BOW representation:"", corpus[0])	print('Number of unique tokens:', len(dictionary))	print('Number of documents:', len(corpus))"
573,3_topic_modeling.ipynb,99,code,-,16,"from gensim.models import LdaModel		# train LDA model	lda = LdaModel(	corpus,  # documents in BOW format	id2word=dictionary.id2token,  # id -> word mapping to make topics interpretable	num_topics=3,  # desired number of topics	passes=30,  # number of iterations	)		# recall what true categories are	print(""True categories:"", train_dataset.target_names)		# look at the topics	print(""LDA topics:"")	lda.print_topics()"
574,3_topic_modeling.ipynb,100,code,-,8,"# check that topics provide good clusterization of the training data	def get_topic(lda, doc):	# helper function for gensim	return max(lda.get_document_topics(doc), key=lambda t: t[1])[0]		train_topics = [get_topic(lda, doc) for doc in corpus]	plot_confusion_matrix(train_dataset.target, train_topics)	estimate_clustering_quality(train_dataset.target, train_topics)"
575,3_topic_modeling.ipynb,101,markdown,slide,2,# Colab demo: pyLDAvis	Python library for interactive topic model visualization.
576,3_topic_modeling.ipynb,102,code,-,4,"!pip install -q pyldavis	# pyldavis makes gensim spam with warnings, suppress	import warnings	warnings.filterwarnings('ignore')"
577,3_topic_modeling.ipynb,103,code,-,6,"# Colab hint: View output fullscreen	import pyLDAvis	import pyLDAvis.gensim  # don't skip this		pyLDAvis.enable_notebook()	pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
578,3_topic_modeling.ipynb,104,markdown,slide,7,"# Evaluate quality of topics.		Visually, we can evaluate top words (ordered by $\Pr(w|t)$) and decide if they form a meaningful topic.		**Idea**: top words of a topic should often co-occur.		**Definition**: topic coherence is the measure of how often the most ""representative"" words of a topic occur together."
579,3_topic_modeling.ipynb,105,markdown,slide,1,## Colab demo: implementation of coherence with PMI
580,3_topic_modeling.ipynb,106,code,-,9,"def get_top_topic_words(lda, dictionary, k=10):	# helper function that returns top k words for each topic	top_words = dict()	for topicid in range(lda.num_topics):	top_words[topicid] = [dictionary[tokenid] for tokenid, _ in lda.get_topic_terms(topicid, k)]	return top_words		from pprint import pprint	pprint(get_top_topic_words(lda, dictionary, k=10), compact=True)"
581,3_topic_modeling.ipynb,107,code,-,43,"def topic_coherence(documents, topic_words, window_size=10):	from collections import Counter	from itertools import combinations	import numpy as np		# Step 1. Compute word occurrence and co-occurrence in documents	words_count = Counter()	word_pairs_count = Counter()	num_windows = 0	for document in documents:	# count word co-occurrence in a sliding window	for window_start in range(max(0, len(document) - window_size) + 1):	num_windows += 1	window = document[window_start:window_start + window_size]	for w in window:	words_count[w] += 1	for word_pair in combinations(sorted(window), 2):	assert word_pair[0] <= word_pair[1]	word_pairs_count[word_pair] += 1		# Step 2. Estimate how often top words in topics co-occur.	scores = []	for topic_word_pair in combinations(sorted(topic_words), 2):	assert topic_word_pair[0] <= topic_word_pair[1]	# we can estimate how dependent two words are	# P(w1, w2) is the probability to find both words close to each other	P_w1_w2 = word_pairs_count[topic_word_pair] / num_windows	# P(w1) is the probability to find the first word in a document	P_w1 = words_count[topic_word_pair[0]] / num_windows	# P(w2) is the probability to find the second word in a document	P_w2 = words_count[topic_word_pair[1]] / num_windows		# if P(w1, w2) == P(w1) * P(w2), then w1 and w2 are independent	# if P(w1, w2) > P(w1) * P(w2), then w1 and w2 often co-occur		# compute log(P(w1, w2) / (P(w1) * P(w2))) as a measure of co-occurrence	# (it is called Pointwise Mutual Information, or PMI)	score = np.log(P_w1_w2 / (P_w1 * P_w2))	scores.append(score)	return np.mean(scores)		for topic, topic_words in get_top_topic_words(lda, dictionary, k=10).items():	print(topic, topic_coherence(train_documents, topic_words))"
582,3_topic_modeling.ipynb,108,markdown,slide,3,## Colab quiz 4		Choose the words with highest $PMI$.
583,3_topic_modeling.ipynb,109,code,-,2,enable_mathjax_in_cell()	quiz_coherence()()
584,3_topic_modeling.ipynb,110,markdown,slide,1,## Colab demo: coherence in gensim
585,3_topic_modeling.ipynb,111,markdown,-,1,"`gensim` implements several versons of coherence, but the idea is always the same: estimate how often top words in topics co-occur."
586,3_topic_modeling.ipynb,112,code,-,10,"from gensim.models import CoherenceModel	cm_uci = CoherenceModel(	model=lda,	texts=train_documents,	dictionary=dictionary,	coherence='c_uci',  # 'c_uci' is similar to our implementation	topn=10	)	print(""'c_uci' coherence per topic:"", cm_uci.get_coherence_per_topic())	print(""average 'c_uci' coherence:"", cm_uci.get_coherence())"
587,3_topic_modeling.ipynb,113,code,-,11,"# 'c_v' is the current state-of-the-art	# see http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf for details	cm_v = CoherenceModel(	model=lda,	texts=train_documents,	dictionary=dictionary,	coherence='c_v',	topn=10	)	print(""'c_v' coherence per topic:"", cm_v.get_coherence_per_topic())	print(""average 'c_v' coherence:"", cm_v.get_coherence())"
588,3_topic_modeling.ipynb,114,markdown,-,1,"Using coherence , we can choose the optimal number of topics."
589,3_topic_modeling.ipynb,115,code,-,13,"import warnings	warnings.filterwarnings('ignore')		num_topics_grid = list(range(1, 7))	coherences = []	for num_topics in tqdm(num_topics_grid):	# train LDA with num_topics	lda_ = LdaModel(corpus, id2word=dictionary.id2token, num_topics=num_topics, passes=5)	# evaluate coherence	cm = CoherenceModel(model=lda_, texts=train_documents, dictionary=dictionary, coherence='c_v', topn=10)	coherences.append(cm.get_coherence())	plt.plot(num_topics_grid, coherences)	plt.show()"
590,3_topic_modeling.ipynb,116,markdown,slide,1,# Colab demo: apply LDA model
591,3_topic_modeling.ipynb,117,code,-,4,"# experiment: document containing 1 word ""space""	space_doc = dictionary.doc2bow([""space""])	print(""document topics:"", lda.get_document_topics(space_doc))  # belongs to topic 0	print(""term topics:"", lda.get_term_topics(space_doc[0][0], minimum_probability=0))"
592,3_topic_modeling.ipynb,118,code,-,7,"# apply on new data	test_documents = [tokens(t) for t in tqdm(test_dataset.data, desc=""tokenize"")]	print(""test document example:"", test_documents[0])	test_corpus = [dictionary.doc2bow(doc) for doc in test_documents]	test_topics = [get_topic(lda, doc) for doc in test_corpus]	plot_confusion_matrix(test_dataset.target, test_topics)	estimate_clustering_quality(test_dataset.target, test_topics)"
593,3_topic_modeling.ipynb,119,markdown,slide,1,# Coding session 3
594,3_topic_modeling.ipynb,120,markdown,-,3,## Exercise 1		Tokenize the texts. Use the method `tokens()` implemented above.
595,3_topic_modeling.ipynb,121,code,-,3,"import pandas as pd	df = pd.read_csv(""harbour-space-text-mining-course/datasets/content_classification.csv"", header=0, names=[""text"", ""label""])	# YOUR CODE HERE"
596,3_topic_modeling.ipynb,122,markdown,-,5,"## Exercise 2		Create the dictionary for Gensim.	- Filter out words that occur less than in 5 documents, or more than in 50% of the documents.	- **NB**. There is a bug in Gensim: the dictionary should be initialized. To initialize the dictionary, you can just access an element of it. So print the dictionary element with the index 10."
597,3_topic_modeling.ipynb,123,code,-,3,from gensim.corpora import Dictionary		# YOUR CODE HERE
598,3_topic_modeling.ipynb,124,markdown,-,3,"## Exercise 3		Create the corpus, converting each document from the exercise 1 to the bag-of-words representation using `.doc2bow()` method of the dictionary created in the Exercise 2."
599,3_topic_modeling.ipynb,125,code,-,1,# YOUR CODE HERE
600,3_topic_modeling.ipynb,126,markdown,-,3,## Exercise 4		Train `LdaModel` with 7 topics. Use 10 passes over the training data.
601,3_topic_modeling.ipynb,127,code,-,3,from gensim.models import LdaModel		# YOUR CODE HERE
602,3_topic_modeling.ipynb,128,markdown,-,3,## Exercise 5		Visualize the LDA model from the Exercise 4 using `pyLDAVis`.
603,3_topic_modeling.ipynb,129,code,-,1,# YOUR CODE HERE
604,3_topic_modeling.ipynb,130,markdown,-,5,## Exercise 6		Choose the best parameters of `LdaModel` using `CoherenceModel`.		You can try different number of topics and more passes.
605,3_topic_modeling.ipynb,131,code,-,4,import warnings	warnings.filterwarnings('ignore')		# YOUR CODE HERE
606,3_topic_modeling.ipynb,132,markdown,slide,7,"# Summary	1. We can find groups of related documents using  $k$-means algorithm and TF-IDF vectors.	1. Centroids of clusters are interpretable: words with largest components form _topics_.	1. We can find topics using non-negative matrix factorization (NMF).	1. Probabilistic versions of NMF PLSA and LDA (EM-algorithm).	1. Python libraries: gensim, pyLDAvis.	1. We can evaluate quality of topics using _coherence_: how often ""important"" words of the topic occur together."
607,3_topic_modeling.ipynb,133,markdown,slide,5,"# Recommended resources	- [📖 ""In Depth: k-Means Clustering"" by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)	- [📖 ""Topic Modeling with Gensim (Python)"" by Selva Prabhakaran](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)	- [📖 Exploring the Space of Topic Coherence Measures](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)	- [📖 Topic Models (in Russian)](http://www.machinelearning.ru/wiki/images/f/fb/Voron-ML-TopicModels.pdf)"
608,3_topic_modeling.ipynb,134,markdown,-,1,# [OPTIONAL] Regilarized non-negative factorization
609,3_topic_modeling.ipynb,135,markdown,-,2,"Clearly, there is no unique solution for factorization.	We may start with different initial values and get different results:"
610,3_topic_modeling.ipynb,136,code,-,2,"W, H = non_negative_number_factorization(4, D0=0.1, W0=10)	print(W, H, W*H)"
611,3_topic_modeling.ipynb,137,markdown,-,18,"We may want to restrict the number of solutions.		Let's put the additional constraint: $W \approx H$.		This constraint is enforced with *regularization*: instead of minimizing	$$	L_X(W, H) = \frac{1}{2} (X - WH)^2	$$	we will minimize	$$	L_{X, \alpha}(W, H) = \frac{1}{2} (X - WH)^2 + \color{red}{\frac{\alpha}{2} (W^2 + H^2)}	$$		Intuition: we need $W \approx H$, or $(W - H)^2 = W^2 + H^2 - 2WH \approx 0$. Since $WH$ is restricted ($WH \approx X$), we need the remaining part $W^2 + H^2$ to be as small as possible.		Now the derivatives are	- $\dfrac{\mathrm{d}L_{X, \alpha}}{\mathrm{d}H} = (WH-X)W \color{red}{+ \alpha H}$	- $\dfrac{\mathrm{d}L_{X, \alpha}}{\mathrm{d}W} = (WH-X)H \color{red}{+ \alpha W}$"
612,3_topic_modeling.ipynb,138,code,-,32,"def non_negative_number_factorization_regularized(	X,  # number to factorize	H0=None,  # optional initial value for H	W0=None,  # optional initial value for W	tolerance=1e-4,  # stop iterations if |X - WH| < tolerance	max_iterations=100000,  # stop iterations after max_iterations even if |X - WH| > tolerance	learning_rate=1e-5,  # lambda in the formula	alpha=0.1,  # regularization factor	seed=0,	):	import numpy as np	# initialize W and H with W0, H0 or randomly	np.random.seed(seed)	W = W0 or np.random.random() * X  # random from 0 to X	H = H0 or np.random.random() * X  # random from 0 to X		# repeat alternating steps until convergence	num_iterations = 0	while np.abs(X - W*H) > tolerance:	if num_iterations > max_iterations:	break	num_iterations += 1	H_step = -learning_rate * ((W*H - X) * W + alpha * H)	H += H_step	assert H > 0	W_step = -learning_rate * ((W*H - X) * H + alpha * W)	W += W_step	assert W > 0	return W, H		W, H = non_negative_number_factorization_regularized(4, W0=10, H0=0.1, alpha=0.5)	print(W, H, W*H)"
613,3_topic_modeling.ipynb,139,markdown,-,11,"# [OPTIONAL] Math behind `lda.get_document_topics`		This is the illustrative example how to compute probability of topic for a given document: $\Pr(t|d)$.		If $d$ contains only one word $w$, then $\Pr(t|d) = \Pr(t|w)$.		What we have is $\Pr(w|t)$, transform it to $\Pr(t|w)$ using Bayes' rule:	$$	\Pr(t|w) = \dfrac{\Pr(t, w)}{\Pr(w)} = \dfrac{\Pr(w|t)\Pr(t)}{\Pr(w)}	$$	We can estimate $\Pr(t)$ and $\Pr(w)$ summing over all documents."
614,4_word2vec.ipynb,0,code,-,30,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.utils import (	enable_mathjax_in_cell,	display_pca_scatterplot,	display_pca_scatterplot_interactive	)	from tmcourse.demo import (	demo_word2vec_batch,	demo_gradient_descent,	)	from tmcourse.quiz import (	quiz_skibidi,	quiz_word2vec_context,	quiz_word2vec_word_vector,	quiz_word2vec_subsampling,	quiz_word2vec_negative_sampling,	quiz_most_similar,	quiz_earl,	)	from tmcourse.ipyquiz import Quiz	from tqdm.notebook import tqdm		import warnings	warnings.filterwarnings('ignore')"
615,4_word2vec.ipynb,1,markdown,slide,1,<center><h1>Word vectors</h1></center>
616,4_word2vec.ipynb,2,markdown,slide,8,# Outline	1. Sparsity problem (words are not orthogonal)	1. Word meanings and distributional hypothesis	1. word2vec: idea	1. word2vec: algorithm	1. word2vec in gensim	1. Visualization of word vectors	1. Properties of word vectors
617,4_word2vec.ipynb,3,markdown,slide,5,"# Sparsity problem (words are not orthogonal)		So far, we treated words (or terms) as discrete symbols.	- For each word, `TfidfVectorizer` creates a separate vector component.	- Hence all words are _orthogonal_."
618,4_word2vec.ipynb,4,markdown,slide,6,"The following experiment shows that it may be a problem:	1. Take two questions from the Quora Question Pairs dataset:	- ""How can I be a good geologist?""	- ""What should I do to be a great geologist?""	1. Compute the distance between their tfidf-vectors	1. Repeat with only one change in the second question: ""good"" instead of ""great""."
619,4_word2vec.ipynb,5,code,slide,23,"from sklearn.feature_extraction.text import TfidfVectorizer	import pandas as pd	import numpy as np	from scipy.spatial.distance import cosine	def row_to_array(sparse_matrix_row):	return np.squeeze(np.asarray(sparse_matrix_row.todense()))		df = pd.read_csv(""harbour-space-text-mining-course/datasets/quora_question_pairs/train.csv"")		# words 'good' and 'great' are similar, but have different TF-IDF representations	# let's see if it makes any difference		vec = TfidfVectorizer().fit(list(df[""question1""].fillna(""."")) + list(df[""question2""].fillna(""."")))		question_1 =              ""How can I be a good geologist?""  # df.iloc[7][""question1""]	question_2 =              ""What should I do to be a great geologist?""  # df.iloc[7][""question2""]	question_2_reformulated = ""What should I do to be a good geologist?""		vector_1 = row_to_array(vec.transform([question_1]))	vector_2 = row_to_array(vec.transform([question_2]))	vector_2_reformulated = row_to_array(vec.transform([question_2_reformulated]))	print(""cosine(\""{}\"", \""{}\"") = {:.2f}"".format(question_1, question_2, cosine(vector_1, vector_2)))	print(""cosine(\""{}\"", \""{}\"") = {:.2f}"".format(question_1, question_2_reformulated, cosine(vector_1, vector_2_reformulated)))"
620,4_word2vec.ipynb,6,markdown,slide,1,"Words ""good"" and ""great"" have similar meanings in this case, but for TF-IDF they are different."
621,4_word2vec.ipynb,7,markdown,slide,1,"**Our goal**: instead of discrete representations (all words are different, or orthogonal), find representations that captures similarity between words."
622,4_word2vec.ipynb,8,markdown,slide,1,"**Technically**, we will represent words as _vectors_ such that the words with similar meanings will have similar (close) vectors."
623,4_word2vec.ipynb,9,markdown,slide,1,"But, what is ""meaning""? How do **we** understand the meaning of the word?"
624,4_word2vec.ipynb,10,markdown,slide,1,# Word meanings. Distributional hypothesis.
625,4_word2vec.ipynb,11,markdown,slide,2,"## Colab quiz 1	What is ""skibidi""?"
626,4_word2vec.ipynb,12,code,-,1,quiz_skibidi()()
627,4_word2vec.ipynb,13,markdown,slide,4,"Actually, neither the word ""skibidi"" nor the definitions 1-4 exist.	This quiz was generated using https://www.thisworddoesnotexist.com/		How did you know the meaning of ""skibidi""?"
628,4_word2vec.ipynb,14,markdown,slide,4,"## Distributional semantics	**Idea**: the meaning of a word can be deduced from  _contexts_ the word appears in.		> ""You shall know a word by the company it keeps"" (Firth, 1957)"
629,4_word2vec.ipynb,15,markdown,slide,3,"- **What we need**: represent words by vectors such that words with similar meanings have similar vectors.	- **What we know (assume)**: the meaning of a word can be deduced from  _contexts_ the word appears in	- **What we will do**: given a word, predict its context."
630,4_word2vec.ipynb,16,markdown,slide,1,# word2vec
631,4_word2vec.ipynb,17,markdown,slide,1,"**Idea**: for each word, find a vector such that words with similar vectors often occur in the same context."
632,4_word2vec.ipynb,18,markdown,slide,6,"- A large corpus of text (Wikipedia).	- Every word is represented by a vector.	- For each position in the text:	1. Get the word on this position.	1. Get the context of the word.	1. Adjust vectors to maximize the probability of the observed context (the closer the vectors, the larger the probability)."
633,4_word2vec.ipynb,19,markdown,slide,3,"## Colab demo: word2vec context		By definition, the context consists of $m$ surrounding words on the left and $m$ on the right. See the demonstration for $m=3$."
634,4_word2vec.ipynb,20,code,-,1,"demo_word2vec_batch(""never gonna give you up never gonna let you down"".split(), 3)"
635,4_word2vec.ipynb,21,markdown,slide,2,## Colab quiz 2	Choose the correct context
636,4_word2vec.ipynb,22,code,-,1,quiz_word2vec_context()()
637,4_word2vec.ipynb,23,markdown,slide,13,"## word2vec: objective function		Consider the position $t$ in the text:	- Word: $w_t$	- Context: $$w_{t-m}, w_{t-m + 1}, \dots, w_{\color{red}{t-1}}, w_{\color{red}{t+1}}, \dots, w_{t+m-1}, w_{t+m}$$	- Probability of the context	$$	\Pr(w_{t-m}, w_{t-m + 1}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+m-1}, w_{t+m}|w_t) \equiv \Pr_{\textrm{context}}(t)	$$	- Likelihood of the corpus is:	$$	\prod_{t=1}^T \Pr_{\textrm{context}}(t)	$$"
638,4_word2vec.ipynb,24,markdown,slide,7,"	Word vectors are _parameters_ $\theta$.		**Our goal**: maximize the likelihood, or minimize the _negative log-likelihood_ w.r.t. $\theta$:	$$	L(\theta) = -\sum_{t=1}^T \log\Pr_{\textrm{context}}(t)	$$"
639,4_word2vec.ipynb,25,markdown,slide,3,"**Assumption**: for each $t$, words both inside and outside the context are conditionally independent given $w_t$.		> Informally, for any word $\hat{w}$, the probability to find $\hat{w}$ in the context of $w_t$ depends only on $w_t$."
640,4_word2vec.ipynb,26,markdown,slide,4,So we can write:	$$	\Pr_{\textrm{context}}(t) = \left(\prod_{w' \in \textrm{context}(w_t)} \Pr(w'|w_t)\right)\left(\prod_{w'' \notin \textrm{context}(w_t)}(1 - \Pr(w''|w))\right)	$$
641,4_word2vec.ipynb,27,markdown,slide,6,"The objective function:	$$	L(\theta) = -\sum_{t=1}^T \left(\sum_{w' \in \textrm{context}(w_t)} \log\Pr(w'|w_t) + \sum_{w'' \notin \textrm{context}(w_t)}\log(1 - \Pr(w''|w))\right)	$$		As usual, we will use gradient descent to find parameters (word vectors) $\theta$ that minimize $L(\theta)$."
642,4_word2vec.ipynb,28,markdown,slide,1,## Colab demo: gradient descent
643,4_word2vec.ipynb,29,code,-,2,"demo_gradient_descent(lambda x: x**4 - x**3 - x**2 + 1, theta_0=-1, learning_rate=0.5)  # local minima	# demo_gradient_descent(lambda x: x**2, theta_0=-1, learning_rate=1.1)  # divergence"
644,4_word2vec.ipynb,30,markdown,slide,11,"## Problem 1: word in its own context		How do we estimate $\Pr(o|c)$?		Suppose the word $o$ has vector $\mathbf{u}$, and the word $c$ has vector $\mathbf{v}$.		It's natural to estimate	$$	\Pr(o|c) = \sigma(\mathbf{u} \cdot \mathbf{v})	$$	where $\mathbf{u} \cdot \mathbf{v}$ is the dot product, and $\sigma(s) = \dfrac{1}{1 + e^{-s}}$ is the sigmoid function."
645,4_word2vec.ipynb,31,markdown,slide,3,"But what if $o = c$?		The dot product is large, but the probability to see the word in its own context is low!"
646,4_word2vec.ipynb,32,markdown,slide,3,"**Solution**: for each word $w$, instead of _one_ vector, keep _two_ vectors:	1. When $w$ is the context word: $\mathbf{u}_w$.	1. When $w$ is the central word: $\mathbf{v}_w$."
647,4_word2vec.ipynb,33,markdown,slide,5,## Problem 2: computational complexity	Remember that the objective function is	$$	L(\theta) = -\sum_{t=1}^T \left(\sum_{w' \in \textrm{context}(w_t)} \log\Pr(w'|w_t) + \sum_{\color{red}{w'' \notin \textrm{context}(w_t)}}\log(1 - \Pr(w''|w))\right)	$$
648,4_word2vec.ipynb,34,markdown,slide,2,"- In each term, the second sum is computed over the full vocabulary (except for the context of $w_t$).	- We need to compute it for each position $t$ in the text."
649,4_word2vec.ipynb,35,markdown,slide,3,"**Solution**: **for each context word**, keep only the fixed number (5-20) of terms in the second sum.		> This is called **negative sampling**."
650,4_word2vec.ipynb,36,markdown,slide,6,"## Problem 3: ""meaningless"" words		Some words (like prepositions and articles in English)	- are very frequent,	- play only a technical role and therefore don't have meaning.	"
651,4_word2vec.ipynb,37,markdown,slide,3,"So they harm the performance of stochastic gradient descent, because	- they cause may updates because they are frequent,	- these updates are ""noisy"", because ""technical"" words don't have meaning."
652,4_word2vec.ipynb,38,markdown,slide,3,"**Solution**: throw frequent words away with some probability (the higher the frequency, the higher the probability).		> This is called **subsampling**."
653,4_word2vec.ipynb,39,markdown,slide,1,## Colab demo: word2vec implementation
654,4_word2vec.ipynb,40,code,-,185,"import numpy as np	from collections import Counter	from scipy.special import softmax	from scipy.special import expit as sigmoid  # ""expit"" is the inverse of ""logit""	from scipy.spatial.distance import cosine as cosine_distance		class Word2Vec:	""""""	This is an instructive implementation of skip-gram word2vec algorithm with negative sampling loss.	It is deliberately inefficient: it uses built-in Python data structures (dicts and lists)	instead of matrix-vector operations.		The recommended way to read the code is the following:	1. First, read the implementation of the `train()` method:	- Training loop: iterate few times (epochs) over the training data.	Number of epochs is provided in the `n_epoch` parameter,	and `self.learning_rate` is predefined step size of gradient descent.	- In each iteration, a batch (central_word and context_words) is generated.	- Too frequent words are discarded (subsampling).	- The gradients are computed and gradient step is performed.	2. Then look at the method `generate_batches()`.	It creates pairs (central word, [list of context words])	by moving sliding window of predefined size `self.window_size`.	3. Then check the `generate_negative_samples()` method.	Its implementation is short, but there are two important details:	- Sampling is performed with replacement. It mimics i.i.d. assumption:	negative samples must be independent.	- The central word and the context word are discarded from the set	of negative samples.	4. The most important part is how gradients are computed: the method `compute_gradients()`.	The gradients are derived in this lecture; this method just implements them in Python code.	5. Then take a look at `build_vocabulary()`: it is mostly technical.	6. `get_vector()` method is just a one-liner, but pay attention to the fact	that we use only one of two matrices (the matrix of central vectors).	7. `get_similar_words()` method implements inefficient (O(|V|)) search of	words whose vectors are closest to the vector of the given word.	8. `__init__()` method just stores all hyperparameters and initializes necessary data structures.	""""""	def __init__(	self,	window_size=2,	vector_size=10,	n_negative_samples=20,	learning_rate=1e-5,	seed=0	):	self.window_size = window_size	self.learning_rate = learning_rate		self.vocab = Counter()		# how many dimensions each vector has	self.vector_size = vector_size	# vectors are initialized in .build_vocabulary() after the vocabulary is known	self.central_vectors = dict()	self.context_vectors = dict()		# number of negative samples for each context word	self.n_negative_samples = n_negative_samples		# prepare vocabulary for negative sampling and sumsampling	self.words_array = []	self.negative_sampling_probabilities = []	self.subsampling_probabilities = dict()		np.random.seed(seed)		def build_vocabulary(self, sequences):	# count all the words	for sequence in sequences:	for word in sequence:	self.vocab[word] += 1		sum_counts_negative_sampling = sum(v**0.75 for v in self.vocab.values())	sum_counts_subsampling = sum(self.vocab.values())	for word, count in self.vocab.items():	self.words_array.append(word)	self.negative_sampling_probabilities.append(count**0.75 / sum_counts_negative_sampling)	word_frequency = count / sum_counts_subsampling	self.subsampling_probabilities[word] = (1 + np.sqrt(word_frequency / 0.001)) * 0.001 / word_frequency		# initialize vectors with small values around 0	for word in self.vocab:	self.central_vectors[word] = np.random.randn(self.vector_size) * 0.1	self.context_vectors[word] = np.random.randn(self.vector_size) * 0.1		def generate_batches(self, sequence):	# move sliding window over the sequence	# for each position of the window generate list of context words	# return list of lists pairs (central word, [context words])	batches = []	for i_central, central_word in enumerate(sequence):	# ignore the words outside of vocabulary (we don't have vectors for them)	if central_word not in self.vocab:	continue	context_words = []	# iterate from i_central - window_size, i_central + window_size	for i_context in range(i_central - self.window_size, i_central + self.window_size + 1):	# handle boundary cases	if i_context < 0 or i_context >= len(sequence):	continue	# exclude the central word itself	if i_context == i_central:	continue	context_word = sequence[i_context]	# ignore the words outside of vocabulary	if context_word not in self.vocab:	continue	# update the batch that corresponds to the current central word	context_words.append(context_word)	# update batches with the collected batch	batches.append((central_word, context_words))	return batches		def generate_negative_samples(self, central_word, context_word):	negative_samples = np.random.choice(	self.words_array,	size=self.n_negative_samples,	replace=True,  # sampling with replacement: mimics i.i.d assumption	p=self.negative_sampling_probabilities	)	return [n for n in negative_samples if n != central_word and n != context_word]		def compute_gradients(self, central_word, context_word, negative_samples):	central_vector = self.central_vectors[central_word]		# accumulate gradient of the central word	central_word_gradient = np.zeros(self.vector_size)		context_word_vector = self.context_vectors[context_word]	context_words_gradient = -central_vector * (1 - sigmoid(np.dot(context_word_vector, central_vector)))		central_word_gradient += -context_word_vector * (1 - sigmoid(np.dot(context_word_vector, central_vector)))		# a word in negative sample can be sampled many times, so we accumulate gradients	negative_samples_gradients = {n: np.zeros(self.vector_size) for n in negative_samples}	for negative_sample in negative_samples:	negative_sample_vector = self.context_vectors[negative_sample]	negative_samples_gradients[negative_sample] += central_vector * (1 - sigmoid(-np.dot(negative_sample_vector, central_vector)))	central_word_gradient += negative_sample_vector * (1 - sigmoid(-np.dot(negative_sample_vector, central_vector)))		return central_word_gradient, context_words_gradient, negative_samples_gradients		def train(self, sequences, n_epoch=5):	T = 0	for epoch in range(n_epoch):	for sequence in sequences:	# learning rate decays proportional to the number of iterations	# similar to the original word2vec code	alpha = self.learning_rate * max(0.0001, (1 - T / (n_epoch * len(sequences))))	T += 1	for batch in self.generate_batches(sequence):	central_word, context_words = batch		# subsample: randomly discard too frequent words	if np.random.random() > self.subsampling_probabilities[central_word]:	continue		for context_word in context_words:	# generate negative samples	# negative samples do not include the central word and the context word	negative_samples = self.generate_negative_samples(central_word, context_word)		# compute gradients	central_word_gradient, context_word_gradient, negative_samples_gradients = self.compute_gradients(	central_word, context_word, negative_samples	)	# perform one step of gradient descent: update all the parameters in direction opposite to gradient	self.central_vectors[central_word] -= alpha * central_word_gradient	self.context_vectors[context_word] -= alpha * context_word_gradient	for negative_sample in negative_samples:	self.context_vectors[negative_sample] -= alpha * negative_samples_gradients[negative_sample]		def get_vector(self, word):	return self.central_vectors[word]		def get_similar_words(self, word, k=10):	word_vector = self.get_vector(word)	cosine_similarities = [1 - 0.5 * cosine_distance(word_vector, self.get_vector(w)) for w in self.words_array]	return list(	reversed([	(self.words_array[i], cosine_similarities[i])	for i in np.argsort(cosine_similarities)[-k-1:-1]  # exclude the word itself	])	)"
655,4_word2vec.ipynb,41,code,-,38,"# toy example: ""A"" and ""B"" always occur near X	# all the other words (actually, characters) from the left and from the right are random	from string import ascii_lowercase	import random	from pprint import pprint		random.seed(0)	n_examples = 1000	# generate random left and right halves	left_contexts = [random.choices(ascii_lowercase, k=5) for _ in range(n_examples)]	right_contexts = [random.choices(ascii_lowercase, k=5) for _ in range(n_examples)]		# insert ""XA"" and ""XB"" to the middle	toy_dataset = [	l + list(""XA"") + r for l, r in zip(left_contexts, right_contexts)	] + [	l + list(""XB"") + r for l, r in zip(left_contexts, right_contexts)	]	random.shuffle(toy_dataset)		print(""A few lines of the toy dataset:"")	print(""\n"".join(["""".join(t) for t in toy_dataset[:3]]))	window_size = 1	vector_size = 2	learning_rate = 0.1	n_epoch = 5	n_negative_samples = 5	word2vec = Word2Vec(	seed=1,	window_size=window_size,	vector_size=vector_size,	learning_rate=learning_rate,	n_negative_samples=n_negative_samples	)	word2vec.build_vocabulary(toy_dataset)	word2vec.train(toy_dataset, n_epoch=n_epoch)	print(""Words most similar to 'A'"")	pprint(word2vec.get_similar_words(""A""))"
656,4_word2vec.ipynb,42,markdown,slide,3,## Colab quiz 3		How are word vectors computed?
657,4_word2vec.ipynb,43,code,-,1,quiz_word2vec_word_vector()()
658,4_word2vec.ipynb,44,markdown,slide,3,## Colab quiz 4		Which words are subsampled?
659,4_word2vec.ipynb,45,code,-,1,quiz_word2vec_subsampling()()
660,4_word2vec.ipynb,46,markdown,slide,3,## Colab quiz 5		Details of negative sampling.
661,4_word2vec.ipynb,47,code,-,1,quiz_word2vec_negative_sampling()()
662,4_word2vec.ipynb,48,markdown,slide,1,## Derivation of gradients
663,4_word2vec.ipynb,49,markdown,slide,2,## Exercise 1	Prove that $\sigma(-\mathbf{x}) = 1 - \sigma(\mathbf{x})$.
664,4_word2vec.ipynb,50,markdown,slide,3,## Exercise 2		Prove that $\dfrac{\mathrm{d}\sigma(\mathbf{x})}{\mathrm{d}\mathbf{x}} = \sigma(\mathbf{x})(1-\sigma(\mathbf{x}))$.
665,4_word2vec.ipynb,51,markdown,slide,5,"## Exercise 3		Prove that for arbitrary vectors $\mathbf{x}, \mathbf{y}$	- $\dfrac{\mathrm{d}(\mathbf{x} \cdot \mathbf{y})}{\mathrm{d}\mathbf{x}} = \mathbf{y}$	- $\dfrac{\mathrm{d}(\mathbf{x} \cdot \mathbf{y})}{\mathrm{d}\mathbf{y}} = \mathbf{x}$"
666,4_word2vec.ipynb,52,markdown,slide,7,"### Derivation of gradient w.r.t. central word		Consider one term in the objective function:	$$	\ell(c, o, N) = -\log\Pr(o|c) - \sum_{n \in N}\log(1 - \Pr(n|c))	$$	where $c$ is the central word, $o$ is the context (""outside"") word, $n \in N$ is a negative sample."
667,4_word2vec.ipynb,53,markdown,slide,4,"Since $\Pr(o|c) = \sigma(\mathbf{u}_o \cdot \mathbf{v}_c)$ and $1 - \Pr(n|c) = \sigma(-\mathbf{u}_n \cdot \mathbf{v}_c)$, we have	$$	\ell(c, o, N) = -\log\sigma(\mathbf{u}_o \cdot \mathbf{v}_c) - \sum_{n \in N}\log\sigma(-\mathbf{u}_n \cdot \mathbf{v}_c)	$$"
668,4_word2vec.ipynb,54,markdown,slide,3,$$	-\dfrac{\mathrm{d}\log\sigma(\mathbf{u}_o \cdot \mathbf{v}_c)}{\mathrm{d}\mathbf{v}_c}	$$
669,4_word2vec.ipynb,55,markdown,slide,3,$$	= - \dfrac{1}{\sigma(\mathbf{u}_o \cdot \mathbf{v}_c)} \dfrac{\mathrm{d}\sigma(\mathbf{u}_o \cdot \mathbf{v}_c)}{\mathrm{d}\mathbf{v}_c}	$$
670,4_word2vec.ipynb,56,markdown,slide,3,$$	= - \dfrac{1}{\sigma(\mathbf{u}_o \cdot \mathbf{v}_c)} \sigma(\mathbf{u}_o \cdot \mathbf{v}_c) (1 - \sigma(\mathbf{u}_o \cdot \mathbf{v}_c)) \dfrac{\mathrm{d} \mathbf{u}_o \cdot \mathbf{v}_c}{\mathrm{d}\mathbf{v}_c}	$$
671,4_word2vec.ipynb,57,markdown,slide,3,$$	= - (1 - \sigma(\mathbf{u}_o \cdot \mathbf{v}_c)) \mathbf{u}_o	$$
672,4_word2vec.ipynb,58,markdown,slide,7,"Similarly,		$$	\dfrac{\mathrm{d}}{\mathrm{d}\mathbf{v}_c}\left(-\sum_{n \in N}\log\sigma(-\mathbf{u}_n \cdot \mathbf{v}_c)\right)	=	\sum_{n \in N} \mathbf{u}_n(1 - \sigma(-\mathbf{u}_n\cdot \mathbf{v}_c))	$$"
673,4_word2vec.ipynb,59,markdown,slide,2,"## Exercise 4	Find derivatives $\dfrac{\mathrm{d}\ell(c, o, N)}{\mathrm{d}\mathbf{u}_o}$ and $\dfrac{\mathrm{d}\ell(c, o, N)}{\mathrm{d}\mathbf{u}_n}$."
674,4_word2vec.ipynb,60,markdown,slide,1,# Colab demo: word2vec in gensim
675,4_word2vec.ipynb,61,markdown,-,1,## How to train
676,4_word2vec.ipynb,62,code,-,14,"print(""Words most similar to 'A' (gensim implementation)"")	from gensim.models import Word2Vec		gensim_word2vec = Word2Vec(	sg=1,  # skip-gram	hs=0,  # negative sampling	size=vector_size,	window=window_size,	alpha=learning_rate,	negative=n_negative_samples	)	gensim_word2vec.build_vocab(toy_dataset)	gensim_word2vec.train(toy_dataset, total_examples=len(toy_dataset), epochs=n_epoch)	pprint(gensim_word2vec.wv.most_similar(""A""))"
677,4_word2vec.ipynb,63,code,-,18,"# try the real dataset	from sklearn.datasets import fetch_20newsgroups	dataset = fetch_20newsgroups(	subset=""all"",	shuffle=True,	random_state=1,	remove=('headers', 'footers', 'quotes')	)		from tqdm.notebook import tqdm	import spacy		# extract tokens (lemmas) with spaCy	nlp = spacy.load(""en"", disable=['parser', 'ner', 'tagger'])	def tokens(s):	return [t.lemma_ for t in nlp(s)]		texts = [tokens(text) for text in tqdm(dataset.data, desc=""tokenize"")]"
678,4_word2vec.ipynb,64,code,-,9,"# initialize, build vocabulary and train at once	gensim_word2vec = Word2Vec(	texts,	sg=1,  # skip-gram	hs=0,  # negative sampling	size=32,	window=5,  # context is a 5-word window around the target word	min_count=5  # ignore words that occur less than 5 times	).wv  # ""wv"" stands for ""word vectors"""
679,4_word2vec.ipynb,65,code,-,1,"gensim_word2vec.most_similar(""space"")"
680,4_word2vec.ipynb,66,code,-,1,"gensim_word2vec.most_similar(""gif"")"
681,4_word2vec.ipynb,67,code,-,1,"gensim_word2vec.most_similar(""hockey"")"
682,4_word2vec.ipynb,68,markdown,-,1,## Pre-trained vectors
683,4_word2vec.ipynb,69,code,-,8,"import gensim.downloader as api		# list all available models	from tabulate import tabulate	from IPython.display import display, HTML	all_gensim_models = api.info()[""models""]	gensim_model_description = [(m, all_gensim_models[m][""description""]) for m in all_gensim_models]	display(HTML(tabulate(gensim_model_description, headers=(""model name"", ""description""), tablefmt=""html"")))"
684,4_word2vec.ipynb,70,code,-,2,"# load relatively small (128MB) model	model = api.load(""glove-wiki-gigaword-100"")"
685,4_word2vec.ipynb,71,code,-,1,"model.most_similar(""hockey"")"
686,4_word2vec.ipynb,72,markdown,slide,3,"## Colab quiz 6		What is the most similar to ""twitter""?"
687,4_word2vec.ipynb,73,code,-,1,quiz_most_similar()()
688,4_word2vec.ipynb,74,markdown,slide,1,# Visualization of word vectors
689,4_word2vec.ipynb,75,code,slide,10,"display_pca_scatterplot(	model,	['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',	'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',	'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',	'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',	'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',	'homework', 'assignment', 'problem', 'exam', 'test', 'class',	'school', 'college', 'university', 'institute',]	)"
690,4_word2vec.ipynb,76,markdown,slide,1,# Properties of word vectors
691,4_word2vec.ipynb,77,code,slide,10,"display_pca_scatterplot(	model,	[	""china"", ""beijing"",	""russia"", ""moscow"",	""turkey"", ""ankara"",	""poland"", ""warsaw"",	""germany"", ""berlin"",	]	)"
692,4_word2vec.ipynb,78,markdown,slide,8,"For example,	$$	\overrightarrow{\mathrm{Warsaw}} - \overrightarrow{\mathrm{Poland}} \approx  \overrightarrow{\mathrm{Moscow}} - \overrightarrow{\mathrm{Russia}}	$$	or, equivalently	$$	\overrightarrow{\mathrm{Warsaw}} - \overrightarrow{\mathrm{Poland}}  + \overrightarrow{\mathrm{Russia}} \approx \overrightarrow{\mathrm{Moscow}}	$$"
693,4_word2vec.ipynb,79,markdown,-,1,"Thus, word vectors represent some (but not all!) relations between words (""Warsaw is related to Poland the same way Moscow is related to Russia"")."
694,4_word2vec.ipynb,80,code,slide,4,"from scipy.spatial.distance import cosine	print(""Distance between <warsaw> and <moscow>:"", cosine(model[""warsaw""], model[""moscow""]))	print(""Distance between <poland> and <russia>:"", cosine(model[""poland""], model[""russia""]))	print(""Distance between <warsaw> - <poland> + <russia> and <moscow>:"", cosine(model[""warsaw""] - model[""poland""] + model[""russia""], model[""moscow""]))"
695,4_word2vec.ipynb,81,code,slide,2,"# we can solve word analogy task with gensim	model.most_similar(positive=[""poland"", ""moscow""], negative=[""warsaw""])"
696,4_word2vec.ipynb,82,markdown,slide,1,## Colab quiz 7
697,4_word2vec.ipynb,83,code,-,3,# earl - man = X - woman	enable_mathjax_in_cell()	quiz_earl()()
698,4_word2vec.ipynb,84,markdown,slide,9,"# Summary	1. Words can be represented as non-orthogonal vectors.	1. The meaning of the word can be deduced from contexts the word appears in (distributional semantics).	1. word2vec	- Find vectors such that words with similar vectors often occur in the same context.	- Optimize log-likelihood using gradient descent.	- Details: 2 vectors for each word, negative sampling, subsampling.	1. word2vec in gensim.	1. Word vectors capture relations between words."
699,4_word2vec.ipynb,85,markdown,slide,8,"# Recommended resources	- [📖 Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)	- [📖 YDSA NLP course, lecture 1](https://github.com/yandexdataschool/nlp_course/tree/2019/week01_embeddings)	- Stanford CS224n, lecture 1	- [📺 video](https://www.youtube.com/watch?v=8rXD5-xhemo&feature=youtu.be)	- [📖 slides](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture01-wordvecs1.pdf)	- [📖 notes](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes01-wordvecs1.pdf)	- [📖 The original word2vec code](https://code.google.com/archive/p/word2vec/)"
700,4_word2vec.ipynb,86,markdown,-,3,"## [OPTIONAL] Principal Component Analysis (PCA)		The illustration of the Principal Component Analysis. In case of 2 dimensions, PCA can be understood as the best projection of 2-dimensional data into a straingt line."
701,4_word2vec.ipynb,87,code,-,16,"# project 2-dimensional ellipsoid onto a 1-dimensional line	# in other words: explain each point with a single number	def rotation_matrix(radians):	c, s = np.cos(radians), np.sin(radians)	return np.matrix([[c, s], [-s, c]])		def generate_cloud(size, var1, var2, radians):	import numpy as np	j = rotation_matrix(radians)	return np.asarray(np.random.multivariate_normal([0., 0.], [[var1, 0], [0, var2]], size=size) @ j)		cloud = generate_cloud(100, 0.5, 20, 0.5)	import matplotlib.pyplot as plt		plt.scatter(cloud[:, 0], cloud[:, 1])	plt.axis(""equal"")"
702,4_word2vec.ipynb,88,code,-,13,"# find the rotation angle via slope	from sklearn.linear_model import LinearRegression	clf = LinearRegression()	clf.fit(cloud[:, 0].reshape(-1, 1), cloud[:, 1])	# clf.coef_ is the slope = tan(rotation angles)	# rotate back	angle_to_rotate = -np.arctan(clf.coef_).item()	rotated_cloud = np.asarray(cloud @ rotation_matrix(angle_to_rotate))	plt.scatter(	rotated_cloud[:, 0],  # the first principal component	rotated_cloud[:, 1]	)	plt.axis(""equal"")"
703,4_word2vec.ipynb,89,code,-,6,"from sklearn.decomposition import PCA	trf = PCA()	result = trf.fit_transform(cloud)		plt.scatter(result[:, 0], result[:, 1])	plt.axis(""equal"")"
704,5_SUPPL_collect_actor_actor_graph.ipynb,0,markdown,-,1,# collect actor-actor graph
705,5_SUPPL_collect_actor_actor_graph.ipynb,1,markdown,-,1,Dataset desctiption: https://www.imdb.com/interfaces/
706,5_SUPPL_collect_actor_actor_graph.ipynb,2,code,-,6,!wget https://datasets.imdbws.com/title.principals.tsv.gz	!gunzip title.principals.tsv.gz	!wget https://datasets.imdbws.com/name.basics.tsv.gz	!gunzip name.basics.tsv.gz	!wget https://datasets.imdbws.com/title.basics.tsv.gz	!gunzip title.basics.tsv.gz
707,5_SUPPL_collect_actor_actor_graph.ipynb,3,code,-,2,!wc -l title.principals.tsv	!wc -l tconst_nconst.tsv
708,5_SUPPL_collect_actor_actor_graph.ipynb,4,code,-,8,"# we are interested in a small subset of data, so filter out unnecessary rows and columns		# title.basics.tsv: keep only tconst and primaryTitle	!cut -f1,3 title.basics.tsv > tconst_primaryTitle.tsv	# name.basics.tsv: keep only nconst and primaryName	!cut -f1,2 name.basics.tsv > nconst_primaryName.tsv	# title.principals.tsv: keep only rows with actors or actresses, keep tconst, nconst	! grep -E $'\t(actor|actress)\t' title.principals.tsv | cut -f1,3 > tconst_nconst.tsv"
709,5_SUPPL_collect_actor_actor_graph.ipynb,5,code,-,3,"import pandas as pd	# names of actors, directors, etc	tconst_nconst = pd.read_csv(""tconst_nconst.tsv"", sep=""\t"", names=[""tconst"", ""nconst""])"
710,5_SUPPL_collect_actor_actor_graph.ipynb,6,code,-,1,"print(""tconst_nconst memory usage, MB:"", tconst_nconst.memory_usage().sum() / 1024 / 1024)"
711,5_SUPPL_collect_actor_actor_graph.ipynb,7,code,-,1,tconst_nconst.head()
712,5_SUPPL_collect_actor_actor_graph.ipynb,8,code,-,16,"# the main graph	from tqdm.notebook import tqdm	import networkx as nx		edges = []		for index, row in tqdm(tconst_nconst.iterrows(), total=tconst_nconst.shape[0]):	edges.append((row[""tconst""], row[""nconst""]))		title_actor_graph = nx.Graph()	title_actor_graph.add_edges_from(edges)	actor_actor_graph = nx.algorithms.bipartite.weighted_projected_graph(	title_actor_graph,	[e[1] for e in edges]	)	print(""Total edges:"", len(actor_actor_graph.edges))"
713,5_SUPPL_collect_actor_actor_graph.ipynb,9,code,-,4,"with open(""actor_actor_graph.txt"", ""w"") as f:	for e in actor_actor_graph.edges.data('weight'):	src, tgt, w = e	f.write(f""{src}\t{tgt}\t{w}\n"")"
714,5_word2vec_continued.ipynb,0,code,-,3,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell and restart runtime</font></b>		!python -m spacy download en_core_web_lg"
715,5_word2vec_continued.ipynb,1,code,-,18,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	sys.path.append('harbour-space-text-mining-course')		from tmcourse.utils import (	enable_mathjax_in_cell,	)	from tmcourse.node2vec import Node2VecGraph	# from tmcourse.demo import (	# )	# from tmcourse.quiz import (	# )	from tmcourse.ipyquiz import Quiz	from tqdm.notebook import tqdm	!pip install fasttext"
716,5_word2vec_continued.ipynb,2,markdown,slide,1,<center><h1>More about word vectors</h1></center>
717,5_word2vec_continued.ipynb,3,markdown,slide,6,# Plan for today	1. GloVe	1. node2vec	1. FastText	1. Word vectors in spaCy	1. Coding session: build news summarizer with word2vec
718,5_word2vec_continued.ipynb,4,markdown,slide,1,# GloVe
719,5_word2vec_continued.ipynb,5,markdown,slide,1,"In word2vec, we predict co-occurrence probabilities $\Pr(o|c)$ using dot product of vectors for $o$ and $c$."
720,5_word2vec_continued.ipynb,6,markdown,slide,1,**Insight**: _ratios_ of co-occurrence probabilities also give information: they encode meaningful componens.
721,5_word2vec_continued.ipynb,7,markdown,slide,1,![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ratios-1.png)
722,5_word2vec_continued.ipynb,8,markdown,slide,1,![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ratios-2.png)
723,5_word2vec_continued.ipynb,9,markdown,slide,9,	**Idea**: if we predict	$$	\log\Pr(o|c) = \mathbf{v}_o \cdot \mathbf{v}_c	$$	then _vector differences_ correspond to ratios of co-oocurrence probabilities!	$$	\log\dfrac{\Pr(o|c_2)}{\Pr(o|c_1)} = \mathbf{v}_o \cdot (\mathbf{v}_{c_2} - \mathbf{v}_{c_1})	$$
724,5_word2vec_continued.ipynb,10,markdown,slide,12,"	So use the following objective function (GloVe):	$$	L(\theta) = \sum_{i \in V, j \in V, i \neq j} f(X_{ij}) \cdot (\mathbf{v}_i \cdot \tilde{\mathbf{v}}_j + b_i + \tilde{b}_j - X_{ij})^2	$$	where	- $i, j$ -- words	- $X_{ij}$ -- how many times words $i$ and $j$ co-occur	- $\mathbf{v}, \tilde{\mathbf{v}}, b, \tilde{b}$ -- parameters	- $f(X_{ij})$ -- suppress rare (noisy) counters		![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/f.png)"
725,5_word2vec_continued.ipynb,11,code,slide,8,"import gensim.downloader as api		# list all available models	from tabulate import tabulate	from IPython.display import display, HTML	all_gensim_models = api.info()[""models""]	gensim_model_description = [(m, all_gensim_models[m][""description""]) for m in all_gensim_models]	display(HTML(tabulate(gensim_model_description, headers=(""model name"", ""description""), tablefmt=""html"")))"
726,5_word2vec_continued.ipynb,12,markdown,slide,2,# node2vec	
727,5_word2vec_continued.ipynb,13,markdown,slide,1,word2vec algorithm is actually not specific for texts: it is applicable for any _discrete sequences_ (if the total number of elements is finite).
728,5_word2vec_continued.ipynb,14,markdown,slide,5,Can we apply it for nodes in graphs?	- Find similar nodes.	- Extract features for machine learning.		![alt text](https://snap.stanford.edu/node2vec/homo.png)
729,5_word2vec_continued.ipynb,15,markdown,slide,1,"**Idea**: graph traversal (BFS, DFS, ...) returns a sequence of nodes - what if we apply word2vec?"
730,5_word2vec_continued.ipynb,16,markdown,slide,7,"node2vec algorithm:	- For each vertex, generate $n$ random walks of length $k$ each	- Parametrize random walks to make it ""BFS-like"" or ""DFS-like"".		Example of transition from $t$ to $v$: $x_1$ is the neighbor of $t$, $x_2$ and $x_3$ are not.		![alt text](https://snap.stanford.edu/node2vec/walk.png)"
731,5_word2vec_continued.ipynb,17,markdown,slide,1,## Colab demo: find similar GoT characters with node2vec
732,5_word2vec_continued.ipynb,18,code,-,4,"# read more about the data: https://www.kaggle.com/moradnejad/interaction-networks-for-game-of-thrones-saga	import pandas as pd	df = pd.read_csv(""harbour-space-text-mining-course/datasets/got/asoiaf-book5-edges.csv"")	df.head()"
733,5_word2vec_continued.ipynb,19,code,-,12,"# create the weighted graph	import networkx as nx	G = nx.Graph()	for i, r in df.iterrows():	G.add_edge(r[""Source""], r[""Target""], weight=r[""weight""])		# preprocess the graph and generate random walks	node2vec_G = Node2VecGraph(G, p=1.0, q=1.0)	walks = node2vec_G.simulate_walks(num_walks=30, walk_length=10)	print(""Total walks:"", len(walks))	from pprint import pprint	pprint(walks[0])"
734,5_word2vec_continued.ipynb,20,code,-,15,"# train word2vec on walks	from gensim.models import Word2Vec		gensim_word2vec = Word2Vec(	sg=1,  # skip-gram	hs=0,  # negative sampling	size=200,	window=3,	alpha=0.01,	negative=20	)	gensim_word2vec.build_vocab(walks)	gensim_word2vec.train(walks, total_examples=len(walks), epochs=10)	# find characters similar to Jon Show	pprint(gensim_word2vec.wv.most_similar(""Jon-Snow""))"
735,5_word2vec_continued.ipynb,21,markdown,slide,1,# FastText
736,5_word2vec_continued.ipynb,22,markdown,slide,3,## Word vectors with subword information		Sometimes we may guess (at least partially) the meaning of a word by looking at its pieces.
737,5_word2vec_continued.ipynb,23,markdown,slide,1,"For example, for the word ""athazagoraphobia"" we may guess it is a phobia (a fear of something)."
738,5_word2vec_continued.ipynb,24,markdown,slide,1,"(In case you are wondering, ""athazagoraphobia"" is the fear of being forgotten or ignored and fear of forgetting.)"
739,5_word2vec_continued.ipynb,25,markdown,slide,1,"**Idea**: learn not only word vectors, but also subword vectors."
740,5_word2vec_continued.ipynb,26,markdown,slide,5,"**Algorithm**:	- Represent each word as a collection of character $n$-gram.	- Include the word w itself in the set of its $n$-grams.	- Add special boundary symbols `<` and `>` at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences.	- Word vector is the sum of the vector representations of its $n$-grams."
741,5_word2vec_continued.ipynb,27,markdown,slide,5,"**Example**: for $n=3$, the word ""where"" is represented as		`<wh, whe, her, ere, re>, <where>`		> Note that the sequence `<her>`, corresponding to the word ""her"" is different from the trigram `her` from the word ""where""."
742,5_word2vec_continued.ipynb,28,markdown,slide,1,## Colab demo: FastText (unsupervised)
743,5_word2vec_continued.ipynb,29,code,-,1,import fasttext
744,5_word2vec_continued.ipynb,30,code,-,2,"from sklearn.datasets import fetch_20newsgroups	dataset = fetch_20newsgroups(subset=""all"", shuffle=True, random_state=1, categories=(""sci.space"",), remove=('headers', 'footers', 'quotes'))"
745,5_word2vec_continued.ipynb,31,code,-,4,"def prepare_text(t):	import re	s = "" "".join(t.split()).lower()  # no newlines, lowercase	return re.sub(r'[^\w\s]',' ',s)  # remove punctuation"
746,5_word2vec_continued.ipynb,32,code,-,4,"# fasttest works with files	with open(""texts.txt"", ""w"") as f:	for t in dataset.data:	f.write(prepare_text(t) + ""\n"")"
747,5_word2vec_continued.ipynb,33,code,-,4,"model = fasttext.train_unsupervised(	input=""texts.txt"",	epoch=10	)"
748,5_word2vec_continued.ipynb,34,code,-,1,"model.get_nearest_neighbors(""satellite"")"
749,5_word2vec_continued.ipynb,35,code,-,1,"model.get_nearest_neighbors(""satelite"")"
750,5_word2vec_continued.ipynb,36,markdown,slide,6,"## Text classification		**Idea**:	- Represent a text as the average of word vectors.	- Learn matrix $A$ such that $\mathrm{softmax}(A \cdot v)$ gives probability distribution over classes.	- Instead of the loss function for word2vec (negative log-likelihood), optimize the loss function for classification (cross-entropy)."
751,5_word2vec_continued.ipynb,37,code,slide,13,"import matplotlib.pyplot as plt	from scipy.special import softmax	fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))		x = [-0.1, -1, 0.7, 0.5, 0.33]	i = list(range(len(x)))	ax1.bar(i, x)	ax2.bar(i, softmax(x))	ax1.set_title(r""$X = {}$"".format(x))	ax2.set_title(r""$\mathrm{softmax}(X) \sim e^X$"")	ax1.set_xticks([])	ax2.set_xticks([])	plt.show()"
752,5_word2vec_continued.ipynb,38,markdown,slide,1,## Colab demo: FastText (supervised)
753,5_word2vec_continued.ipynb,39,code,-,8,"from sklearn.datasets import fetch_20newsgroups	fetch_params = dict(	shuffle=True,	random_state=1,	remove=('headers', 'footers', 'quotes')	)	train_dataset = fetch_20newsgroups(subset=""train"", **fetch_params)	test_dataset = fetch_20newsgroups(subset=""test"", **fetch_params)"
754,5_word2vec_continued.ipynb,40,code,-,12,"def convert_dataset_to_fasttext_file(texts, labels, path):	with open(path, ""w"") as f:	for t, l in zip(texts, labels):	t = prepare_text(t)	# labels are stored in file with prefix ""__label__""	f.write(f""__label__{l}\t{t}\n"")		convert_dataset_to_fasttext_file(	train_dataset.data,	[train_dataset.target_names[c] for c in train_dataset.target],	""train.txt""	)"
755,5_word2vec_continued.ipynb,41,code,-,1,!head -n1 train.txt
756,5_word2vec_continued.ipynb,42,code,-,5,"model = fasttext.train_supervised(	input=""train.txt"",	lr=0.1,	epoch=25	)"
757,5_word2vec_continued.ipynb,43,code,-,9,"total = 0	correct = 0	for t, c in zip(test_dataset.data, test_dataset.target):	total += 1	label = test_dataset.target_names[c]	predicted_label = model.predict(prepare_text(t))[0][0][len(""__label__""):]	if label == predicted_label:	correct += 1	print(f""accuracy: {correct / total}"")"
758,5_word2vec_continued.ipynb,44,markdown,slide,1,# Colab demo: word vectors in spaCy
759,5_word2vec_continued.ipynb,45,code,-,5,"import spacy	# spaCy is equipped by pre-trained word vectors	# for example, they are stored in ""en_core_web_lg""	# in Colab, you need to run !python -m spacy download en_core_web_lg (the first cell) and restart runtime	nlp = spacy.load(""en_core_web_lg"", disable=[""tagger"", ""ner"", ""parser""])"
760,5_word2vec_continued.ipynb,46,code,-,8,"doc = nlp(""dog cat banana afskfsd"")		# you can check the attribute `.has_vector` of a token	for token in doc:	print(f""Token '{token.text}' has vector: {token.has_vector}"")		# doc (parsed text, the collection of tokens) also has vector (the average over all tokens)	print(f""Doc has vector: {doc.has_vector}"")"
761,5_word2vec_continued.ipynb,47,code,-,6,"# for any pair of objects that have vectors, we can find similarity	# the example from the last lesson	doc1 = nlp(""How can I be a good geologist?"")	doc2 = nlp(""What should I do to be a great geologist?"")	doc3 = nlp(""What should I do to be a good geologist?"")	doc1.similarity(doc2), doc1.similarity(doc3)"
762,5_word2vec_continued.ipynb,48,code,-,4,"# in particular, any subsequence of a document (so-called Span) also may have a vector	# for example, a sentence in a document has vector (the average over all tokens of the sentence)	# to split the document into sentences, we need to add 'sentencizer' to NLP pipeline	nlp.add_pipe(nlp.create_pipe('sentencizer'))"
763,5_word2vec_continued.ipynb,49,code,-,15,"from sklearn.datasets import fetch_20newsgroups	dataset = fetch_20newsgroups(	subset=""all"",	shuffle=True,	random_state=1,	categories=(""sci.space"",),	remove=('headers', 'footers', 'quotes')	)	from pprint import pprint		# after the sentenciser has been added, the parsed document contains `.sents` attribute:	for sentence in nlp(dataset.data[0]).sents:	print(""SENTENCE:\n"", sentence)	print(""HAS VECTOR:"", sentence.has_vector)	print(""---"")"
764,5_word2vec_continued.ipynb,50,markdown,slide,11,"# Coding session		Build news summarizer using word2vec.		Try the following idea:	1. Assume that there is a ""central"" vector (the main idea) of the article which is the average vector of all tokens in the article.	1. Find $k$ sentences closest to the ""central"" vector. The assumption is that these sentences are ""informative"", and other sentences are ""noisy"" and not important.		The function `parse_techcrunch_url(url)` parses the content from [techcrunch.com](https://techcrunch.com), you can use it to check your implementation on real data.		Also, implement a random baseline (get $k$ random sentences from the text) and compare the results."
765,5_word2vec_continued.ipynb,51,code,-,15,"def parse_techcrunch_url(url):	import bs4	import re	import requests	response = requests.get(url)	soup = bs4.BeautifulSoup(response.text, ""html.parser"")	items = soup.find(""div"", {""class"": ""article-content""}).findAll(""p"")	raw_html = ""\n"".join(map(str, items))	cleanr = re.compile('<.*?>')	clean_html = re.sub(cleanr, '', raw_html)	return clean_html		from pprint import pprint	url = ""https://techcrunch.com/2020/05/23/hackers-iphone-new-jailbreak/""	pprint(parse_techcrunch_url(url))"
766,5_word2vec_continued.ipynb,52,code,-,2,def summarize_text(text):	# YOUR CODE HERE
767,5_word2vec_continued.ipynb,53,markdown,slide,7,"# Recommended resources	- [node2vec](https://snap.stanford.edu/node2vec/)	- FastText	- [Documentation](https://fasttext.cc/)	- [Paper ""Enriching Word Vectors with Subword Information""](https://arxiv.org/abs/1607.04606)	- [Paper ""Bag of Tricks for Efficient Text Classification""](https://arxiv.org/abs/1607.01759)	- [vectors in spaCy](https://spacy.io/usage/vectors-similarity)"
768,6.5_pytorch.ipynb,0,markdown,-,19,"# Coding session: PyTorch		You are asked to build a classifier that solves XOR classification problem from the lecture.		The classifier should have one hyperparameter $N$ -- the size of hidden layer.		It should have the following layers:	- $L_1(x) = \mathrm{relu}(W_1 x + b_1)$, where $W_1$ has $N$ rows and 2 columns, $b_1$ has $N$ components.	- $L_2(x) = \mathrm{relu}(W_2 x + b_2)$, where $W_2$ has $N$ rows and $N$ columns, $b_1$ has $N$ components.	- $L_3(x) = \mathrm{softmax}(W_3 x + b_3)$, where $W_2$ has $2$ rows and $N$ columns, $b_1$ has $2$ components.		So, the parameters are $W_1, b_1, W_2, b_2, W_3, b_3$. For the input vector $x$, computations should be the following:	- $A = W_1 x + b_1$.	- $B = \mathrm{relu}(A)$	- $C = W_2 B + b_2$	- $D = \mathrm{relu}(C)$	- $E = W_3 D + b_3$	- $F = \mathrm{softmax}(E)$	- return $F$"
769,6.5_pytorch.ipynb,1,markdown,-,3,"Helper functions for you:	- `generate_data()`. Returns points ($(x, y)$ pairs) and labels (0 or 1) for XOR problem.	- `visualize_clf(clf, points)`. Applies PyTorch classifier to points generated by `generate_data()`."
770,6.5_pytorch.ipynb,2,code,-,35,"import numpy as np	from tqdm.notebook import tqdm	import matplotlib.pyplot as plt	import torch		def generate_data(xmin=-15, xmax=15, ymin=-15, ymax=15):	points = []	labels = []	for x in range(xmin, xmax):	for y in range(ymin, ymax):	points.append((float(x), float(y)))	labels.append(int(x * y < 0))	return points, labels		def visualize_clf(clf, points):	import matplotlib.pyplot as plt	plt.axis('equal')	X = []	Y = []	C = []	for p in points:	x, y = p	r, g, b, a = 0, 0, 0, 0	result = clf.forward(torch.tensor([x, y]))	prob_1 = result[1].item()	if prob_1 > 0.5:	b = 1	else:	r = 1	a = max(prob_1, 1 - prob_1)	X.append(x)	Y.append(y)	C.append((r, g, b, a))	plt.scatter(X, Y, c=C)	plt.show()"
771,6.5_pytorch.ipynb,3,markdown,-,3,## Exercise 1		Implement the class `XORSolver` according to the description of the coding session.
772,6.5_pytorch.ipynb,4,code,-,18,"import torch	from torch import nn			class XORSolver(nn.Module):	def __init__(self, N=10):	super().__init__()	torch.manual_seed(0)	# YOUR CODE HERE	# define the parameters W1, b1, W2, b2, W3, b3	# initialize randomly		def forward(self, xy):	# YOUR CODE HERE	# input is a 1-dimensional tensor: xy[0] is x, xy[0] is y	# implement the computations according to the description:	# linear -> relu -> linear-> relu -> linear -> softmax	# compute softmax with additional argument dim=0"
773,6.5_pytorch.ipynb,5,markdown,-,4,## Exercise 2	Create the `XORSolver` with $N=10$ and visualize its predictions using the function `visualize_clf`. It accepts two arguments: the classifier and the set of points (it doesn't accept true labels).		`points` and `labels` are already created for you.
774,6.5_pytorch.ipynb,6,code,-,4,"points, labels = generate_data()	N = 10		# YOUR CODE HERE"
775,6.5_pytorch.ipynb,7,markdown,-,5,## Exercise 3		- Complete the implementation of function `train_clf` that trains the classifier.	- Train the classifier from the previous exercises.	- Visualize the resulting classifier. Does it solve the XOR problem?
776,6.5_pytorch.ipynb,8,code,-,36,"def train_clf(clf, points, labels, num_iterations, learning_rate, seed=0):	np.random.seed(seed)	# note that we are using CrossEntropyLoss for classification	loss_function = nn.CrossEntropyLoss()		for t in tqdm(range(num_iterations)):	lr = learning_rate / np.sqrt(t + 1)		# here we pick a random training sample	idx = np.random.choice(range(len(points)))	x, y = points[idx]	label = labels[idx]		# `label` is a number	# you need to convert it into a one-dimensional tensor `target`	# this is because CrossEntropyLoss accepts a vector of targets	target = # YOUR CODE HERE		# compute the prediction	# you need to convert it into 2-dimensional matrix with one row!	# this is because CrossEntropyLoss accepts a matrix of predictions	# you can do it using the `unsqueeze()` method	prediction = # YOUR CODE HERE		loss = loss_function(prediction, target)	clf.zero_grad()	loss.backward()		with torch.no_grad():	# update the parameters	# `lr` is the learning rate	for parameter in clf.parameters():	# YOUR CODE HERE		# YOUR CODE HERE: train the XORSolver using train_clf	# YOUR CODE HERE: visualize the result"
777,6.5_pytorch.ipynb,9,markdown,-,4,## Exercise 4.		Implement the XOR classifier using `nn.Sequential`.	Train and visualize the result (like in the Exercise 3).
778,6.5_pytorch.ipynb,10,code,-,6,clf2 = nn.Sequential(	# YOUR CODE HERE	)		# YOUR CODE HERE: train clf2	# YOUR CODE HERE: visualize the result
779,6_neural_networks_backpropagation.ipynb,0,code,-,33,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	if 'harbour-space-text-mining-course' not in sys.path:	sys.path.append('harbour-space-text-mining-course')		from tmcourse.demo import (	demo_function_approximation,	demo_gradient_descent,	demo_computational_graph,	demo_universal_approximator,	demo_pytorch_computational_graph,	demo_2d_classification,	)	from tmcourse.quiz import (	quiz_bumps,	quiz_derivative,	quiz_correct_computational_graph,	quiz_derivative_pytorch,	)	from tmcourse.ipyquiz import Quiz, Function	from tmcourse.utils import enable_mathjax_in_cell		from IPython.display import HTML, display		# get_ipython().events.register('pre_run_cell', enable_mathjax_in_cell)	from IPython.display import set_matplotlib_formats	%matplotlib inline	set_matplotlib_formats('svg')	import matplotlib.pyplot as plt	plt.rcParams['figure.figsize'] = (10.0, 6.0)"
780,6_neural_networks_backpropagation.ipynb,1,markdown,slide,1,<h1><center>Backpropagation. Neural networks.</center></h1>
781,6_neural_networks_backpropagation.ipynb,2,markdown,slide,9,	# Outline	1. A universal approximator	2. Train the universal approximator with gradient descent	3. Computational graphs and backpropagation	4. Layers: forward-backward API	5. XOR problem: add more layers	6. Feedforward neural networks	7. PyTorch basics
782,6_neural_networks_backpropagation.ipynb,3,markdown,slide,1,# A universal approximator
783,6_neural_networks_backpropagation.ipynb,4,markdown,slide,1,"Consider the regression problem: approximate the unknown function $f(x)$ given the finite set of values $\{(x_n, f(x_n)\}_{1\leq n \leq N}$."
784,6_neural_networks_backpropagation.ipynb,5,markdown,slide,1,"In practice, we always want the approximation to be computationally efficient."
785,6_neural_networks_backpropagation.ipynb,6,code,slide,14,"	import numpy as np	import matplotlib.pyplot as plt		np.random.seed(0)	x = np.linspace(-1, 2, 20)	y = np.cosh(x)		plt.figure(figsize=(8, 4))	plt.plot(x, y, 'o')	plt.plot(x, y, '--')	plt.legend([r""Observations of $f(x)$"", r""$f(x)$""])		plt.show()"
786,6_neural_networks_backpropagation.ipynb,7,markdown,slide,3,"	The simplest approximation is a linear model $ax+b$.	Clearly, it is not always good."
787,6_neural_networks_backpropagation.ipynb,8,code,slide,9,"	plt.figure(figsize=(8, 4))	linear_approximation = np.poly1d(np.polyfit(x, y, 1))	plt.plot(	x, y, 'o',	x, linear_approximation(x), '-'	)	plt.legend([r""Observations of $f(x)$"", r""Linear approximation: $f(x) \approx ax + b$""])	plt.show()"
788,6_neural_networks_backpropagation.ipynb,9,markdown,slide,5,**Theorem**: any function can be approximated with the linear combination of nonlinear functions which linearly depend on $x$:	$$	f(x) \approx \sum\limits_{1 \leq k \leq K} w_k \cdot g(a_k x+b_k)	$$	$K$ is a hyperparameter (the number of nonlinear terms) and $g$ is any nonlinear function (also a hyperparameter).
789,6_neural_networks_backpropagation.ipynb,10,markdown,slide,6,We will prove it using the _Heaviside function_ as the function $g$:	$$	g(x) = \begin{cases} 1 & \text{if } x \geq 0 \\	0 & \text{otherwise}	\end{cases}	$$
790,6_neural_networks_backpropagation.ipynb,11,code,slide,5,"	X = np.linspace(-2, 2, 1000)	plt.plot(X, np.heaviside(X, 1), '-')	plt.legend([""Heaviside function""])	plt.show()"
791,6_neural_networks_backpropagation.ipynb,12,markdown,slide,4,"### Exercise 1	Prove that the Heaviside function is nonlinear.		**Hint**: if a function $f(x)$ is linear, then $f(x_1 + x_2) = f(x_1) + f(x_2)$ for all $x_1$, $x_2$."
792,6_neural_networks_backpropagation.ipynb,13,markdown,slide,1,Why is the Heaviside function is sufficient to approximate $f(x) \approx \sum\limits_{1 \leq k \leq K} w_k \cdot g(a_k x+b_k)$?
793,6_neural_networks_backpropagation.ipynb,14,markdown,slide,1,"Consider the term $w \cdot g(ax + b)$. Each parameter $w, a, b$ corresponds to a certain transformation."
794,6_neural_networks_backpropagation.ipynb,15,markdown,slide,1,The coefficient $b$ shifts left or right.
795,6_neural_networks_backpropagation.ipynb,16,code,slide,8,"	X = np.linspace(-2, 2, 1000)	plt.plot(	X, np.heaviside(X, 1), '-',	X, np.heaviside(X - 1, 1), '--'	)	plt.legend([r""$g(x)$"", r""$g(x-1)$""])	plt.show()"
796,6_neural_networks_backpropagation.ipynb,17,markdown,slide,1,The coefficient $a$ mirrors.
797,6_neural_networks_backpropagation.ipynb,18,code,slide,8,"	X = np.linspace(-2, 2, 1000)	plt.plot(	X, np.heaviside(X, 1), '-',	X, np.heaviside(-X, 1), '--'	)	plt.legend([r""$g(x)$"", r""$g(-x)$""])	plt.show()"
798,6_neural_networks_backpropagation.ipynb,19,markdown,slide,1,The coefficient $w$ scales.
799,6_neural_networks_backpropagation.ipynb,20,code,slide,8,"	X = np.linspace(-2, 2, 1000)	plt.plot(	X, np.heaviside(X, 1), '-',	X, -0.5 * np.heaviside(X, 1), '--'	)	plt.legend([r""$g(x)$"", r""$-\frac{1}{2}g(x)$""])	plt.show()"
800,6_neural_networks_backpropagation.ipynb,21,markdown,slide,9,"**Idea**: use Heaviside functions $g$ to construct a ""bump"" of height $h$ in the interval $[x_0, x_1]$:	$$\Pi_{x_0, x_1, h}(x) = \begin{cases} h & \text{if } x_0 \leq x \leq x_1 \\	0 & \text{otherwise}	\end{cases}$$		using the formula:	$$	\Pi_{x_0, x_1, h}(x) = \frac{h}{2}g(x-x_0) + \frac{h}{2}g(-(x-x_1)) - \frac{h}{2}g(-(x-x_0)) - \frac{h}{2}g(x-x_1)	$$"
801,6_neural_networks_backpropagation.ipynb,22,code,slide,26,"	X = np.linspace(-2, 2, 1000)		plt.plot(	X,	1.5 * np.heaviside(X - 0.5, 1) + 1.5 * np.heaviside(-(X - 1), 1)-1.5 * np.heaviside(-X + 0.5, 1)-1.5 * np.heaviside(X - 1, 1),	'-',	linewidth=3.0	)	plt.plot(	X, 1.5 * np.heaviside(X - 0.5, 1), '--',	X, 1.5 * np.heaviside(-(X - 1), 1), '--',	X, -1.5 * np.heaviside(-X + 0.5, 1), '--',	X, -1.5 * np.heaviside(X - 1, 1), '--',	)	plt.legend(	[	r""$\Pi_{x_0=\frac{1}{2}, x_1=1, h=3}(x)$"",	r""$\frac{h}{2}g(x-x_0)$"",	r""$\frac{h}{2}g(-(x-x_1))$"",	r""$-\frac{h}{2}g(-(x-x_0))$"",	r""$-\frac{h}{2}g(x-x_1)$"",	],	fontsize=""large""	)	plt.show()"
802,6_neural_networks_backpropagation.ipynb,23,markdown,slide,1,"Using ""bumps"", we can make a piecewise-constant approximation of any function $f(x)$."
803,6_neural_networks_backpropagation.ipynb,24,code,slide,23,"	plt.plot(x, y, 'o')	plt.plot(x, y, '--')		x_approx = []	y_approx = []	for i in range(len(x)):	if i == 0:	dleft = 0	else:	dleft = x[i] - x[i-1]	if i == len(x) - 1:	dright = 0	else:	dright = x[i+1] - x[i]	x_approx.append(x[i] - dleft/2)	y_approx.append(y[i])	x_approx.append(x[i] + dright/2)	y_approx.append(y[i])	plt.plot(x_approx, y_approx, '-')	plt.legend([r""Observations of $f(x)$"", r""$f(x)$"", ""Piecewise-constant approximation of f(x)""])		plt.show()"
804,6_neural_networks_backpropagation.ipynb,25,markdown,slide,12,"### Colab demo 1: function approximation		In the demo `demo_function_approximation()` we can try as $g$:	- `step` - the step, or Heaviside, function	- `relu` - Rectified Linear Unit: $g(x) = \max(0, x)$		The hyperparameter $K$ is set by `num_functions`.		For example, try to construct a bump $\Pi_{x_0, x_1, h}(x)$ using the formula	$$	\Pi_{x_0, x_1, h}(x) = \frac{h}{2}g(x-x_0) + \frac{h}{2}g(-(x-x_1)) - \frac{h}{2}g(-(x-x_0)) - \frac{h}{2}g(x-x_1)	$$"
805,6_neural_networks_backpropagation.ipynb,26,markdown,slide,0,
806,6_neural_networks_backpropagation.ipynb,27,code,-,1,demo_function_approximation(num_functions=4)
807,6_neural_networks_backpropagation.ipynb,28,markdown,slide,2,## Colab quiz 1	Choose the correct approximation.
808,6_neural_networks_backpropagation.ipynb,29,code,-,2,enable_mathjax_in_cell()	quiz_bumps()()
809,6_neural_networks_backpropagation.ipynb,30,markdown,slide,3,# Train the universal approximator with gradient descent		
810,6_neural_networks_backpropagation.ipynb,31,markdown,slide,5,Our goal is to find a function of the form	$$	h(x) = \sum\limits_{1 \leq i \leq K} w_k g(a_k x + b_k)	$$	which approximates $f(x)$ the best.
811,6_neural_networks_backpropagation.ipynb,32,markdown,slide,4,"	All we know about $f(x)$ is the set of $N$ input-output pairs $\lbrace(x_n, f(x_n))\rbrace_{1 \leq n \leq N}$.		Let's denote **feature matrix** $X = \{ x_n \}_{1 \leq n \leq N}$ and **target vector** $y = \{ f(x_n) \}_{1 \leq n \leq N}$."
812,6_neural_networks_backpropagation.ipynb,33,code,slide,11,"import numpy as np	import matplotlib.pyplot as plt		x = np.linspace(-1, 2, 10)	y = np.cosh(x)		plt.figure(figsize=(8, 4))	plt.plot(x, y, 'o')	plt.legend([r""Observations of $f(x)$""])		plt.show()"
813,6_neural_networks_backpropagation.ipynb,34,markdown,slide,1,The standard machine learning approach is:
814,6_neural_networks_backpropagation.ipynb,35,markdown,slide,1,"**Step 1.** Somehow choose _hyperparameters_ $K$, $g$. There are still many functions $h(x)$!"
815,6_neural_networks_backpropagation.ipynb,36,code,slide,18,"	relu = lambda x: np.maximum(x, 0)	h1 = lambda x: relu(3*x - 2)	x_smooth = np.linspace(-1, 2, 500)	h1 = lambda x: relu(3*x - 2) + relu(-x)	h2 = lambda x: 3*relu(0.5*x) + 0.5*relu(-x+0.5)		plt.plot(x, y, 'o', x_smooth, h1(x_smooth), x_smooth, h2(x_smooth))	plt.title(r""Two hypotheses with hyperparameters $K=2, g = \mathrm{relu}$"")	plt.legend(	[	r""Observations of $f(x)$"",	r""$h_1(x) = g(3x-2) + g(-x)$"",	r""$h_2(x) = 3g\left(\frac{1}{2}x\right) + \frac{1}{2}g\left(-x+\frac{1}{2}\right)$"",	]	)		plt.show()"
816,6_neural_networks_backpropagation.ipynb,37,markdown,slide,2,"**Step 2.** Decide how to measure the difference between a hypothesis $h(x)$ and observations $X, y$.	This measure is called **loss function** $L(x, y, h)$."
817,6_neural_networks_backpropagation.ipynb,38,markdown,slide,4,"A loss function is usually the average value of individual losses $\ell$ for each observation:	$$	L(x, y, h) = \dfrac{1}{N}\sum\limits_{1 \leq n \leq N} \ell(y_n, h(x_n))	$$"
818,6_neural_networks_backpropagation.ipynb,39,markdown,slide,1,"For example, **mean squared error (MSE)** loss is given by $\ell(y, h) = \frac{1}{2}(y - h)^2$."
819,6_neural_networks_backpropagation.ipynb,40,code,slide,31,"	relu = lambda x: np.maximum(x, 0)	x_smooth = np.linspace(-1, 2, 500)	h1 = lambda x: relu(3*x - 2) + relu(-x)		plt.plot(x, y, 'o', x_smooth, h1(x_smooth))	losses = []	for i, item in enumerate(zip(x, y)):	n = i+1	xi, yi = item	l = 0.5 * (yi - h1(xi))**2	losses.append(l)	plt.plot([xi, xi], [yi, h1(xi)], c=""r"")	plt.annotate(r""$y_{{{}}} = {:.2f}$"".format(n, yi), (xi, yi), ha='center')	plt.annotate(r""$h(x_{{{}}}) = {:.2f}$"".format(n, h1(xi)), (xi, h1(xi)), ha='center')	plt.annotate(""{:.2f}"".format(l), (xi, 0.5*(yi + h1(xi))), c=""r"")		plt.legend(	[	r""Observations of $f(x)$"",	r""$h(x) = g(3x-2) + g(-x)$"",	]	)		plt.show()		display(	HTML(	r""$L(X, y, h) = \dfrac{1}{N}\sum_{1 \leq n \leq N}\frac{1}{2}(y_n-h(x_n))^2="" + r""{:.2f}$"".format(np.mean(losses))	)	)"
820,6_neural_networks_backpropagation.ipynb,41,markdown,slide,1,"**Step 3.** Minimize the loss function $L(X, y, h)$ using **gradient descent**."
821,6_neural_networks_backpropagation.ipynb,42,markdown,slide,3,"What does $L(X, y, h)$ depend on?	- Observations $X, y$: cannot change them at all	- Hyperparameters $K, g$ of function $h$: chosen and fixed"
822,6_neural_networks_backpropagation.ipynb,43,markdown,slide,8,"Recall that	$$	h(x) = \sum\limits_{1 \leq k \leq K} w_k \cdot g(a_k x+b_k)	$$	So $L(x, y, h)$ ultimately depends on parameters $a_1, \dots, a_K, b_1, \dots, b_k, w_1, \dots, w_k$:	$$	L(X, y, h) = L(a_1, \dots, a_K, b_1, \dots, b_K, w_1, \dots, w_K)	$$"
823,6_neural_networks_backpropagation.ipynb,44,markdown,slide,3,"To simplify the notation, we will denote the set of parameters as $\theta$:	$$\theta = \{a_k, b_k, w_k\}_{1 \leq k \leq K}$$	So $L(X, y, h)$ becomes just $L(\theta)$."
824,6_neural_networks_backpropagation.ipynb,45,code,slide,8,"	h_ = lambda x, w: relu(w*x - 2) + relu(-x)	L = lambda w: np.mean([0.5 * (y_i - h_(x, w))**2 for (x_i, y_i) in zip(x, y)])		w_range = np.linspace(-10, 5, 100)	plt.plot(w_range, [L(w) for w in w_range])	plt.title(r""How $L(X, y, h)$ depends on $a$, where $h(x) = \mathrm{relu}(ax - 2) + \mathrm{relu}(-x)$"")	plt.show()"
825,6_neural_networks_backpropagation.ipynb,46,markdown,slide,14,"We need to find parameters $\theta$ that give the lowest value of $L(\theta)$.		It can be done via **gradient descent**:	1. Choose the initial values of parameters $\theta^{(0)}$	2. Compute **the gradient** of $L$ wrt $\theta$ at the point $\theta^{(0)}$:	$$	\nabla L(\theta^{(0)}) = \left(\dfrac{\partial L}{\partial \theta^{(0)}_1}(\theta^{(0)}), \dfrac{\partial L}{\partial \theta^{(0)}_2}(\theta^{(0)}), \dots\right)^T	$$	3. Move in the direction of negative gradient (steepest descent):	$$	\theta^{(1)} = \theta^{(0)} - \lambda \nabla L(\theta^{(0)})	$$	Step size $\lambda$ is called **learning rate**.	4. Repeat until convergence."
826,6_neural_networks_backpropagation.ipynb,47,markdown,slide,1,### Colab demo 2: gradient descent
827,6_neural_networks_backpropagation.ipynb,48,code,-,1,"demo_gradient_descent(lambda x: x**4 - x**3 - x**2 + 1, theta_0=-1, learning_rate=0.05)  # local minima"
828,6_neural_networks_backpropagation.ipynb,49,code,-,1,"demo_gradient_descent(lambda x: x**2, theta_0=-1, learning_rate=1.1)  # divergence"
829,6_neural_networks_backpropagation.ipynb,50,markdown,slide,1,How do we find the gradient $\nabla L(\theta)$?
830,6_neural_networks_backpropagation.ipynb,51,markdown,slide,6,**Option 1. Numerical estimation.**		$\dfrac{\partial L}{\partial \theta_i}$ can be estimated as	$$	\dfrac{\partial L}{\partial \theta_i} \approx \dfrac{L(\theta_i + \delta) - L(\theta_i - \delta)}{2 \delta}	$$
831,6_neural_networks_backpropagation.ipynb,52,markdown,slide,5,Problems:	1. Not exact	1. Too expensive	- **Each parameter** $\theta_i$ requires 2 evaluations of $L$	- The total complexity is proportional to the number of parameters $\theta$
832,6_neural_networks_backpropagation.ipynb,53,markdown,slide,17,"## Exercise 2		Consider the univariate function $f(x)$ with the derivative $f'(x)$.		Prove that the estimation of the derivative	$$	f'(x) \approx d_1(x) = \dfrac{f(x + \delta) - f(x - \delta)}{2 \delta}	$$	is more precice than the ""naïve"" estimation	$$	f'(x) \approx d_2(x) = \dfrac{f(x + \delta) - f(x)}{\delta}	$$	in the following sense:	$$|f'(x) - d_1(x)| = O(\delta^2)\textrm{ for all }x$$	$$|f'(x) - d_2(x)| = O(\delta)\textrm{ for all }x$$		**Hint:** use Taylor series of $f$ at $x$."
833,6_neural_networks_backpropagation.ipynb,54,markdown,slide,1,**Option 2. Analytical differentiation**
834,6_neural_networks_backpropagation.ipynb,55,markdown,slide,6,"Recall that the loss function is the average of individual losses:	$$	L(X, y, h) = \dfrac{1}{N}\sum\limits_{1 \leq n \leq N} \ell(y_n, h(x_n))	$$		So it is sufficient to calculate the derivative of $\ell(y, h(x))$ wrt $a_1, \dots, a_K, b_1, \dots, b_K, w_1, \dots, w_K$."
835,6_neural_networks_backpropagation.ipynb,56,markdown,slide,3,$$	\dfrac{\partial \ell}{\partial w_k} =	$$
836,6_neural_networks_backpropagation.ipynb,57,markdown,slide,3,$$	= \dfrac{\partial}{\partial w_k}\frac{1}{2}(y - h(x))^2 =	$$
837,6_neural_networks_backpropagation.ipynb,58,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \dfrac{\partial h}{\partial w_k} =	$$
838,6_neural_networks_backpropagation.ipynb,59,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \underbrace{g(a_k x + b_k)}_{\frac{\partial h}{\partial w_k}}	$$
839,6_neural_networks_backpropagation.ipynb,60,markdown,slide,3,$$	\dfrac{\partial \ell}{\partial a_k} =	$$
840,6_neural_networks_backpropagation.ipynb,61,markdown,slide,3,$$	= \dfrac{\partial}{\partial a_k}\frac{1}{2}(y - h(x))^2 =	$$
841,6_neural_networks_backpropagation.ipynb,62,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \dfrac{\partial h}{\partial a_k} =	$$
842,6_neural_networks_backpropagation.ipynb,63,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k) \cdot x}_{\frac{\partial g(a_k x + b)}{\partial a_k}}	$$
843,6_neural_networks_backpropagation.ipynb,64,markdown,slide,4,$$	\dfrac{\partial \ell}{\partial b_k} =	$$	
844,6_neural_networks_backpropagation.ipynb,65,markdown,slide,3,$$	= \dfrac{\partial}{\partial b_k}\frac{1}{2}(y - h(x))^2 =	$$
845,6_neural_networks_backpropagation.ipynb,66,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \dfrac{\partial h}{\partial b_k} =	$$
846,6_neural_networks_backpropagation.ipynb,67,markdown,slide,3,$$	= \underbrace{(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k)}_{\frac{\partial g(a_k x + b_k)}{\partial b_k}}	$$
847,6_neural_networks_backpropagation.ipynb,68,markdown,slide,7,"Analytical inference is exact, but it should be performed for each $\ell$ and $h$ from scratch.	<!--@slideshow fragment-->	For example, if instead of $h$ we use	$$	\tilde h(x) = \sum\limits_{1 \leq k \leq K} w_k g(a_k x + b_k) \color{red}{+ s_k}	$$	then we need to compute all the derivatives $\dfrac{\partial \ell}{\partial s_k}$."
848,6_neural_networks_backpropagation.ipynb,69,markdown,slide,2,## Colab quiz 2	Choose the correct derivative.
849,6_neural_networks_backpropagation.ipynb,70,code,-,2,enable_mathjax_in_cell()	quiz_derivative()()
850,6_neural_networks_backpropagation.ipynb,71,markdown,slide,17,"Fortunately, computation of derivatives can be done algorithmically in a very efficient manner.		Note that the expressions	$$	\dfrac{\partial \ell}{\partial w_k} = \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \underbrace{g(a_k x + b_k)}_{\frac{\partial h}{\partial w_k}}	$$		$$	\dfrac{\partial \ell}{\partial a_k} = \underbrace{(y - h(x))}_{\frac{\partial \ell}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k) \cdot x}_{\frac{\partial g(a_k x + b)}{\partial a_k}}	$$		$$	\dfrac{\partial \ell}{\partial b_k} =\underbrace{(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k)}_{\frac{\partial g(a_k x + b_k)}{\partial b_k}}	$$	have common parts:	- all of them share $\dfrac{\partial \ell}{\partial h}$	- $\dfrac{\partial \ell}{\partial a_k}$, $\dfrac{\partial \ell}{\partial b_k}$ also share $\dfrac{\partial h}{\partial g}$"
851,6_neural_networks_backpropagation.ipynb,72,markdown,-,3,This observation brings us to		**Option 3. Computational graphs and backpropagation.**
852,6_neural_networks_backpropagation.ipynb,73,markdown,slide,5,"# Computational graphs and backpropagation		**Our goal**: compute the derivative of a multivariate function $F$.		**Example**: $F(\alpha, \beta) = f(\alpha \cdot \beta)$, where $f$ is a scalar function with the derivative $f'$."
853,6_neural_networks_backpropagation.ipynb,74,markdown,slide,3,"$$	\dfrac{\partial F}{\partial \alpha}(\alpha, \beta)	$$"
854,6_neural_networks_backpropagation.ipynb,75,markdown,slide,3,$$	= \dfrac{\partial f}{\partial \alpha}(\alpha \cdot \beta)	$$
855,6_neural_networks_backpropagation.ipynb,76,markdown,slide,3,$$	= f'(\alpha \cdot \beta) \dfrac{\partial (\alpha \cdot \beta)}{\partial \alpha}	$$
856,6_neural_networks_backpropagation.ipynb,77,markdown,slide,3,$$	= \beta f'(\alpha \cdot \beta)	$$
857,6_neural_networks_backpropagation.ipynb,78,markdown,slide,3,"$$	\dfrac{\partial F}{\partial \beta}(\alpha, \beta)	$$"
858,6_neural_networks_backpropagation.ipynb,79,markdown,slide,2,$$= \dfrac{\partial f}{\partial \beta}(\alpha \cdot \beta)	$$
859,6_neural_networks_backpropagation.ipynb,80,markdown,slide,2,$$= f'(\alpha \cdot \beta) \dfrac{\partial (\alpha \cdot \beta)}{\partial \beta}	$$
860,6_neural_networks_backpropagation.ipynb,81,markdown,slide,2,$$= \alpha f'(\alpha \cdot \beta)	$$
861,6_neural_networks_backpropagation.ipynb,82,markdown,slide,1,## Colab demo: computational graph
862,6_neural_networks_backpropagation.ipynb,83,code,-,11,"alpha = r""$\alpha$""	beta = r""$\beta$""	mul = r""$\times$""	f = r""$f$""		connections = [	(alpha, mul, r""$\alpha$"", r""$\beta f'(\alpha \cdot \beta)$"" ""\n"" r""$\left[\dfrac{\partial (\alpha\cdot\beta)}{\partial\alpha} = \beta\right]$""),	(beta, mul, r""$\beta$"", r""$\alpha f'(\alpha \cdot \beta)$"" ""\n"" r""$\left[\dfrac{\partial (\alpha\cdot\beta)}{\partial\beta} = \alpha\right]$""),	(mul, f, r""$\alpha\cdot\beta$"", r""$f'(\alpha \cdot \beta)$"" ""\n"" r""$\left[ \dfrac{\partial f(\alpha\cdot\beta)}{\partial(\alpha\cdot\beta)} = f'(\alpha\cdot\beta) \right]$""),	]	demo_computational_graph(connections, title=r""$F(\alpha, \beta) = f(\alpha \cdot \beta$)"")"
863,6_neural_networks_backpropagation.ipynb,84,markdown,slide,1,The computational graph suggests the following algorithm to compute derivatives:
864,6_neural_networks_backpropagation.ipynb,85,markdown,slide,1,"each node $n$ with inputs $i_1, i_2, \dots$"
865,6_neural_networks_backpropagation.ipynb,86,markdown,slide,1,- Receives the derivative of $F$ w.r.t. its value $\dfrac{\partial F}{\partial n}$
866,6_neural_networks_backpropagation.ipynb,87,markdown,slide,1,"- Computes the derivatives of its value w.r.t. its inputs: $\dfrac{\partial n}{\partial i_1}, \dfrac{\partial n}{\partial i_2}, \dots$"
867,6_neural_networks_backpropagation.ipynb,88,markdown,slide,2,"- For each input $i_k$, **propagates back** the derivatives of $F$ wrt $i_k$ using chain rule: $\dfrac{\partial F}{\partial i_k} = \dfrac{\partial F}{\partial n} \cdot \dfrac{\partial n}{\partial i_k}$	"
868,6_neural_networks_backpropagation.ipynb,89,markdown,slide,1,This algorithm is called **backpropagation**.
869,6_neural_networks_backpropagation.ipynb,90,markdown,slide,1,## Colab demo: more examples of backpropagation
870,6_neural_networks_backpropagation.ipynb,91,markdown,-,7,"**Example**: $F(\alpha, \beta) = f(\alpha + \beta)$	$$	\dfrac{\partial F}{\partial \alpha}(\alpha, \beta) = \dfrac{\partial f}{\partial \alpha}(\alpha + \beta) = f'(\alpha + \beta) \dfrac{\partial (\alpha + \beta)}{\partial \alpha} = f'(\alpha + \beta)	$$	$$	\dfrac{\partial F}{\partial \beta}(\alpha, \beta) = \dfrac{\partial f}{\partial \beta}(\alpha + \beta) = f'(\alpha + \beta) \dfrac{\partial (\alpha + \beta)}{\partial \beta} = f'(\alpha + \beta)	$$"
871,6_neural_networks_backpropagation.ipynb,92,code,-,11,"alpha = r""$\alpha$""	beta = r""$\beta$""	plus = r""$+$""	f = r""$f$""		connections = [	(alpha, plus, r""$\alpha$"", r""$f'(\alpha + \beta)$""),	(beta, plus, r""$\beta$"", r""$f'(\alpha + \beta)$""),	(plus, f, r""$\alpha + \beta$"", r""$f'(\alpha + \beta)$""),	]	demo_computational_graph(connections, title=r""$F(\alpha, \beta) = f(\alpha + \beta)$"")"
872,6_neural_networks_backpropagation.ipynb,93,markdown,-,4,"**Example:** $F(\alpha) = f(g(\alpha))$, where $g$ is a univariate function with the derivative $g'$.	$$	\dfrac{\partial F}{\partial \alpha}(\alpha) = \dfrac{\partial f(g(\alpha))}{\partial \alpha} = \dfrac{\partial f}{\partial g}(g(\alpha)) \cdot \dfrac{\partial g(\alpha)}{\partial \alpha} = f'(g(\alpha))\cdot g'(\alpha)	$$"
873,6_neural_networks_backpropagation.ipynb,94,code,-,9,"alpha = r""$\alpha$""	g = r""$g$""	f = r""$f$""		connections = [	(alpha, g, r""$\alpha$"", r""$f'(g(\alpha)) \cdot g'(\alpha)$""),	(g, f, r""$g(\alpha)$"", r""$f'(g(\alpha))$""),	]	demo_computational_graph(connections, title=r""$F(\alpha) = f(g(\alpha))$"")"
874,6_neural_networks_backpropagation.ipynb,95,markdown,slide,1,## Colab quiz 3
875,6_neural_networks_backpropagation.ipynb,96,code,-,2,enable_mathjax_in_cell()	quiz_correct_computational_graph()()
876,6_neural_networks_backpropagation.ipynb,97,markdown,-,1,"**Example**: $F(\alpha, \beta, \gamma) = f(\alpha \ln \beta + 3\gamma)$"
877,6_neural_networks_backpropagation.ipynb,98,code,-,24,"alpha = r""$\alpha$""	beta = r""$\beta$""	gamma = r""$\gamma$""	ln = r""$\ln$""	f = r""$f$""	plus = r""$+$""	mul = r""$\times$""	mul3 = r""$\times 3$""		connections = [	(alpha, mul, r""$\alpha$"", r""$\ln\beta f'(\alpha\ln\beta + 3\gamma)$""),	(beta, ln, r""$\beta$"", r""$\dfrac{\alpha}{\beta}f'(\alpha\ln\beta + 3\gamma)$"" ""\n"" r""$\left[\dfrac{\partial\ln\beta}{\partial\beta} = \dfrac{1}{\beta}\right]$""),	(ln, mul, r""$\ln\beta$"", r""$\alpha f'(\alpha\ln\beta + 3\gamma)$""),	(mul, plus, r""$\alpha\ln\beta$"", r""$f'(\alpha\ln\beta + 3\gamma)$""),	(gamma, mul3, r""$\gamma$"", r""$3f'(\alpha\ln\beta + 3\gamma)$"" ""\n"" r""$\left[\dfrac{\partial (3\cdot\gamma)}{\partial \gamma} = 3\right]$""),	(mul3, plus, r""$3\gamma$"", r""$f'(\alpha\ln\beta + 3\gamma)$""),	(plus, f, r""$\alpha\ln\beta + 3\gamma$"", r""$f'(\alpha\ln\beta + 3\gamma)$"")	]	demo_computational_graph(	connections,	title=r""$F(\alpha, \beta, \gamma) = f(\alpha\ln\beta + 3\gamma$)"",	font_size=15,	node_size=400	)"
878,6_neural_networks_backpropagation.ipynb,99,markdown,slide,6,	A node of computational graph may have many _outputs_.		**Example**: $F(t) = x(t) \times e^{y(t)}$.		$F$ depends on $x$ and $y$ that depend on $t$.
879,6_neural_networks_backpropagation.ipynb,100,markdown,slide,1,**Solution**: use _total derivative_.
880,6_neural_networks_backpropagation.ipynb,101,markdown,slide,3,"The connection between $\Delta F, \Delta x, \Delta y$ and partial derivatives:		$$\Delta F \approx \Delta x \dfrac{\partial F}{\partial x} + \Delta y \dfrac{\partial F}{\partial y}$$"
881,6_neural_networks_backpropagation.ipynb,102,markdown,slide,2,Divide both sides by $\Delta t$:	$$\dfrac{\Delta F}{\Delta t} \approx \dfrac{\Delta x}{\Delta t} \dfrac{\partial F}{\partial x} + \dfrac{\Delta y}{\Delta t} \dfrac{\partial F}{\partial y} $$
882,6_neural_networks_backpropagation.ipynb,103,markdown,slide,3,$$\Rightarrow \dfrac{d F}{d t} = \dfrac{d x}{d t} \dfrac{\partial F}{\partial x} + \dfrac{d y}{d t} \dfrac{\partial F}{\partial y}$$		> The incoming gradients add up.
883,6_neural_networks_backpropagation.ipynb,104,markdown,slide,1,## Colab demo: computational graph for $F(t) = x(t)\times e^{y(t)}$
884,6_neural_networks_backpropagation.ipynb,105,code,-,14,"t = r""$t$""	x = r""$x(t)$""	y = r""$y(t)$""	exp = r""$\exp$""	mul = r""$\times$""		connections = [	(t, x, t, r""$\dfrac{d x}{d t} e^{y(t)}$""),	(t, y, t, r""$\dfrac{d y}{d t} x(t)e^{y(t)}$""),	(x, mul, x, r""$e^{y(t)}$""),	(y, exp, y, r""$x(t)e^{y(t)}$""),	(exp, mul, r""$e^{y(t)}$"", r""$x(t)$"")	]	demo_computational_graph(connections, title=r""$F(t) = x(t)\times e^{y(t)}$"", font_size=15, node_size=400)"
885,6_neural_networks_backpropagation.ipynb,106,markdown,slide,13,## Colab demo: computational graph for the universal approximator	Recall that the derivatives are:	$$	\dfrac{\partial l}{\partial w_k} = \dfrac{\partial}{\partial w_k}(y - h(x))^2 = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \dfrac{\partial h}{\partial w_k} = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \underbrace{g(a_k x + b_k)}_{\frac{\partial h}{\partial w_k}}	$$		$$	\dfrac{\partial l}{\partial a_k} = \dfrac{\partial}{\partial a_k}(y - h(x))^2 = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \dfrac{\partial h}{\partial a_k} = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k) \cdot x}_{\frac{\partial g(a_k x + b)}{\partial a_k}}	$$		$$	\dfrac{\partial l}{\partial b_k} = \dfrac{\partial}{\partial b_k}(y - h(x))^2 = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \dfrac{\partial h}{\partial b_k} = \underbrace{2(y - h(x))}_{\frac{\partial l}{\partial h}} \cdot \underbrace{w_k}_{\frac{\partial h}{\partial g}} \cdot \underbrace{g'(a_k x + b_k)}_{\frac{\partial g(a_k x + b_k)}{\partial b_k}}	$$
886,6_neural_networks_backpropagation.ipynb,107,code,-,1,"demo_universal_approximator(1, font_size=8, node_size=300)"
887,6_neural_networks_backpropagation.ipynb,108,code,-,2,"# exercise: analyze computational graph for K=2	demo_universal_approximator(2, font_size=8, node_size=300)"
888,6_neural_networks_backpropagation.ipynb,109,markdown,slide,2,# Layers. Forward/backward API	
889,6_neural_networks_backpropagation.ipynb,110,markdown,slide,3,"In practice, computations are grouped into functions.		There functions are called _layers_, and they have a special form:"
890,6_neural_networks_backpropagation.ipynb,111,markdown,slide,12,"$$	L_{g, \mathbf{W}, \mathbf{b}}(\mathbf{x}) = g \odot (\mathbf{W}\mathbf{x} + \mathbf{b})	$$	where	- $\mathbf{x}$ is the input vector with $N$ components,	- $\mathbf{W}$ is the matrix with $M$ rows and $N$ columns (_weight matrix_),	- $\mathbf{b}$ is the vector with $M$ components (_bias_),	- $g$ is a scalar function (like `sign` or `ReLU`),	- $g \odot \mathbf{v}$ means the _elementwise_ application of $g$ to $\mathbf{v}$.		**Question**: what is the size of the output?	"
891,6_neural_networks_backpropagation.ipynb,112,markdown,slide,6,Consider the universal approximator	$$	h(x) = \sum\limits_{1 \leq i \leq K} w_k g(a_k x + b_k)	$$	It can be represented as the composition of two layers:	
892,6_neural_networks_backpropagation.ipynb,113,markdown,slide,6,"- The first layer:	$$	L^{(1)}_{g, \mathbf{a}, \mathbf{b}}(x) = g \odot (\mathbf{a}x + \mathbf{b})	$$	where $\mathbf{a}, \mathbf{b}$ are vectors of size $K$.	"
893,6_neural_networks_backpropagation.ipynb,114,markdown,slide,7,- The second layer:	$$L^{(2)}_{\mathbf{w}}(\tilde{\mathbf{x}}) = \mathbb{1} \odot \mathbf{w}^T\tilde{\mathbf{x}}$$	where	- $\mathbf{w}$ is the vector of size $K$	- $\mathbf{w}^T$ is the matrix with $1$ row and $K$ columns	- $\mathbb{1}$ is the identity function.	
894,6_neural_networks_backpropagation.ipynb,115,markdown,slide,8,"So	$$	h(x) = \sum\limits_{1 \leq i \leq K} w_k g(a_k x + b_k)	$$	is equivalent to	$$h(x) = L^{(2)}_{\mathbf{w}}(L^{(1)}_{g, \mathbf{a}, \mathbf{b}}(x))$$	- The outermost layer $L^{(2)}_{\mathbf{w}}$ is called _output layer_.	- The inner layer $L^{(1)}_{g, \mathbf{a}, \mathbf{b}}$ is called _hidden layer_."
895,6_neural_networks_backpropagation.ipynb,116,markdown,slide,1,This representation allows to effectively compute gradients using backpropagation.
896,6_neural_networks_backpropagation.ipynb,117,markdown,slide,7,"## Exercise 3		Compute the derivative of function	$$	\mathrm{relu}(x) = \max(0, x)	$$	for $x > 0$ and $x < 0$."
897,6_neural_networks_backpropagation.ipynb,118,markdown,slide,1,## Colab demo: forward-backward API
898,6_neural_networks_backpropagation.ipynb,119,code,-,63,"import numpy as np		class Function:	""""""	Base class for functions g: ReLU, sign, identity, ...	""""""	def f(self, x):	# compute the function itself	raise NotImplementedError	def df(self, x):	# compute the gradient	raise NotImplementedError		class Identity(Function):	def f(self, x):	return x	def df(self, x):	return np.ones(x.shape)		class ReLU(Function):	def f(self, x):	return np.maximum(x, 0)	def df(self, x):	return np.heaviside(x, 0)		class Layer:	""""""	Layer is the function of form g(Wx + b)	It provides so-called ""forward-backward API"":	1. Forward pass: compute the value of the function and store local gradients	2. Backward pass: backpropagate gradiends	""""""	def __init__(self, W, b, g):	# store the parameters W, b and the function g	self.W = W	self.b = b	self.g = g		# store the information to compute gradients	self.x = self.g_gradient = None		def forward(self, x):	# In forward pass, inputs are known, so local gradients can be computed.	self.x = x	self.g_gradient = self.g.df(self.W @ x + self.b)  # this is the local gradient	# return g(Wx + b)	return self.g.f(self.W @ x + self.b)		def backward(self, grad):	# backward pass: x is the input, so d / dx (g(Wx + b)) = g' * W	dgdx = self.g_gradient * self.W	# chain rule	return dgdx.T @ grad		def db(self, grad):	# d / db (g(Wx + b)) = g'	dgdb = self.g_gradient	return dgdb * grad		def dW(self, grad):	# d / dW (g(Wx + b)) = g' * x	dgdW = self.g_gradient @ self.x.T	return dgdW * grad"
899,6_neural_networks_backpropagation.ipynb,120,code,-,32,"# sanity check: compute the gradients and estimate them numerically	def grad_f(f, x, delta=1e-5):	# this is how estimate the gradient of f at x	return (f(x + delta) - f(x - delta)) / (2 * delta)		# the universal approximator for scalar input with 5 components	N = 1  # input dimension	M = 5  # output dimension	W = np.ones((M, N))	b = np.zeros((M, 1))		# this is how the universal approximator is represented as the composition of layers	l1 = Layer(W, b, ReLU())	l2 = Layer(W.T, np.zeros((N, 1)), Identity())		# f(x) computes the universal approximator for x	def f(x):	return l2.forward(l1.forward(x))		for x in (-1, -0.1, 0.1, 1):	x = np.ones((N, 1)) * x	x1 = l1.forward(x)	x2 = l2.forward(x1)	# to compute the gradient using backpropagation, we should start with df / df = 1	grad0 = np.ones((N, 1))	# this is backpropagation	g2 = l2.backward(grad0)	g1 = l1.backward(g2)		# the result of backpropagation (the derivative) is stored in g1	# compare it with the gradient estimated numerically	print(""x = {}.\tGradients: backprop={}, estimated={}"".format(x, g1, grad_f(f, x)))"
900,6_neural_networks_backpropagation.ipynb,121,code,-,10,"def coeffs(l1, l2):	""""""	Helper function for visualization, you can ignore it	""""""	rv = {}	for i in range(K):	rv[""a""+str(i+1)] = l1.W[i].item()	rv[""b""+str(i+1)] = l1.b[i].item()	rv[""w""+str(i+1)] = l2.W[0][i].item()	return rv"
901,6_neural_networks_backpropagation.ipynb,122,code,-,54,"# fit the function f(x) = cosh(x) with the formula \sum_K g(a_k x + b_k) w_k	# K is a hyperparameter	K = 10		# initialize weights at random	import numpy as np	np.random.seed(0)	A = np.random.random((K, 1)) - 0.5	B = np.random.random((K, 1)) - 0.5	l1 = Layer(A, B, ReLU())		W = np.random.random((1, K)) - 0.5	l2 = Layer(W, np.zeros((1, 1)), Identity())		# store the initial coefficients for further comparison	coeffs_before = coeffs(l1, l2)		num_samples = 1000	X = np.linspace(-1, 2, num_samples)	Y = np.cosh(X)		# stochastic gradient descent	learning_rate = 0.05	num_iterations = 10000		for t in range(num_iterations):	# decaying learning rate	lr = learning_rate / np.sqrt(t + 1)	# pick a random sample	idx = np.random.choice(range(num_samples))	x = X[idx]	y = Y[idx]		# compute the approximation	x1 = l1.forward(np.array([[x]]))	h = l2.forward(x1)		# the error is (y - h)^2, so the derivative of the loss function	# d / dh ((y - h)^2) = 2 * (h - y)	# backpropagate this derivative	loss_gradient = 2 * (h - y)		# update the output layer	l2.W -= lr * l2.dW(loss_gradient)	# update the hidden layer	g1 = l2.backward(loss_gradient)	l1.W -= lr * l1.dW(g1)	l1.b -= lr * l1.db(g1)		# we can compute d/dx, but we don't need it since we don't update x	dx = l1.backward(g1)		# save the resulting coefficients	coeffs_after = coeffs(l1, l2)"
902,6_neural_networks_backpropagation.ipynb,123,code,-,2,"# this is the approximator with initial weights	demo_function_approximation(num_functions=K, static=True, default_transform=""relu"", **coeffs_before)"
903,6_neural_networks_backpropagation.ipynb,124,code,-,2,"# this is the approximator after training	demo_function_approximation(num_functions=K, static=True, default_transform=""relu"", **coeffs_after)"
904,6_neural_networks_backpropagation.ipynb,125,markdown,slide,3,# XOR problem: add more layers		_Classification_ is also a function approximation problem (for functions with finite number of values).
905,6_neural_networks_backpropagation.ipynb,126,markdown,slide,4,"Consider the function ""exclusive OR"" (XOR):	$$	\mathrm{XOR}(x, y) = \begin{cases}1, & x \cdot y < 0\\-1,&\text{otherwise}\end{cases}	$$"
906,6_neural_networks_backpropagation.ipynb,127,code,slide,3,"import numpy as np	f_xor = lambda x, y: -np.sign(x*y)	demo_2d_classification(f_xor, title=r""$\mathrm{XOR}(x, y) = -\mathrm{sign}(x \cdot y)$"")"
907,6_neural_networks_backpropagation.ipynb,128,markdown,slide,7,"$\mathrm{XOR}$ function is easily approximated with **two hidden layers**.		**Idea**:	- The first hidden layer computes functions $\mathrm{OR}(x, y)$ and $\mathrm{AND}(x, y)$.	- Compute $$\mathrm{XOR}(x, y) = \mathrm{sign}(\mathrm{OR}(x, y) - \mathrm{AND}(x, y))$$		In other words, **the second layer computes features for the first layer**."
908,6_neural_networks_backpropagation.ipynb,129,code,slide,2,"f_true_and = lambda x, y: (x > 0) * (y > 0)	demo_2d_classification(f_true_and, title=r""$\mathrm{AND}(x, y)$"")"
909,6_neural_networks_backpropagation.ipynb,130,code,slide,2,"f_true_or = lambda x, y: (x > 0) + (y > 0) - 1	demo_2d_classification(f_true_or, title=r""$\mathrm{OR}(x, y)$"")"
910,6_neural_networks_backpropagation.ipynb,131,markdown,slide,1,Start with the function $\mathrm{sign}(x) + \mathrm{sign}(y)$:
911,6_neural_networks_backpropagation.ipynb,132,code,slide,2,"f = lambda x, y: np.sign(x) + np.sign(y)	demo_2d_classification(f, show_zero=True, title=r""$\mathrm{sign}(x) + \mathrm{sign}(y)$"")"
912,6_neural_networks_backpropagation.ipynb,133,markdown,slide,1,"If we shift all the values down by 1, only one quarter is positive:"
913,6_neural_networks_backpropagation.ipynb,134,code,slide,2,"f_shifted_down = lambda x, y: np.sign(x) + np.sign(y) - 1	demo_2d_classification(f_shifted_down, show_zero=True, title=r""$\mathrm{sign}(x) + \mathrm{sign}(y) - 1$"")"
914,6_neural_networks_backpropagation.ipynb,135,markdown,slide,2,"Taking sign, we obtain $\mathrm{AND}(x, y)$ function:	"
915,6_neural_networks_backpropagation.ipynb,136,code,slide,2,"f_and = lambda x, y: np.sign(np.sign(x) + np.sign(y) - 1)	demo_2d_classification(f_and, title=r""$\mathrm{sign}(\mathrm{sign}(x) + \mathrm{sign}(y) - 1)$"")"
916,6_neural_networks_backpropagation.ipynb,137,markdown,slide,1,"Similarly, we obtain the function $\mathrm{OR}(x, y)$ by shifting up and taking sign:"
917,6_neural_networks_backpropagation.ipynb,138,code,slide,2,"f_or = lambda x, y: np.sign(np.sign(x) + np.sign(y) + 1)	demo_2d_classification(f_or, title=r""$\mathrm{sign}(\mathrm{sign}(x) + \mathrm{sign}(y) + 1)$"")"
918,6_neural_networks_backpropagation.ipynb,139,markdown,slide,1,"Finally, we get $\mathrm{XOR}(x, y) = \mathrm{sign}(\mathrm{OR}(x, y) - \mathrm{AND}(x, y))$"
919,6_neural_networks_backpropagation.ipynb,140,code,slide,2,"approx_f_xor = lambda x, y: np.sign(f_or(x, y) - f_and(x, y))	demo_2d_classification(approx_f_xor, title=r""$\mathrm{sign}(\mathrm{OR}(x, y) - \mathrm{AND}(x, y))$"")"
920,6_neural_networks_backpropagation.ipynb,141,code,slide,49,"step_x = ""step_x""	step_y = ""step_y""	plus_1 = ""plus_1""	inc_1 = ""inc_1""	dec_1 = ""dec_1""	step_and = ""step_and""	step_or = ""step_or""	minus_1 = ""minus_1""	step_xor = ""step_xor""		plus_label = r""$+$""	step_label = r""$\mathrm{sign}$""	labels = {	step_x: step_label,	step_y: step_label,	plus_1: plus_label,	inc_1: plus_label,	dec_1: plus_label,	step_and: step_label,	step_or: step_label,	minus_1: r""$-$"",	step_xor: step_label,	}	connections = [	(""x"", step_x, r""$x$"", """"),	(""y"", step_y, r""$y$"", """"),	(step_x, plus_1, r""$\mathrm{sign}(x)$"", """"),	(step_y, plus_1, r""$\mathrm{sign}(y)$"", """"),	(r""$+1$"", inc_1, r""$+1$"", """"),	(plus_1, inc_1, r""$\mathrm{sign}(x) + \mathrm{sign}(y)$"", """"),	(r""$-1$"", dec_1, r""$-1$"", """"),	(plus_1, dec_1, r""$\mathrm{sign}(x) + \mathrm{sign}(y)$"", """"),	(inc_1, step_or, r""$\mathrm{sign}(x) + \mathrm{sign}(y) + 1$"", """"),	(dec_1, step_and, r""$\mathrm{sign}(x) + \mathrm{sign}(y) - 1$"", """"),	(step_or, minus_1, r""$\mathrm{OR}(x, y)$"", """"),	(step_and, minus_1, r""$\mathrm{AND}(x, y)$"", """"),	(minus_1, step_xor, r""$\mathrm{OR}(x, y) - \mathrm{AND}(x, y)$"", """"),	]	demo_computational_graph(	connections,	title=r""$\mathrm{XOR}(x, y)$"",	labels=labels,	backward_color=""w"",	figsize=(12, 8),	font_size=11,	scale_x=2.0,	static=True,	forward_idx=13	)"
921,6_neural_networks_backpropagation.ipynb,142,markdown,slide,1,# Feedforward neural networks
922,6_neural_networks_backpropagation.ipynb,143,markdown,slide,1,Let's put it all together:
923,6_neural_networks_backpropagation.ipynb,144,markdown,slide,1,"- Any function can be approximated as a linear combinantion of nonlinear functions (""the universal approximator"")."
924,6_neural_networks_backpropagation.ipynb,145,markdown,slide,1,- It is convenient to represent this linear combination as composition of _layers_.
925,6_neural_networks_backpropagation.ipynb,146,markdown,slide,1,- Parameters of layers are adjusted via minimization of loss function using gradient descent.
926,6_neural_networks_backpropagation.ipynb,147,markdown,slide,1,- _Backpropagation_ algorithm is used to compute gradients.
927,6_neural_networks_backpropagation.ipynb,148,markdown,slide,2,- We can add more hidden layers: they extract features for next layers.	
928,6_neural_networks_backpropagation.ipynb,149,markdown,slide,1,**Definition**: _feedforward neural network_ is a composition of layers.
929,6_neural_networks_backpropagation.ipynb,150,markdown,slide,6,"Why is it called ""neural network""?	![alt text](https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png)		The simplest mathematical model of a neuron:	- Get a linear combination of inputs.	- The input signal ""activates"" the axon (Heaviside function: 0 or 1)."
930,6_neural_networks_backpropagation.ipynb,151,markdown,slide,3,Neural networks are	- powerful approximators	- **computationally efficient**
931,6_neural_networks_backpropagation.ipynb,152,markdown,slide,5,"Two types of operations:	- _Elementwise_ computation of nonlinearities	- Matrix addition and multipliction		These operations can be parallelized, which is especially effective on GPU."
932,6_neural_networks_backpropagation.ipynb,153,markdown,slide,5,"# PyTorch basics		In this course, we will use PyTorch to build and train neural networks.		The main feature of PyTorch is dynamic computational graph. It is especially convenient for text analysis because texts can have arbitrary length."
933,6_neural_networks_backpropagation.ipynb,154,markdown,slide,7,"## Tensor		Tensor is a multidimensional array (like `numpy.array`).		What is special about tensors in PyTorch:	- Tensor operations can be performed on GPU (we won't cover it yet).	- Tensors can store references to functions which compute them. Moreover, tensors store _gradients_ of these functions."
934,6_neural_networks_backpropagation.ipynb,155,code,slide,1,import torch
935,6_neural_networks_backpropagation.ipynb,156,code,slide,3,"# create 2-dimensional tensor x	x = torch.tensor([[1, 2, 3], [4, 5, 6]])	x"
936,6_neural_networks_backpropagation.ipynb,157,code,slide,2,# x contains 2 rows (first dimension) and 3 columns (second dimension)	x.size()
937,6_neural_networks_backpropagation.ipynb,158,code,slide,4,"# we can add two tensors together	x = torch.tensor([1, 2])	y = torch.tensor([3, 4])	x + y"
938,6_neural_networks_backpropagation.ipynb,159,code,slide,2,"# ""x + y"" is syntactic sugar for torch.add(x, y)	torch.add(x, y)"
939,6_neural_networks_backpropagation.ipynb,160,code,slide,4,"# pointwise multiplication	x = torch.tensor([1, 2])	y = torch.tensor([3, 4])	x * y"
940,6_neural_networks_backpropagation.ipynb,161,code,slide,2,"# again, ""x * y"" is syntactic sugar for torch.mul(x, y)	torch.mul(x, y)"
941,6_neural_networks_backpropagation.ipynb,162,code,slide,3,"# broadcasting: a scalar value is added to all values of 2-dimensional tensor	x = torch.tensor([[1, 2], [3, 4]])	x + 3"
942,6_neural_networks_backpropagation.ipynb,163,code,slide,6,"# broadcasting: add 1-dimensional tensor y to each row of x	x = torch.tensor([[1, 2], [3, 4]])	y = torch.tensor([-1, 1])	print(""x ="", x)	print(""y ="", y)	print(""x + y ="", x + y)"
943,6_neural_networks_backpropagation.ipynb,164,code,slide,6,"# broadcasting: add 2-dimensional tensor with one column y to each row of x	x = torch.tensor([[1, 2], [3, 4]])	y = torch.tensor([[-1], [1]])	print(""x ="", x)	print(""y ="", y)	print(""x + y ="", x + y)"
944,6_neural_networks_backpropagation.ipynb,165,code,slide,11,"# indexing and slicing		x = torch.tensor([[1, 2], [3, 4]])	print(""x ="", x)	# access the first column (index 0) of the second row (index 1)	print(""x[1][0] ="", x[1][0])	# get a Python number instead of a tensor	print(""x[1][0].item() ="", x[1][0].item())	# modify an element of a tensor	x[1][0] = 42	print(""x ="", x)"
945,6_neural_networks_backpropagation.ipynb,166,code,slide,7,"# sometimes we need to create 2-dimensional matrices from 1-dimensional arrays (and vice versa)	a = torch.tensor([1, 2, 3])	b = a.unsqueeze(0)  # assume that there is the dimension 0 (rows), and add this dimension	c = b.squeeze(0)  # the inverse operation for `unsqueeze`	print(f""a={a}"")	print(f""b={b}"")	print(f""c={c}"")"
946,6_neural_networks_backpropagation.ipynb,167,code,slide,6,"# matrix-vector product	W = torch.tensor([[1., 2.], [3., 4.]])	x = torch.tensor([1., 1.])	print(f""W = {W}"")	print(f""x = {x}"")	print(f""W * x = {torch.matmul(W, x)}"")"
947,6_neural_networks_backpropagation.ipynb,168,markdown,slide,1,## Gradients
948,6_neural_networks_backpropagation.ipynb,169,code,slide,5,"# if the flag `requres_grad` is set, a tensor starts tracking all operations on it	x = torch.tensor(3.0, requires_grad=True)	y = torch.tensor(2.0)	print(""x ="", x)	print(""y ="", y)"
949,6_neural_networks_backpropagation.ipynb,170,code,slide,5,"z = x * y	print(""z ="", z)	# note that z has the `grad_fn` attribute	# z.grad_fn is a function which computes the gradients of z wrt its inputs	print(""z.grad_fn ="", z.grad_fn)"
950,6_neural_networks_backpropagation.ipynb,171,code,slide,10,"# compute derivatives of z wrt all inputs that have `requres_grad` flag	# the method `.backward()` accepts the initial value for backpropagation	# in this case, it is dz/dz == 1.0	z.backward(torch.tensor(1.0))  # is equivalent to z.backward() since z is a scalar		# dz/dx is computed since g.requires_grad is True	# the gradient dz/dx is stored as the x.grad property	print(""dz/dx ="", x.grad)	# dz/dy is not computed since g.requires_grad is False	print(""dz/dy ="", y.grad)"
951,6_neural_networks_backpropagation.ipynb,172,markdown,slide,2,Let's look how PyTorch creates computational graphs	
952,6_neural_networks_backpropagation.ipynb,173,code,slide,15,"# consider the function z(x, y) = x * (x + y)	x = torch.tensor(3.0, requires_grad=True)	y = torch.tensor(2.0)	z = x * (x + y)		# z.grad_fn is the gradient of product (MulBackward0)	print(""z.grad_fn ="", z.grad_fn)	# z.grad_fn is designed for backpropagation:	#  - receives the ""outer"" gradient d something / dz	#  - computes dz / di for all inputs i	#  - returns d something / di for all inputs i	print(""z.grad_fn(1) ="", z.grad_fn(torch.tensor(1.0)))  # d something / dz = dz/dz = 1		# z.grad_fn stores edges to the input functions in the `.next_functions` property	print(""z.grad_fn.next_functions ="", z.grad_fn.next_functions)"
953,6_neural_networks_backpropagation.ipynb,174,markdown,slide,1,"Visualize the computational graph for $z(x, y) = x \cdot (x + y)$."
954,6_neural_networks_backpropagation.ipynb,175,code,slide,5,"x = torch.tensor(3.0, requires_grad=True)	y = torch.tensor(2.0)	z = x * (x + y)	# dz / dx = (x + y) + x = 8	demo_pytorch_computational_graph(z)"
955,6_neural_networks_backpropagation.ipynb,176,markdown,slide,1,"Visualize the computational graph for $z(x, y) = x^2 + xy$."
956,6_neural_networks_backpropagation.ipynb,177,code,slide,5,"x = torch.tensor(3.0, requires_grad=True)	y = torch.tensor(2.0)	z = x * x + x * y	# dz / dx = 2x + y = 8	demo_pytorch_computational_graph(z)"
957,6_neural_networks_backpropagation.ipynb,178,markdown,slide,1,"The computational graph for $z(x, y) = x \cdot (x + y)$, both $x$ and $y$ track the gradients."
958,6_neural_networks_backpropagation.ipynb,179,code,slide,6,"x = torch.tensor(3.0, requires_grad=True)	y = torch.tensor(2.0, requires_grad=True)	z = x * (x + y)	# dz / dx = (x + y) + x = 8	# dz / dy = x = 3	demo_pytorch_computational_graph(z)"
959,6_neural_networks_backpropagation.ipynb,180,markdown,slide,3,### Colab quiz 4		Compute the derivative of $f(x) = x^x$ with PyTorch.
960,6_neural_networks_backpropagation.ipynb,181,code,-,9,"def solution(x: float) -> float:	""""""	return the derivative of f(x) = x^x	""""""	# YOUR CODE HERE	return 1		derivative_pytorch_checker = quiz_derivative_pytorch()	derivative_pytorch_checker(solution)"
961,6_neural_networks_backpropagation.ipynb,182,markdown,slide,12,"## Colab demo: implement the universal approximator using PyTorch layers		As we saw earlier, it's convenient to group computations into _layers_.	PyTorch contains implementations of many standard layers in the module `torch.nn`.		PyTorch also provides a useful abstraction `torch.nn.Module` that allows to compose layers together.		Let's implement the universal approximator	$$	h(x) = \sum\limits_{1 \leq i \leq K} w_k g(a_k x + b_k)	$$	in PyTorch."
962,6_neural_networks_backpropagation.ipynb,183,markdown,-,1,### `nn.Module`
963,6_neural_networks_backpropagation.ipynb,184,code,-,29,"import torch	from torch import nn		class H(nn.Module):	# this class represents the universal approximator	def __init__(self, K):	super().__init__()	# the universal approximator has a hyperparameter K	# for simplicity, we assume that the nonlinearity is always ReLU		# we will store the parameters a_k, b_k, w_k in tensors	# these tensors must be wrapped in `nn.Parameter` class	# (otherwise PyTorch won't understand that they should be trained)	# initialize W with a 1d-tensor with K random elements	# always seed the RNG for reproducibility	torch.manual_seed(0)	# A is a K x 1 matrix (1 input, K outputs)	# B is a vector with K components	self.A = nn.Parameter(torch.rand(K, 1) - 0.5)	self.B = nn.Parameter(torch.rand(K) - 0.5)	# W is 1 x K matrix (K inputs, 1 output)	self.W = nn.Parameter(torch.rand(1, K) - 0.5)		def forward(self, x):	# as we know, PyTorch computes the gradients automatically for all standard operations with tensors	# so it is enough to provide only forward pass in forward/backward API	nonlinearity = torch.relu(torch.matmul(self.A, x) + self.B)	output = torch.matmul(self.W, nonlinearity)	return output"
964,6_neural_networks_backpropagation.ipynb,185,code,-,13,"K = 10	h = H(K)	# note that all parameters (W, A, B) automatically require gradients	print(""h.W ="", h.W)	print(""h.A ="", h.A)	print(""h.B ="", h.B)		# compute some values	# note that x must be 1-dimensional tensor (vector with 1 component)	x1 = torch.tensor([2.])	print(""h(2) ="", h(x1))	x2 = torch.tensor([-1.])	print(""h(-1) ="", h(x2))"
965,6_neural_networks_backpropagation.ipynb,186,code,-,44,"# fit the function f(x) = cosh(x)	import numpy as np	# generate data	num_samples = 1000	X = np.linspace(-1, 2, num_samples, dtype=np.float32)	Y = np.cosh(X)		# train using stochastic gradient descent	def train(h, X, Y, num_iterations, learning_rate):	# we will be minimizing MSE	loss_function = nn.MSELoss()		# stochastic gradient descent	for t in range(num_iterations):	lr = learning_rate / np.sqrt(t + 1)	# get random sample	idx = np.random.choice(range(num_samples))	# input is a 1-dimensional tensor	x = torch.tensor([X[idx]])	# output is a 1-dimensional tensor	y = torch.tensor([Y[idx]])		# compute the loss	loss = loss_function(y, h(x))		# NB: by default, gradients are always accumulated	# so for each forward/backward pass they should be zeroed	h.zero_grad()		# compute the backward pass	loss.backward()		# update the parameters	# each parameter update is a function, so in principle we can track its gradients	# but we don't need it, so wrap the parameter update with torch.no_grad()	with torch.no_grad():	# h.parameters() is the iterator over all the parameters	# each parameter contains the gradient afer backprop in the .grad property	for parameter in h.parameters():	parameter -= lr * parameter.grad		num_iterations = 10000	learning_rate = 0.05	train(h, X, Y, num_iterations, learning_rate)"
966,6_neural_networks_backpropagation.ipynb,187,code,-,8,"# visualize	coeffs = {}	for i, par in enumerate(zip(h.W.squeeze(), h.A.squeeze(), h.B.squeeze())):	w, a, b = par	coeffs[""w"" + str(i+1)] = w.item()	coeffs[""a"" + str(i+1)] = a.item()	coeffs[""b"" + str(i+1)] = b.item()	demo_function_approximation(num_functions=K, static=True, default_transform=""relu"", **coeffs)"
967,6_neural_networks_backpropagation.ipynb,188,markdown,-,3,### `nn.Sequential`		Feedforward neural networks can be implemented using `nn.Sequential`:
968,6_neural_networks_backpropagation.ipynb,189,code,-,6,"h2 = nn.Sequential(	nn.Linear(1, K),  # nn.Linear is a linear transformation: 1 input, K outputs	nn.ReLU(),  # apply ReLU to the output of the previous layer	nn.Linear(K, 1, bias=False)  # K inputs from the previous layer, 1 output. Do not add bias	)	train(h2, X, Y, num_iterations, learning_rate)"
969,6_neural_networks_backpropagation.ipynb,190,code,-,10,"# visualize	A, B, W = list(h2.parameters())		coeffs = {}	for i, par in enumerate(zip(W.squeeze(), A.squeeze(), B.squeeze())):	w, a, b = par	coeffs[""w"" + str(i+1)] = w.item()	coeffs[""a"" + str(i+1)] = a.item()	coeffs[""b"" + str(i+1)] = b.item()	demo_function_approximation(num_functions=K, static=True, default_transform=""relu"", **coeffs)"
970,6_neural_networks_backpropagation.ipynb,191,markdown,slide,8,"# Summary	1. Any function can be approximated with just one nonlinearity.	2. Parameters of approximation can be found by minimization of loss function with gradient descent.	3. Gradients are efficiently computed using backpropagation algorithm on the computational graph.	4. Computations are grouped in layers with forward-backward API.	5. More layers help to extract features.	6. Feedforward neural networks are powerful (can approximate any function) and computationally efficient.	7. PyTorch basics: tensors, autogradients, modules."
971,6_neural_networks_backpropagation.ipynb,192,markdown,slide,13,"# Recommended resources		Backpropagation	-	[📖 CS231n: backpropagation](https://cs231n.github.io/optimization-2/)	- [📖 Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)		GPU	- [📺 CPU vs GPU (What's the Difference?) - Computerphile	](https://www.youtube.com/watch?v=_cyVDoyI6NE)		PyTorch	- [📺 ""PyTorch: Fast Differentiable Dynamic Graphs in Python"" by Soumith Chintala](https://www.youtube.com/watch?v=DBVLcgq2Eg0)"
972,6_neural_networks_backpropagation.ipynb,193,markdown,slide,7,`torch.autograd`	- https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html	- https://pytorch.org/docs/stable/notes/autograd.html#how-autograd-encodes-the-history	- https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95		`torch.nn`	- https://pytorch.org/tutorials/beginner/nn_tutorial.html#
973,7_rnn.ipynb,0,code,-,23,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	if 'harbour-space-text-mining-course' not in sys.path:	sys.path.append('harbour-space-text-mining-course')		# from tmcourse.demo import (	# )	# from tmcourse.quiz import (	# )	from tmcourse.ipyquiz import Quiz, Function	from tmcourse.utils import enable_mathjax_in_cell		from IPython.display import HTML, display		# get_ipython().events.register('pre_run_cell', enable_mathjax_in_cell)	from IPython.display import set_matplotlib_formats	%matplotlib inline	set_matplotlib_formats('svg')	import matplotlib.pyplot as plt	plt.rcParams['figure.figsize'] = (10.0, 6.0)"
974,7_rnn.ipynb,1,markdown,slide,1,<center><h1>Recurrent Neural Networks</h1></center>
975,7_rnn.ipynb,2,markdown,slide,5,# Outline	1. Motivation: order matters	1. Sequences and recurrent functions	1. Recurrent Neural Networks	1. RNN for classification and language modeling
976,7_rnn.ipynb,3,markdown,slide,1,# Motivation: order matters
977,7_rnn.ipynb,4,markdown,slide,2,"- Machine Learning algorithms (and Neural Networks in particular) require to represent data as vectors (or tensors in general)	- So far, we represented texts as vectors using TF-IDF and word2vec (averaged over words)"
978,7_rnn.ipynb,5,markdown,slide,7,"**Problem**: both methods lose order of words!		**Example**: two different hotel reviews	- ""great location, but poor experience overall""	- ""poor location, but great experience overall""		Both have the same vector representation."
979,7_rnn.ipynb,6,markdown,slide,3,**Possible solution**: use $n$-grams.		**Problem**: $n$-grams suffer from _sparsity problem_.
980,7_rnn.ipynb,7,markdown,slide,1,**What we want**: figure out how to process _sequences_ as inputs for neural networks.
981,7_rnn.ipynb,8,markdown,slide,1,# Sequences and recurrent functions
982,7_rnn.ipynb,9,markdown,slide,3,"**Toy example**: for a sequence of numbers $x_1, x_2, \dots, x_t, \dots$, for each $t$ compute the average value $y_t$.		**Our goal**:  _organize computations_ in the most general way."
983,7_rnn.ipynb,10,markdown,slide,1,- $x_1 = 5 \rightarrow y_1 = 5$
984,7_rnn.ipynb,11,markdown,slide,1,- $x_2 = 3 \rightarrow y_2 = \dfrac{3 + 5}{2} = 4$
985,7_rnn.ipynb,12,markdown,slide,1,- $x_3 = 10 \rightarrow y_3 = \dfrac{3 + 5 + 10}{3} = 6$
986,7_rnn.ipynb,13,markdown,slide,1,- $\dots$
987,7_rnn.ipynb,14,markdown,slide,18,"Let's organize computations as follows:		- Keep the _hidden state_: the sum of previous elements $s_t$ and the number of previous elements $n_t$	$$	s_0 = 0, n_0 = 0	$$	- For each $t$:	- Update the hidden state:	$$	s_t = s_{t-1} + x_t	$$	$$	n_t = n_{t-1} + 1	$$	- Compute the output:	$$	y_t = \dfrac{s_t}{n_t}	$$"
988,7_rnn.ipynb,15,markdown,slide,10,"	Abstracting from the concrete algorithm, this is what we are doing:	- Update the hidden state:	$$	h_t = f(\color{red}{h_{t-1}}, x_t)	$$	- Compute the output from the hidden state:	$$	y_t = g(\color{red}{h_{t}})	$$"
989,7_rnn.ipynb,16,markdown,slide,1,"For this particular problem, we know the functions $f$ and $g$. In general, we will **learn them from data**."
990,7_rnn.ipynb,17,markdown,slide,1,![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ml_vs_cs.png)
991,7_rnn.ipynb,18,markdown,slide,5,"The function	$$	h_t = f(h_{t-1}, x_t)	$$	is a _recurrent function_, because it depends on its previous values."
992,7_rnn.ipynb,19,markdown,slide,4,"We can ""unroll"" it back in time:	$$	h_t = f(h_{t-1}, x_t)	$$"
993,7_rnn.ipynb,20,markdown,slide,3,"$$	= f(f(h_{t-2}, x_{t-1}), x_t)	$$"
994,7_rnn.ipynb,21,markdown,slide,3,"$$	= f(f(f(h_{t-3}, x_{t-2}), x_{t-1}), x_t)	$$"
995,7_rnn.ipynb,22,markdown,slide,3,$$	\dots	$$
996,7_rnn.ipynb,23,markdown,slide,3,"$$	= f(f(f(\dots \color{red}{f(h_0, x_1)} \dots, x_{t-2}), x_{t-1}), x_t)	$$"
997,7_rnn.ipynb,24,markdown,slide,1,So it has a _dynamic_ computational graph (built while reading the sequence $x_t$).
998,7_rnn.ipynb,25,markdown,slide,3,"Computational graphs may be different:		<center><img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-all.png""></center>"
999,7_rnn.ipynb,26,markdown,slide,3,"""Vanilla"" (non-recurrent) function: one input, one output.		<center><img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-nn.png""></center>"
1000,7_rnn.ipynb,27,markdown,slide,3,"Classification of a sequence: read the full sequence, return the predicted label at the end.		<center><img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-classification.png""></center>"
1001,7_rnn.ipynb,28,markdown,slide,3,"Language Model: for each input word, predict the next word.		<center><img src=""https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-lm.png""></center>"
1002,7_rnn.ipynb,29,markdown,slide,1,# Recurrent Neural Networks
1003,7_rnn.ipynb,30,markdown,slide,9,"We formalize computations over sequences using recurrent functions:	$$	h_t = f(h_{t-1}, x_t)	$$	$$	y_t = g(h_{t})	$$		**Our goal**: learn functions $f$ and $g$ from data."
1004,7_rnn.ipynb,31,markdown,slide,9,"**Idea**: $f$ and $g$ are _layers_.	$$	f(h_{t-1}, x_t) = \mathrm{SomeFunction}(W_{hh}h_{t-1} + W_{xh} x_t)	$$	$$	y_t = \mathrm{SomeFunction}(W_{hy} h_t)	$$		Where $\mathrm{SomeFunction}$ may be $\mathrm{relu}$, $\mathrm{tanh}$, identity, ..."
1005,7_rnn.ipynb,32,markdown,slide,3,"**Q**: how to find $W_{hh}, W_{xh}, W_{hy}$?		**A**: ""unroll"" the computational graph and use backpropagation."
1006,7_rnn.ipynb,33,markdown,slide,13,"# Colab demo: classification with RNN		`ClassificationRNN` implements a Recurrent Neural Network for classification:	- Read the input sequence $x_1, x_2, \dots, x_t, \dots$	- Outputs are probability distributions over classes.	- Update hidden states and outputs according to RNN equations:	$$	h_t = f(h_{t-1}, x_t)	$$	$$	y_t = g(h_t)	$$	- The last output is the result of classification: it is compared with the true category."
1007,7_rnn.ipynb,34,code,-,32,"import torch.nn as nn		class ClassificationRNN(nn.Module):	def __init__(self, input_size, hidden_size, output_size):	super().__init__()	# instead of explicit matrix multiplications, we use Linear layers	# W_hh: hidden -> hidden	self.hh = nn.Linear(hidden_size, hidden_size)	# W_xh: input -> hidden	self.xh = nn.Linear(input_size, hidden_size)	# W_hy: hidden -> output	self.hy = nn.Linear(hidden_size, output_size)		# nonlinearities we will use:	# ReLU for h_t = f(h_{t-1}, x_t)	self.relu = nn.ReLU()	# LogSoftmax for y_t = g(h_t) (because we are doing classification)	self.softmax = nn.LogSoftmax(dim=1)		# keep hidden sise for initialization of h_0	self.hidden_size = hidden_size		def forward(self, x, h):	# h_t = f(h_{t-1}, x_t)	hidden = self.relu(self.hh(h) + self.xh(x))	# y_t = g(h_t)	output = self.softmax(self.hy(hidden))	return output, hidden		def h0(self):	# compute the initial value (all zeros) for the hidden state	return torch.zeros(1, self.hidden_size)"
1008,7_rnn.ipynb,35,markdown,-,1,RNN creates computational graph dynamically while reading the sequence. This is done in the function `process_sequence_classification`.
1009,7_rnn.ipynb,36,code,-,36,"import torch		def process_sequence_classification(rnn, target_tensor, input_tensor, learning_rate):	""""""	The algorithm:	- Read the input sequence left-to-right	- At each step, compute new hidden state and new output	- Get the last output (probability distribution over classes) and compute loss	- Backpropagate gradients and update parameters		input_tensor is a 3-dimensional tensor: sequence length x 1 x vocabulary size	- the first dimension: each element in this dimension corresponds to x_t	- the second dimension is a ""batch dimension"", in our case it always contains 1 element	- the last dimension contains components of x_t	""""""		# prepare for backprop	rnn.zero_grad()	# initialize hidden state	hidden = rnn.h0()		# read all elements of the input sequence, update hidden state and output	for i in range(input_tensor.size()[0]):	output, hidden = rnn(input_tensor[i], hidden)		# get the last output, compute the loss function and backpropagate	loss_function = nn.NLLLoss()	loss = loss_function(output, target_tensor)	loss.backward()		# update parameters	with torch.no_grad():	for parameter in rnn.parameters():	parameter -= learning_rate * parameter.grad		return loss.item()"
1010,7_rnn.ipynb,37,markdown,-,3,**Get the data**		The dataset contains names in different languages.
1011,7_rnn.ipynb,38,code,-,6,"from collections import defaultdict	category_to_names = defaultdict(list)	with open(""harbour-space-text-mining-course/datasets/pytorch_tutorial/names_by_language.txt"") as f:	for line in f:	name, category = line.strip().split(""\t"")	category_to_names[category].append(name)"
1012,7_rnn.ipynb,39,code,-,6,"from pprint import pprint	print(""Categories:"")	pprint([(k, len(v)) for k, v in category_to_names.items()])	print(""Total categories:"", len(category_to_names))	print(""Total names:"", sum(len(v) for v in category_to_names.values()))	print(""Example for German:"", category_to_names[""German""][10])"
1013,7_rnn.ipynb,40,markdown,-,9,"**Turning Names into Tensors**		Now that we have all the names organized, we need to turn them into Tensors to make any use of them.		To represent a single letter, we use a “one-hot vector” of size `<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. `""b"" = <0 1 0 0 0 ...>`.		To make a word we join a bunch of those into a 2D matrix `<line_length x 1 x n_letters>`.		That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here."
1014,7_rnn.ipynb,41,code,-,10,"# count total number of letters	all_letters = set()	all_categories = set()		for category, names in category_to_names.items():	all_categories.add(category)	for name in names:	all_letters.update(list(name))	all_letters = list(sorted(all_letters))	all_categories = list(sorted(all_categories))"
1015,7_rnn.ipynb,42,code,-,2,"from pprint import pprint	pprint(all_letters, compact=True)"
1016,7_rnn.ipynb,43,code,-,1,"pprint(all_categories, compact=True)"
1017,7_rnn.ipynb,44,markdown,-,1,"The function `name_to_tensor` creates 3-dimensional tensor from string `name`. It needs the ""vocabulary"" of all possible characters `all_letters`."
1018,7_rnn.ipynb,45,code,-,9,"def name_to_tensor(name, all_letters):	tensor = torch.zeros(len(name), 1, len(all_letters))	for li, letter in enumerate(name):	letter_idx = all_letters.index(letter)	tensor[li][0][letter_idx] = 1  # one-hot encoding	return tensor		name_tensor = name_to_tensor(""Tao"", all_letters)	name_tensor.size(), name_tensor"
1019,7_rnn.ipynb,46,markdown,-,5,"Create training samples for stochastic gradient descent.		We put them into a dictionary: keys are categories (languages), and values are pairs of tensors (tensor for category and for name).		It helps to cope with disbalanced classes: for each iteration of SGD, first we will pick a random class and then a random name of this class."
1020,7_rnn.ipynb,47,code,-,7,"category_to_training_samples = defaultdict(list)	for category, names in category_to_names.items():	category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)	for name in names:	name_tensor = name_to_tensor(name, all_letters)	sample = category_tensor, name_tensor	category_to_training_samples[category].append(sample)"
1021,7_rnn.ipynb,48,code,-,1,"category_to_training_samples[""Russian""][0]"
1022,7_rnn.ipynb,49,markdown,-,3,"Create the `rnn`:	- It reads a sequence of one-hot encoded letters, so the input size is the number of letters.	- It predicts one of `all_categories`, so the output size is the number of categories."
1023,7_rnn.ipynb,50,code,-,2,"n_hidden = 128	rnn = ClassificationRNN(len(all_letters), n_hidden, len(all_categories))"
1024,7_rnn.ipynb,51,markdown,-,1,Apply `rnn` on a sample to get the idea how everything works together.
1025,7_rnn.ipynb,52,code,-,8,"ts = category_to_training_samples[""Arabic""][999]	print(""Training sample:"")	print(ts)	print()	category_tensor, i = ts	h = rnn.h0()	o, _ = rnn(i[0], h)	print(o)"
1026,7_rnn.ipynb,53,markdown,-,4,"Train RNN using SGD:	- Pick a random sample (first pick a category, then a name)	- Update weights (`process_sequence`)	- Keep losses"
1027,7_rnn.ipynb,54,code,-,27,"import math	from tqdm.notebook import tqdm	import random	import numpy as np	random.seed(0)		n_iters = 100000	print_every = 5000		# Keep track of losses for plotting	all_losses = []	iterations = []	losses = []		for i in tqdm(range(1, n_iters + 1)):	# sample categories uniformly	random_category = random.choice(all_categories)	category_tensor, name_tensor = random.choice(category_to_training_samples[random_category])	# process sequence: update RNN weights and compute loss	loss = process_sequence_classification(rnn, category_tensor, name_tensor, 0.001)	all_losses.append(loss)		if i % print_every == 0:	iterations.append(i)	avg_loss = np.mean(all_losses[-print_every:])	losses.append(avg_loss)	print(f""iteration {i}: loss {avg_loss}"")"
1028,7_rnn.ipynb,55,markdown,-,1,Visualize losses
1029,7_rnn.ipynb,56,code,-,4,"import matplotlib.pyplot as plt		plt.plot(iterations, losses)	plt.show()"
1030,7_rnn.ipynb,57,markdown,-,1,Look at the predictions.
1031,7_rnn.ipynb,58,code,-,8,"def evaluate(rnn, name_tensor):	# read the sequence and return the final output	hidden = rnn.h0()		for i in range(name_tensor.size()[0]):	output, hidden = rnn(name_tensor[i], hidden)		return output"
1032,7_rnn.ipynb,59,code,-,19,"def predict(input_name, n_predictions=3):	print('\n> %s' % input_name)	with torch.no_grad():	output = evaluate(rnn, name_to_tensor(input_name, all_letters))		# Get top N categories	topv, topi = output.topk(n_predictions, 1, True)	predictions = []		for i in range(n_predictions):	value = topv[0][i].item()	category_index = topi[0][i].item()	print('(%.2f) %s' % (value, all_categories[category_index]))	predictions.append([value, all_categories[category_index]])		predict('Satoshi')	predict('Khoroshenkikh')	predict('Hinton')	predict('Bengio')"
1033,7_rnn.ipynb,60,markdown,slide,53,"# Colab demo: language model with RNN		`LanguageModelRNN` implements a Recurrent Neural Network for language modeling.		We will train many language models for different languages at once using _conditioning_. It means that for each sequence we will add the category to the input.		- Read the input sequence $x_1, x_2, \dots, x_t, \dots$	- The input sequence has the category $c$.	- Outputs are probability distributions over letters.	- Update hidden states and outputs according to RNN equations:	$$	h_t = f(h_{t-1}, x_t, \color{red}{c})	$$	$$	y_t = g(h_t)	$$	- For each $t$, the target output is the next letter (with a special character for the end of a sequence). For example:	<table>	<thead>	<tr>	<th>t</th>	<th>input</th>	<th>target output</th>	</tr>	</thead>	<tbody>	<tr>	<td>1</td>	<td>T</td>	<td>r</td>	</tr>	<tr>	<td>2</td>	<td>r</td>	<td>u</td>	</tr>	<tr>	<td>3</td>	<td>u</td>	<td>m</td>	</tr>	<tr>	<td>4</td>	<td>m</td>	<td>p</td>	</tr>	<tr>	<td>5</td>	<td>p</td>	<td>&lt;EOS&gt;</td>	</tr>	</tbody>	</table>"
1034,7_rnn.ipynb,61,code,-,27,"import torch	import torch.nn as nn		class LanguageModelRNN(nn.Module):	def __init__(self, input_size, category_size, hidden_size, output_size):	super().__init__()	self.hidden_size = hidden_size		# hidden -> hidden	self.hh = nn.Linear(hidden_size, hidden_size)	# input -> hidden	self.xh = nn.Linear(input_size, hidden_size)	# category -> hidden	self.ch = nn.Linear(category_size, hidden_size)	# hidden -> output	self.hy = nn.Linear(hidden_size, output_size)	self.relu = nn.ReLU()	self.softmax = nn.LogSoftmax(dim=1)		def forward(self, c, x, h):	# h_t = f(h_{t-1}, x_t, c)	hidden = self.relu(self.hh(h) + self.xh(x) + self.ch(c))	output = self.softmax(self.hy(hidden))	return output, hidden		def h0(self):	return torch.zeros(1, self.hidden_size)"
1035,7_rnn.ipynb,62,code,-,25,"def process_sequence_language_model(	category_tensor,	input_line_tensor,	target_line_tensor,	learning_rate	):	loss_function = nn.NLLLoss()	hidden = rnn.h0()	rnn.zero_grad()		loss = 0		for i in range(input_line_tensor.size(0)):	output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)	# update the loss for each element x_t	l = loss_function(output, target_line_tensor[i])	loss += l		loss.backward()		with torch.no_grad():	for parameter in rnn.parameters():	parameter -= learning_rate * parameter.grad		return loss.item() / input_line_tensor.size(0)"
1036,7_rnn.ipynb,63,code,-,22,"# One-hot vector for category	def get_category_tensor(category, all_categories):	li = all_categories.index(category)	tensor = torch.zeros(1, len(all_categories))	tensor[0][li] = 1	return tensor		# One-hot matrix of first to last letters (not including <EOS>) for input	def get_input_tensor(line, all_letters):	n_letters = len(all_letters) + 1	tensor = torch.zeros(len(line), 1, n_letters)	for li in range(len(line)):	letter = line[li]	tensor[li][0][all_letters.index(letter)] = 1	return tensor		# LongTensor of second letter to end (<EOS>) for target	def get_target_tensor(line, all_letters):	n_letters = len(all_letters) + 1	letter_indexes = [all_letters.index(line[li]) for li in range(1, len(line))]	letter_indexes.append(n_letters - 1)  # <EOS>	return torch.LongTensor(letter_indexes).unsqueeze(-1)"
1037,7_rnn.ipynb,64,code,-,3,"n_letters = len(all_letters) + 1  # Plus EOS marker	n_categories = len(all_categories)	rnn = LanguageModelRNN(n_letters, n_categories, 128, n_letters)"
1038,7_rnn.ipynb,65,code,-,30,"n_iters = 100000	print_every = 5000	all_losses = []	iterations = []	losses = []		for i in tqdm(range(1, n_iters + 1)):	# sample categories uniformly	random_category = random.choice(all_categories)	sample_idx = random.choice(range(len(category_to_training_samples[random_category])))	category_tensor, line_tensor = category_to_training_samples[random_category][sample_idx]	name = category_to_names[random_category][sample_idx]		category_tensor = get_category_tensor(random_category, all_categories)	input_tensor = get_input_tensor(name, all_letters)	target_tensor = get_target_tensor(name, all_letters)	loss = process_sequence_language_model(	category_tensor,	input_tensor,	target_tensor,	0.0005	)		all_losses.append(loss)		if i % print_every == 0:	iterations.append(i)	avg_loss = np.mean(all_losses[-print_every:])	losses.append(avg_loss)	print(f""iteration {i}: loss {avg_loss}"")"
1039,7_rnn.ipynb,66,code,-,4,"import matplotlib.pyplot as plt		plt.plot(iterations, losses)	plt.show()"
1040,7_rnn.ipynb,67,markdown,-,1,Sampling
1041,7_rnn.ipynb,68,code,-,33,"max_length = 20		# Sample from a category and starting letter	def sample(category, start_letter='A'):	with torch.no_grad():  # no need to track history in sampling	category_tensor = get_category_tensor(category, all_categories)	input_tensor = get_input_tensor(start_letter, all_letters)	hidden = rnn.h0()		output_name = start_letter		for i in range(max_length):	output, hidden = rnn(category_tensor, input_tensor[0], hidden)	topv, topi = output.topk(1)	topi = topi[0][0]	if topi == n_letters - 1:	break	else:	letter = all_letters[topi]	output_name += letter	input_tensor = get_input_tensor(letter, all_letters)		return output_name		# Get multiple samples from one category and multiple starting letters	def samples(category, start_letters='ABC'):	for start_letter in start_letters:	print(sample(category, start_letter))		samples('Russian', 'RUS')	samples('German', 'GER')	samples('Spanish', 'SPA')	samples('Chinese', 'CHI')"
1042,7_rnn.ipynb,69,markdown,slide,16,# Coding session		You are given the dataset with startup investments information: [link](https://www.kaggle.com/arindam235/startup-investments-crunchbase). It contains columns `name` and `market`.		## Exercise 1	- Find top 10 markets.	- Get startup names that have markets from top-10.	- Create tensors for `ClassificationRNN`. Encode each market with `sklearn.preprocessing.LabelEncoder`.		## Exercise 2	Predict market given a startup name. Use tensors from the Exercise 1.		## [OPTIONAL] Exercise 3	- Create tensors for language model.	- Train LanguageModel conditioned on market.	- Generate a few startup names.
1043,7_rnn.ipynb,70,code,-,3,"import pandas as pd	df = pd.read_csv(""harbour-space-text-mining-course/datasets/startup_investments/investments_VC.csv"")	df.head()"
1044,7_rnn.ipynb,71,code,-,1,# YOUR CODE HERE
1045,7_rnn.ipynb,72,markdown,slide,5,"# Summary	1. Recurrent functions accept sequences as inputs	1. Recurrent Neural Networks are recurrent _layers_: they consist of linear transformation followed by activation functions	1. RNNs are trained with backpropagation on ""unrolled"" computational graph	1. Examples of tasks solved by RNNs: classification and language modeling"
1046,7_rnn.ipynb,73,markdown,slide,6,# Recommended resources	- [CS224n Lecture 6: Language Models and Recurrent Neural Networks](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture06-rnnlm.pdf)	- [CS231n Lecture 10: Recurrent Neural Networks](http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf)	- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)	- [NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)	- [NLP FROM SCRATCH: GENERATING NAMES WITH A CHARACTER-LEVEL RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
1047,8_rnn_2.ipynb,0,code,-,23,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	if 'harbour-space-text-mining-course' not in sys.path:	sys.path.append('harbour-space-text-mining-course')		# from tmcourse.demo import (	# )	# from tmcourse.quiz import (	# )	from tmcourse.ipyquiz import Quiz, Function	from tmcourse.utils import enable_mathjax_in_cell		from IPython.display import HTML, display		# get_ipython().events.register('pre_run_cell', enable_mathjax_in_cell)	from IPython.display import set_matplotlib_formats	%matplotlib inline	set_matplotlib_formats('svg')	import matplotlib.pyplot as plt	plt.rcParams['figure.figsize'] = (10.0, 6.0)"
1048,8_rnn_2.ipynb,1,markdown,slide,1,<h1><center>Recurrent Neural Networks - 2</center></h1>
1049,8_rnn_2.ipynb,2,markdown,slide,11,"# Last time	1. Recurrent functions accept sequences as inputs.	$$	h_t = f(x_t, h_{t-1})	$$	1. Recurrent Neural Networks are recurrent _layers_: they consist of linear transformations followed by activation functions.	$$	h_t = \mathrm{SomeFunction}(W_{xh}x_t + W_{hh}h_{t-1})	$$	1. RNNs are trained with backpropagation on ""unrolled"" computational graph.	1. Examples of tasks solved by RNNs: classification and language modeling."
1050,8_rnn_2.ipynb,3,markdown,slide,4,# Plan for today	1. Methods of gradient optimization.	1. When RNN fails: conflicting updates.	1. Long Short-Term Memory (LSTM)
1051,8_rnn_2.ipynb,4,markdown,slide,3,# Methods of gradient optimization	## Colab demo: gradient methods for neural networks	[link](https://colab.research.google.com/drive/1TEX4GuQLU1oEYes-zhAMvoahRZ1yJHMl?usp=sharing)
1052,8_rnn_2.ipynb,5,markdown,slide,1,## Colab demo: PyTorch optimizers
1053,8_rnn_2.ipynb,6,code,-,27,"import torch	import torch.nn as nn		class RNN(nn.Module):	# the same as LanguageModelRNN from the previous lecture, but without conditioning	def __init__(self, input_size, hidden_size, output_size):	super().__init__()	self.hidden_size = hidden_size		# hidden -> hidden	self.hh = nn.Linear(hidden_size, hidden_size)	# input -> hidden	self.xh = nn.Linear(input_size, hidden_size)	# hidden -> output	self.hy = nn.Linear(hidden_size, output_size)	self.relu = nn.ReLU()	self.softmax = nn.LogSoftmax(dim=1)		def forward(self, x, h):	# h_t = f(h_{t-1}, x_t)	hidden = self.relu(self.hh(h) + self.xh(x))	# o_t = g(h_t)	output = self.softmax(self.hy(hidden))	return output, hidden		def h0(self):	return torch.zeros(1, self.hidden_size)"
1054,8_rnn_2.ipynb,7,markdown,-,3,`process_sequence_rnn()` builds many-to-many computational graph.		![link text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-lm.png)
1055,8_rnn_2.ipynb,8,code,-,31,"def process_sequence_rnn(	rnn,	input_line_tensor,	target_line_tensor,	learning_rate	):	# the same as process_sequence_language_model from the previous lecture	# (but witout conditioning)		# instead of manual parameter update, we will use PyTorch optimizer	# create the optimizer	optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)	optimizer.zero_grad()		loss_function = nn.NLLLoss()	hidden = rnn.h0()		loss = 0		for i in range(input_line_tensor.size(0)):	output, hidden = rnn(input_line_tensor[i], hidden)	# update the loss for each element x_t	l = loss_function(output, target_line_tensor[i])	loss += l		loss.backward()		# perform gradient step	optimizer.step()		return loss.item() / input_line_tensor.size(0)"
1056,8_rnn_2.ipynb,9,markdown,slide,1,# When RNN fails: conflicting updates
1057,8_rnn_2.ipynb,10,code,slide,19,"def toy_example(char_seq):	""""""	Synthetic dataset.	Input: sequence of characters `char_seq`.	Target: ""turn on"" 1 if ""+"" is found, wait for ""-"" to turn off.	""""""	target = []	turned_on = 0	for c in char_seq:	if c == ""+"":	turned_on = 1	elif c == ""-"":	turned_on = 0	target.append(turned_on)	return target		inp = ""aajh-dd+dklddf+dd-jjola""	print(""input: "", inp)	print(""target:"", """".join(map(str, toy_example(inp))))"
1058,8_rnn_2.ipynb,11,markdown,slide,1,## Colab demo: train RNN on the toy dataset
1059,8_rnn_2.ipynb,12,markdown,-,1,Generate synthetic sequences with `generate_data()`.
1060,8_rnn_2.ipynb,13,code,-,35,"import torch	import random		def get_vocabulary(n_additional_symbols):	lower = ""abcdefghijklmnopqrstuvwxyz""	return list(""+-"" + lower[:n_additional_symbols])		def generate_data(n_samples, seed=0, n_additional_symbols=0):	alphabet = get_vocabulary(n_additional_symbols)	random.seed(seed)		char_tensor = torch.zeros(n_samples, 1, len(alphabet))  # one-hotted, 1 batch	target_tensor = torch.zeros(n_samples, 1)	index_seq = random.choices(range(len(alphabet)), k=n_samples)	char_seq = [alphabet[i] for i in index_seq]  # human-readable	for k, idx in enumerate(index_seq):	char_tensor[k][0][idx] = 1		# target: turn on if ""+"", turn off if ""-""	target_seq = toy_example(char_seq)	for i, target in enumerate(target_seq):	target_tensor[i][0] = target		return """".join(char_seq), """".join(map(str, target_seq)), char_tensor, target_tensor.type(torch.LongTensor)		print(""0 additional symbols"")	chars, target, chars_tensor, target_tensor = generate_data(100)	print(chars)	print(target)	print()		print(""10 additional symbols"")	chars, target, chars_tensor, target_tensor = generate_data(100, n_additional_symbols=10)	print(chars)	print(target)"
1061,8_rnn_2.ipynb,14,markdown,-,1,Training loop for many-to-many RNN.
1062,8_rnn_2.ipynb,15,code,-,43,"def train_many_to_many_rnn(	n_additional_symbols=0,	hidden_size=16,	sequence_length=50,	n_iters=1000,	learning_rate=0.0005	):	from tqdm.notebook import tqdm	import numpy as np	import matplotlib.pyplot as plt	print_every = n_iters // 20		rnn = RNN(n_additional_symbols + 2, hidden_size, 2)	all_losses = []	iterations = []	losses = []		for i in tqdm(range(1, n_iters + 1)):	# random sample	chars, target, input_tensor, target_tensor = generate_data(	sequence_length,	seed=i,	n_additional_symbols=n_additional_symbols	)	loss = process_sequence_rnn(	rnn,	input_tensor,	target_tensor,	learning_rate	)		all_losses.append(loss)		if i % print_every == 0:	iterations.append(i)	avg_loss = np.mean(all_losses[-print_every:])	losses.append(avg_loss)	print(f""iteration {i}: loss {avg_loss}"")		plt.plot(iterations, losses)	plt.show()		return rnn"
1063,8_rnn_2.ipynb,16,markdown,-,1,Auxiliary functions.
1064,8_rnn_2.ipynb,17,code,-,32,"def evaluate(rnn, input_tensor):	# read the sequence and return the final output	hidden = rnn.h0()		outputs = torch.zeros(input_tensor.size()[0], 1, 2)	for i in range(input_tensor.size()[0]):	output, hidden = rnn(input_tensor[i], hidden)	outputs[i][0] = output		return outputs		def prediction_chars(prediction_tensor):	chars = []	for i in range(prediction_tensor.size()[0]):	if prediction_tensor[i][0][0] > prediction_tensor[i][0][1]:	chars.append('0')	else:	chars.append('1')	return chars		def visualize_predictions(rnn, seed, n_additional_symbols, sequence_length=50):	chars, target, input_tensor, target_tensor = generate_data(	sequence_length,	seed=seed,	n_additional_symbols=n_additional_symbols	)		prediction = evaluate(rnn, input_tensor)		print(""input:     "", """".join(chars))	print(""target:    "", """".join(target))	print(""prediction:"", """".join(prediction_chars(prediction)))"
1065,8_rnn_2.ipynb,18,markdown,-,1,### Case 1: 2 symbols in the input sequence (no additional symbols)
1066,8_rnn_2.ipynb,19,code,-,1,"rnn = train_many_to_many_rnn(n_additional_symbols=0, n_iters=1500)"
1067,8_rnn_2.ipynb,20,code,-,1,"visualize_predictions(rnn, 99999, 0)"
1068,8_rnn_2.ipynb,21,markdown,-,2,### Case 2: 10 additional symbols	More iterations are needed to converge.
1069,8_rnn_2.ipynb,22,code,-,1,"rnn = train_many_to_many_rnn(n_additional_symbols=10, n_iters=4000)"
1070,8_rnn_2.ipynb,23,code,-,1,"visualize_predictions(rnn, 99999, 10)"
1071,8_rnn_2.ipynb,24,markdown,-,5,### Case 3: 26 additional symbols	Note that there are 28 symbols but only 16 components in the hidden state.	This is typical for practical applications: the size of vocabulary is larger than the number of components of the hidden state.		More iterations and lower learning rate.
1072,8_rnn_2.ipynb,25,code,-,1,"rnn = train_many_to_many_rnn(n_additional_symbols=26, n_iters=10000, learning_rate=0.0002)"
1073,8_rnn_2.ipynb,26,code,-,1,"visualize_predictions(rnn, 99999, 26)"
1074,8_rnn_2.ipynb,27,markdown,slide,8,"## Problem: conflicting updates		RNN update rule:	$$	h_t = f(W_{xh} x_t + W_{hh} h_{t-1})	$$		**Problem**: $h_t$ is always updated, but _sometimes_ (for some inputs) we want to protect it from updates."
1075,8_rnn_2.ipynb,28,markdown,slide,15,"	**Notation**: one-hot encoding of inputs and outputs.		**Input**:	$$x = (0, 1, \dots)^T$$	corresponds to `""+""` character, and	$$x = (1, 0, \dots)^T$$	corresponds to `""-""` character.		**Output**:	$$h = (0, 1)^T$$	corresponds to label 1, and	$$h = (1, 0)^T$$	corresponds to label 0.	"
1076,8_rnn_2.ipynb,29,markdown,slide,15,"Only the first two components of $x_t$ are relevant, so we want	$$	W_{xh} = \begin{pmatrix}	1 & 0 & 0 & 0 & \dots\\	0 & 1 & 0 & 0 & \dots\\	\end{pmatrix}	$$		We need the update rule with ""protection"":	$$	h_t = \begin{cases}	W_{xh} x_t & \textrm{if }W_{xh} x_t \textrm{ is nonzero}\\	h_{t-1}, & \textrm{if }W_{xh} x_t \textrm{ is zero}	\end{cases}	$$"
1077,8_rnn_2.ipynb,30,markdown,slide,10,"# Long Short-Term Memory (LSTM)		[Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)		<table>	<tr>	<td><img src=""https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Example-of-LSTM-Architecture.png"" height=""400""/></td>	<td><img src=""https://static.tvtropes.org/pmwiki/pub/images/6db_3.jpg"" height=""400""/> </td>	</tr>	</table>"
1078,8_rnn_2.ipynb,31,markdown,slide,3,"**Idea**: treat the hidden state as ""memory"" and control its updates depending on the input.		> This is the same what we do in the toy example: update the state only for the particular inputs (`""+""` or `""-""`)."
1079,8_rnn_2.ipynb,32,markdown,slide,3,"**What we have**: layers (matrices + nonlinearities).		**What we want**: memory, conditions (if-else)."
1080,8_rnn_2.ipynb,33,markdown,slide,4,**Memory** will be represented as a vector.	- Each element stores a real number.	- An element can be erased.	- An element can be incremented/decremented.
1081,8_rnn_2.ipynb,34,markdown,slide,1,"**Conditions** correspond to the _sigmoid function_, which returns values between 0 (""False"") and 1 (""True"")."
1082,8_rnn_2.ipynb,35,markdown,slide,1,"**Memory updates** (increments and decrements) correspond to _tanh_, which returns values between -1 (""decrement"") and 1 (""increment"")."
1083,8_rnn_2.ipynb,36,markdown,slide,5,"## 1. Cell state and output gate		In general, algorithms don't return the full memory state. So we need to	1. Separate the full memory state from the returning state	1. Decide which parts of the full memory state to return"
1084,8_rnn_2.ipynb,37,markdown,slide,3,"The full memory state is called **cell state** $c_t$.		From the cell state $c_t$, we ""return"" only some elements."
1085,8_rnn_2.ipynb,38,markdown,slide,9,"We need a function $o_t$ that tells which elements of $c_t$ should be returned:	- This function depends on inputs: $o_t(x_t, h_{t-1})$.	- For each element, it should ""return"" 0 or 1.	- We will _learn_ this function, so represent it as a layer:	$$	o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})	$$		The function $o_t$ that decides which parts of memory to return is called **output gate**."
1086,8_rnn_2.ipynb,39,markdown,slide,4,"Also, we control the magnitute of the output using tanh:	$$	h_t = o_t \odot \mathrm{tanh}(c_t)	$$"
1087,8_rnn_2.ipynb,40,markdown,slide,8,"## 2. Update cell state		From the input $x_t, h_{t-1}$, we decide how to update the cell state $c_t$ (increment or decrement elements).	$$	g_t = \mathrm{tanh}(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})	$$		"
1088,8_rnn_2.ipynb,41,markdown,slide,5,But we cannot write	$$	c_t = g_t	$$	because we also need to _protect_ some parts of memory.
1089,8_rnn_2.ipynb,42,markdown,slide,3,"## 3. Input gate and forget gate		To _protect_ some parts of memory $c_t$ from overwriting, we need a function $i_t$ that tells which elements should be overwritten."
1090,8_rnn_2.ipynb,43,markdown,slide,6,"Again, it should return ""boolean"" (between 0 and 1) values, and we learn it, so it is a layer:	$$	i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})	$$		This function is called **input gate**."
1091,8_rnn_2.ipynb,44,markdown,slide,3,"Similarly, we need to erase (forget) some parts of memory.		"
1092,8_rnn_2.ipynb,45,markdown,slide,4,"We need a function $f_t$  (**forget gate**) that ""zeroes out"" some elements of $c_t$ depending on input.	$$	f_t = \sigma(W_{if}x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})	$$"
1093,8_rnn_2.ipynb,46,markdown,slide,4,The final update of the cell state is	$$	c_t = f_t \odot c_{t-1} + i_t \odot g_t	$$
1094,8_rnn_2.ipynb,47,markdown,slide,20,"Putting it all together, we obtain _LSTM equations_:		$$	i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})	$$	$$	f_t = \sigma(W_{if}x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})	$$	$$	g_t = \mathrm{tanh}(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})	$$	$$	o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})	$$	$$	c_t = f_t \odot c_{t-1} + i_t \odot g_t	$$	$$	h_t = o_t \odot \mathrm{tanh}(c_t)	$$"
1095,8_rnn_2.ipynb,48,markdown,slide,1,## Colab demo: LSTM implementation in PyTorch
1096,8_rnn_2.ipynb,49,code,-,52,"class LSTM(nn.Module):	def __init__(self, input_size, hidden_size, output_size):	super().__init__()	self.hidden_size = hidden_size		# input gate	self.ii = nn.Linear(input_size, hidden_size)	self.hi = nn.Linear(hidden_size, hidden_size)		# forget gate	self.if_ = nn.Linear(input_size, hidden_size)	self.hf = nn.Linear(hidden_size, hidden_size)		# cell	self.ig = nn.Linear(input_size, hidden_size)	self.hg = nn.Linear(hidden_size, hidden_size)		# output gate	self.io = nn.Linear(input_size, hidden_size)	self.ho = nn.Linear(hidden_size, hidden_size)		self.tanh = nn.Tanh()	self.sigmoid = nn.Sigmoid()		# transformation of LSTM output to log-probabilities	self.oo = nn.Linear(hidden_size, output_size)	self.softmax = nn.LogSoftmax(dim=1)		def forward(self, x, state):	h, c = state  # state is a pair of hidden state and cell state		# input gate	i = self.sigmoid(self.ii(x) + self.hi(h))	# forget gate	f = self.sigmoid(self.if_(x) + self.hf(h))	# cell	g = self.tanh(self.ig(x) + self.hg(h))	# output gate	o = self.sigmoid(self.io(x) + self.ho(h))		# update cell state	cell = f * c + i * g	# update hidden state	hidden = o * self.tanh(cell)		# output	output = self.softmax(self.oo(hidden))		return output, (hidden, cell)		def h0c0(self):	return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)"
1097,8_rnn_2.ipynb,50,code,-,27,"def process_sequence_lstm(	lstm,	input_line_tensor,	target_line_tensor,	learning_rate	):	loss_function = nn.NLLLoss()	state = lstm.h0c0()	lstm.zero_grad()		loss = 0		for i in range(input_line_tensor.size(0)):	output, state = lstm(input_line_tensor[i], state)	# update the loss for each element x_t	l = loss_function(output, target_line_tensor[i])	loss += l		loss.backward()		# instead of manual parameter update, we use PyTorch optimizer	# create the optimizer	optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)	# perform gradient step	optimizer.step()		return loss.item() / input_line_tensor.size(0)"
1098,8_rnn_2.ipynb,51,code,-,43,"def train_many_to_many_lstm(	n_additional_symbols=0,	hidden_size=16,	sequence_length=50,	n_iters=1000,	learning_rate=0.0005	):	from tqdm.notebook import tqdm	import numpy as np	import matplotlib.pyplot as plt	print_every = n_iters // 20		lstm = LSTM(n_additional_symbols + 2, hidden_size, 2)	all_losses = []	iterations = []	losses = []		for i in tqdm(range(1, n_iters + 1)):	# random sample	chars, target, input_tensor, target_tensor = generate_data(	sequence_length,	seed=i,	n_additional_symbols=n_additional_symbols	)	loss = process_sequence_lstm(	lstm,	input_tensor,	target_tensor,	learning_rate	)		all_losses.append(loss)		if i % print_every == 0:	iterations.append(i)	avg_loss = np.mean(all_losses[-print_every:])	losses.append(avg_loss)	print(f""iteration {i}: loss {avg_loss}"")		plt.plot(iterations, losses)	plt.show()		return lstm"
1099,8_rnn_2.ipynb,52,markdown,-,1,### Case 1: 2 symbols in the input sequence (no additional symbols)
1100,8_rnn_2.ipynb,53,code,-,1,"train_many_to_many_lstm(n_additional_symbols=0, n_iters=1500)"
1101,8_rnn_2.ipynb,54,markdown,-,1,### Case 2: 10 additional symbols
1102,8_rnn_2.ipynb,55,code,-,1,"train_many_to_many_lstm(n_additional_symbols=10, n_iters=4000)"
1103,8_rnn_2.ipynb,56,markdown,-,1,### Case 3: 26 additional symbols
1104,8_rnn_2.ipynb,57,code,-,1,"lstm = train_many_to_many_lstm(n_additional_symbols=26, n_iters=7000)"
1105,8_rnn_2.ipynb,58,markdown,slide,1,## Colab demo: `nn.LSTM`
1106,8_rnn_2.ipynb,59,code,-,25,"import torch	import torch.nn as nn		class ClassificationLSTM(nn.Module):	def __init__(self, input_size, hidden_size, output_size):	super().__init__()	self.lstm = nn.LSTM(input_size, hidden_size)	self.out = nn.Linear(hidden_size, output_size)	self.softmax = nn.LogSoftmax(dim=1)		# keep hidden sise for initialization of h_0, c_0	self.hidden_size = hidden_size		def forward(self, x):	# Set initial states	h0 = torch.zeros(1, 1, self.hidden_size)	c0 = torch.zeros(1, 1, self.hidden_size)		# Forward propagate LSTM	out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (seq_len, batch, hidden_size)		# Decode the hidden state of the last time step	out = self.out(out[-1, :, :])		return self.softmax(out)"
1107,8_rnn_2.ipynb,60,code,-,13,"def process_sequence_classification_lstm(lstm, target_tensor, input_tensor, learning_rate):	loss_function = nn.NLLLoss()	optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)		output = lstm(input_tensor)	loss = loss_function(output, target_tensor)		# backprop + update	optimizer.zero_grad()	loss.backward()	optimizer.step()		return loss.item()"
1108,8_rnn_2.ipynb,61,markdown,-,1,Get the data
1109,8_rnn_2.ipynb,62,code,-,101,"#@title run this cell to copy-paste the code from the previous lecture: get the data, define ClassificationRNN		from collections import defaultdict	category_to_names = defaultdict(list)	with open(""harbour-space-text-mining-course/datasets/pytorch_tutorial/names_by_language.txt"") as f:	for line in f:	name, category = line.strip().split(""\t"")	category_to_names[category].append(name)		# count total number of letters	all_letters = set()	all_categories = set()		for category, names in category_to_names.items():	all_categories.add(category)	for name in names:	all_letters.update(list(name))	all_letters = list(sorted(all_letters))	all_categories = list(sorted(all_categories))		def name_to_tensor(name, all_letters):	tensor = torch.zeros(len(name), 1, len(all_letters))	for li, letter in enumerate(name):	letter_idx = all_letters.index(letter)	tensor[li][0][letter_idx] = 1  # one-hot encoding	return tensor		category_to_training_samples = defaultdict(list)	for category, names in category_to_names.items():	category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)	for name in names:	name_tensor = name_to_tensor(name, all_letters)	sample = category_tensor, name_tensor	category_to_training_samples[category].append(sample)		class ClassificationRNN(nn.Module):	def __init__(self, input_size, hidden_size, output_size):	super().__init__()	# instead of explicit matrix multiplications, we use Linear layers	# W_hh: hidden -> hidden	self.hh = nn.Linear(hidden_size, hidden_size)	# W_xh: input -> hidden	self.xh = nn.Linear(input_size, hidden_size)	# W_hy: hidden -> output	self.hy = nn.Linear(hidden_size, output_size)		# nonlinearities we will use:	# ReLU for h_t = f(h_{t-1}, x_t)	self.relu = nn.ReLU()	# LogSoftmax for y_t = g(h_t) (because we are doing classification)	self.softmax = nn.LogSoftmax(dim=1)		# keep hidden sise for initialization of h_0	self.hidden_size = hidden_size		def forward(self, x, h):	# h_t = f(h_{t-1}, x_t)	hidden = self.relu(self.hh(h) + self.xh(x))	# y_t = g(h_t)	output = self.softmax(self.hy(hidden))	return output, hidden		def h0(self):	# compute the initial value (all zeros) for the hidden state	return torch.zeros(1, self.hidden_size)	import torch		def process_sequence_classification(rnn, target_tensor, input_tensor, learning_rate):	""""""	The algorithm:	- Read the input sequence left-to-right	- At each step, compute new hidden state and new output	- Get the last output (probability distribution over classes) and compute loss	- Backpropagate gradients and update parameters		input_tensor is a 3-dimensional tensor: sequence length x 1 x vocabulary size	- the first dimension: each element in this dimension corresponds to x_t	- the second dimension is a ""batch dimension"", in our case it always contains 1 element	- the last dimension contains components of x_t	""""""		# prepare for backprop	rnn.zero_grad()	# initialize hidden state	hidden = rnn.h0()		# read all elements of the input sequence, update hidden state and output	for i in range(input_tensor.size()[0]):	output, hidden = rnn(input_tensor[i], hidden)		# get the last output, compute the loss function and backpropagate	loss_function = nn.NLLLoss()	loss = loss_function(output, target_tensor)	loss.backward()		# update parameters	with torch.no_grad():	for parameter in rnn.parameters():	parameter -= learning_rate * parameter.grad		return loss.item()"
1110,8_rnn_2.ipynb,63,markdown,-,1,Create and train `ClassificationLSTM`. Compare with `ClassificationRNN`.
1111,8_rnn_2.ipynb,64,code,-,41,"n_hidden = 128	classification_lstm = ClassificationLSTM(len(all_letters), n_hidden, len(all_categories))	classification_rnn = ClassificationRNN(len(all_letters), n_hidden, len(all_categories))		import math	from tqdm.notebook import tqdm	import random	import numpy as np	import matplotlib.pyplot as plt	random.seed(0)		n_iters = 30000	print_every = 1500		all_losses = []	iterations = []	losses = []	all_rnn_losses = []	rnn_losses = []		for i in tqdm(range(1, n_iters + 1)):	random_category = random.choice(all_categories)	category_tensor, name_tensor = random.choice(category_to_training_samples[random_category])		loss = process_sequence_classification_lstm(classification_lstm, category_tensor, name_tensor, 0.0005)	all_losses.append(loss)		rnn_loss = process_sequence_classification(classification_rnn, category_tensor, name_tensor, 0.001)	all_rnn_losses.append(rnn_loss)	if i % print_every == 0:	iterations.append(i)	avg_loss = np.mean(all_losses[-print_every:])	losses.append(avg_loss)	avg_rnn_loss = np.mean(all_rnn_losses[-print_every:])	rnn_losses.append(avg_rnn_loss)	print(f""iteration {i}: LSTM loss {avg_loss}, RNN loss {avg_rnn_loss}"")		plt.plot(iterations, losses, label=""lstm"")	plt.plot(iterations, rnn_losses, label=""rnn"")	plt.legend()	plt.show()"
1112,8_rnn_2.ipynb,65,markdown,slide,6,"# Coding session	## Exercise 1	Refactor your solution from the coding session `6.5_pytorch.ipynb`: in the function `train_clf`, use Adam optimizer.		## Exercise 2	Refactor your solution from the coding session `7_rnn.ipynb` (Exercise 2): use `nn.LSTM` in `ClassificationRNN`."
1113,8_rnn_2.ipynb,66,markdown,slide,9,"# Summary	1. Methods of gradient optimization: ""vanilla"", momentum, Nesterov momentum, Adagrad, RMSProp, Adam.	1. Conflicting updates in RNN	1. Long Short-Term Memory (LSTM)	- Treat the hidden state as ""memory"" and control its updates depending on the input.	- Cell state and cell update	- Output gate	- Input gate	- Forget gate"
1114,8_rnn_2.ipynb,67,markdown,slide,4,"# Recommended resources	- [Deep Learning Book, ch. 10: Sequence Modeling: Recurrentand Recursive Nets](https://www.deeplearningbook.org/contents/rnn.html)	- [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)	- [Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)"
1115,9_nlm.ipynb,0,code,-,25,"#@title <b><font color=""red"">▶</font><font color=""black""> run this cell to prepare supplementary materials for the lesson</font></b>		!rm -rf harbour-space-text-mining-course	!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git	import sys	if 'harbour-space-text-mining-course' not in sys.path:	sys.path.append('harbour-space-text-mining-course')		from tmcourse.utils import (	plot_confusion_matrix,	display_cv_results,	)	from tmcourse.ipyquiz import Quiz, Function	from tmcourse.utils import enable_mathjax_in_cell		from IPython.display import HTML, display		# get_ipython().events.register('pre_run_cell', enable_mathjax_in_cell)	from IPython.display import set_matplotlib_formats	%matplotlib inline	set_matplotlib_formats('svg')	import matplotlib.pyplot as plt		from tqdm.notebook import tqdm	!pip install allennlp"
1116,9_nlm.ipynb,1,markdown,slide,1,<center><h1>Neural Language Models</h1></center>
1117,9_nlm.ipynb,2,markdown,slide,10,# Outline	1. Discussion session	1. Contextual word meanings	1. RNN Language Models for word embeddings	- Stacked RNN	- Bidirectional RNN	- Embeddings from Language Models (ELMo)	1. RNN Language Models for classification	- Universal Language Model Fine-tuning (ULMfit)	1. Coding session.
1118,9_nlm.ipynb,3,markdown,slide,13,"# Discussion session	## How to present your project	1. Describe the real-world problem you are solving in the final project.	1. Describe the data.	1. Describe the stages of your solution.	1. Describe the problems you have encountered.	1. Tell us about the ideas you haven’t tried yet, or about the ideas that didn’t work.	1. Show the demo or share your ideas about it.		## How to discuss the presentation	1. Ask about relevant technical details.	1. If you know how to solve problems mentioned by the speaker, share your experience.	1. Suggest ideas for a mini-product."
1119,9_nlm.ipynb,4,markdown,slide,1,# Contextual word meanings
1120,9_nlm.ipynb,5,markdown,slide,5,"Consider two sentences	1. These people over here **stick** together like glue!	1. His hat and **stick** lay on the table.		The word ""stick"" has different meanings."
1121,9_nlm.ipynb,6,markdown,slide,2,"**Problem**: we cannot learn many meanings (vectors) yet.	> word2vec, GloVe, FastText return one vector for each word."
1122,9_nlm.ipynb,7,markdown,slide,3,"**What we want**:	- Read the sequence.	- For each word, find the meaning (vector) according to the information from the sequence."
1123,9_nlm.ipynb,8,markdown,slide,3,**What we will try**: Recurrent Neural Networks.	- RNN produces a vector (hidden state) for each input element.	- RNN can learn a language model.
1124,9_nlm.ipynb,9,markdown,slide,1,# RNN Language Models for word embeddings
1125,9_nlm.ipynb,10,markdown,slide,1,## Multilayer RNN
1126,9_nlm.ipynb,11,markdown,slide,4,So far we discussed RNNs with only one layer:	$$	h_t = \mathrm{SomeFunction}(W_{xh}x_t + W_{hh} h_{t-1})	$$
1127,9_nlm.ipynb,12,markdown,slide,3,"We know that ""vanilla"" neural networks benefit from adding more layers (XOR problem).		**What we want**: use many layers in recurrent neural networks."
1128,9_nlm.ipynb,13,markdown,slide,8,"Recall the definition of a recurrent function:	$$	h_t = f(x_t, h_{t-1})	$$	It produces the output	$$	o_t = g(h_t)	$$"
1129,9_nlm.ipynb,14,markdown,slide,10,"**Idea**: this output can be the input for another recurrent function.		$$	\hat{h}_t = \hat{f}(o_t, \hat{h}_{t-1})	$$	$$	\hat{o}_t = \hat{g}(\hat{h}_{t})	$$		**Intuition**: the first layer $(f, g)$ extracts features for the second layer $(\hat{f}, \hat{g})$."
1130,9_nlm.ipynb,15,markdown,slide,3,## Bidirectional RNN		
1131,9_nlm.ipynb,16,markdown,slide,4,"	We can use RNNs for language modeling.		Basically, language model is trained to predict the next word."
1132,9_nlm.ipynb,17,markdown,slide,3,"Consider the sequence: ""Students opened their ___"".		What is the next word: **exams** or **laptops**?"
1133,9_nlm.ipynb,18,markdown,slide,3,"But what if the full sentences are given?	1. ""Students opened their ___ as the proctor started the clock.""	1. ""Students opened their ___ and started coding."""
1134,9_nlm.ipynb,19,markdown,slide,1,"**What we want**: allow RNN to look at the ""future"" elements of a sequence."
1135,9_nlm.ipynb,20,markdown,slide,11,"**Idea**: read the sequence left-to-right and right-to-left.		Two RNNs:	$$	\overrightarrow{h}_t = \overrightarrow{f}(x_t, \overrightarrow{h}_{t-1})	$$	$$	\overleftarrow{h}_t = \overleftarrow{f}(x_t, \overleftarrow{h}_{\color{red}{t+1}})	$$		The result is the concatenation of $\overrightarrow{h}_t$ and $\overleftarrow{h}_t$"
1136,9_nlm.ipynb,21,markdown,slide,8,	<center>RNN</center>		![](https://camo.githubusercontent.com/771db850dc4b5aa8b32b4758ae68c137ee06eec4/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30392d4e4e2d54797065732d46502f696d672f524e4e2d67656e6572616c2e706e67)		<center>Bidirectional RNN</center>		![](https://camo.githubusercontent.com/52b9981d135220b278fbf7967f3d52e61fdd8c3e/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3736342f312a36516e505553765f74394259394676385f614c622d512e706e67)
1137,9_nlm.ipynb,22,code,slide,25,"# Source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py	# Bidirectional recurrent neural network (many-to-one)	import torch	from torch import nn	class BiRNN(nn.Module):	def __init__(self, input_size, hidden_size, num_layers, num_classes):	super(BiRNN, self).__init__()	self.hidden_size = hidden_size	self.num_layers = num_layers	self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)	self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection		def forward(self, x):	# Set initial states	# num_layers * 2 for bidirection	h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)	c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)		# Forward propagate LSTM	# out: tensor of shape (batch_size, seq_length, hidden_size*2)	out, _ = self.lstm(x, (h0, c0))		# Decode the hidden state of the last time step	out = self.fc(out[:, -1, :])	return out"
1138,9_nlm.ipynb,23,markdown,slide,3,# Embeddings from Language Models (ELMo)		ELMo is a 3-layer bidirectional LSTM language model.
1139,9_nlm.ipynb,24,markdown,slide,1,## Colab demo: `ElmoEmbedder` in `allennlp`
1140,9_nlm.ipynb,25,code,-,2,from allennlp.commands.elmo import ElmoEmbedder	elmo = ElmoEmbedder(cuda_device=0)
1141,9_nlm.ipynb,26,code,-,25,"import spacy	nlp = spacy.lang.en.English()	def tokens(s):	return [t.text for t in nlp(s)]		tokens1 = tokens(""These people over here stick together like glue!"")	tokens2 = tokens(""Stick with the fireman, Montag"")	tokens3 = tokens(""His hat and stick lay on the table."")		vectors1 = elmo.embed_sentence(tokens1)	vectors2 = elmo.embed_sentence(tokens2)	vectors3 = elmo.embed_sentence(tokens3)		print(f""Number of layers in the output: {len(vectors1)}"")	print(f""Number of vectors: {len(vectors1[0])}"")		from scipy.spatial.distance import cosine		# get embeddings for the word ""stick"" from the last layer	stick1_vector = vectors1[2][tokens1.index(""stick"")]	stick2_vector = vectors2[2][tokens2.index(""Stick"")]	stick3_vector = vectors3[2][tokens3.index(""stick"")]	print(""v1 vs v2:"", cosine(stick1_vector, stick2_vector))	print(""v1 vs v3:"", cosine(stick1_vector, stick3_vector))	print(""v2 vs v3:"", cosine(stick2_vector, stick3_vector))"
1142,9_nlm.ipynb,27,markdown,slide,1,# RNN Language Models for classification
1143,9_nlm.ipynb,28,markdown,slide,4,## Universal Language Model Fine-tuning (ULMfit)	1. Train LM on big general domain corpus (use biLM)	1. Tune LM on target task data	1. Fine-tune as classifier on target task
1144,9_nlm.ipynb,29,markdown,slide,4,	We already used pre-trained language models for classification!		[link](https://colab.research.google.com/drive/1RHx_zJmLblam3y49D9tQ-6RUm7SvHTZN#scrollTo=dWfiPlfCcZUD)
1145,9_nlm.ipynb,30,markdown,slide,1,## Colab demo: ULMfit in `fast.ai`
1146,9_nlm.ipynb,31,markdown,-,1,Get the data
1147,9_nlm.ipynb,32,code,-,20,"from sklearn.datasets import fetch_20newsgroups	fetch_params = dict(	shuffle=True,	random_state=1,	# categories=(""comp.graphics"", ""rec.sport.hockey""),	remove=('headers', 'footers', 'quotes')	)	dataset = fetch_20newsgroups(subset=""train"", **fetch_params)	dataset_test = fetch_20newsgroups(subset=""test"", **fetch_params)	texts_test = dataset_test.data	y_test = dataset_test.target		from sklearn.model_selection import train_test_split		# split data into training and validation set	texts_train, texts_val, y_train, y_val = train_test_split(	dataset.data, dataset.target,	test_size = 0.4,	random_state = 12	)"
1148,9_nlm.ipynb,33,markdown,-,1,Train logistic regression with sklearn
1149,9_nlm.ipynb,34,code,-,31,"from sklearn.feature_extraction.text import TfidfVectorizer	vec = TfidfVectorizer()	vec.fit(texts_train)	X_train = vec.transform(texts_train)	X_test = vec.transform(texts_test)		from sklearn.linear_model import SGDClassifier	clf = SGDClassifier(alpha=.0001, max_iter=50, loss=""log"", random_state=0)		from sklearn.pipeline import Pipeline		pipeline = Pipeline([	(""vec"", vec),	(""clf"", clf),	])		from sklearn.model_selection import GridSearchCV		param_grid = {	""vec__stop_words"": [None, ""english""],	""vec__ngram_range"": [(1, 1), (1, 2)],	""clf__alpha"": [1e-4, 1e-3, 1e-2],	}		pipeline_grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3)	pipeline_grid_search.fit(texts_train, y_train)		display_cv_results(pipeline_grid_search)		from sklearn.metrics import accuracy_score	print(accuracy_score(y_test, pipeline_grid_search.predict(texts_test)))"
1150,9_nlm.ipynb,35,markdown,-,1,Prepare data for fast.ai
1151,9_nlm.ipynb,36,code,-,9,"analyzer = pipeline_grid_search.best_estimator_.named_steps[""vec""].build_analyzer()		def preprocess(texts):	return ["" "".join(analyzer(t)) for t in texts]		import pandas as pd	df_train = pd.DataFrame({'label': y_train, 'text': preprocess(texts_train)})	df_val = pd.DataFrame({'label': y_val, 'text': preprocess(texts_val)})	df_test = pd.DataFrame({'label': y_test, 'text': preprocess(texts_test)})"
1152,9_nlm.ipynb,37,markdown,-,1,Imports from fast.ai
1153,9_nlm.ipynb,38,code,-,7,"from fastai.text import (	AWD_LSTM,  # pretrained LM	TextLMDataBunch,  # fine-tune LM	language_model_learner,  # fine-tune LM	TextClasDataBunch,  # classify based on LM	text_classifier_learner,  # classify based on LM	)"
1154,9_nlm.ipynb,39,markdown,-,2,"- **TextLMDataBunch** creates a data bunch for language modelling. In this, labels are completely ignored. Instead, data is processed so that the RNN can learn what word comes next given a starting word.	- **TextClasDataBunch** sets up the data for classification. Labels play a key role here. We can also set the batch size for learning by changing the bs parameter."
1155,9_nlm.ipynb,40,code,-,14,"# Language model data	data_lm = TextLMDataBunch.from_df(	train_df=df_train,	valid_df=df_val,	path = """"	)	# Classifier model data	data_clas = TextClasDataBunch.from_df(	train_df=df_train,	valid_df=df_val,	vocab=data_lm.train_ds.vocab,	bs=32,	path = """"	)"
1156,9_nlm.ipynb,41,markdown,-,1,Print a batch
1157,9_nlm.ipynb,42,code,-,1,data_clas.show_batch()
1158,9_nlm.ipynb,43,markdown,-,3,The `xx___` tags represent the aspects of language in a way that the computer can understand.	- The `xxbos` tag marks the beginning of a sentence.	- The `xxmaj` tag is used to imply that the first letter of the next word is capitalized.
1159,9_nlm.ipynb,44,code,-,15,"# train a language model	learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)	# find the optimal learning rate: https://arxiv.org/abs/1506.01186	learn.lr_find()	learn.recorder.plot(suggestion=True)	min_grad_lr = learn.recorder.min_grad_lr		# use the found learning rate to train the language model	learn.fit_one_cycle(2, min_grad_lr)	# unfreezing weights and training the rest of the NN	learn.unfreeze()	learn.fit_one_cycle(2, 1e-3)		# save encoder to load from the classifier	learn.save_encoder('ft_enc')"
1160,9_nlm.ipynb,45,markdown,-,1,Train the classifier
1161,9_nlm.ipynb,46,code,-,7,"# create the classifier	learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)	learn.load_encoder('ft_enc')	# find the optimal learning rate to start with	learn.lr_find()	learn.recorder.plot(suggestion=True)	min_grad_lr = learn.recorder.min_grad_lr"
1162,9_nlm.ipynb,47,markdown,-,1,"To train the classifier, we will use a technique called **gradual unfreezing**. We can start by training the last few layers, then go backwards and unfreeze and train layers before. We can use the learner function `learn.freeze_to(-2)` to unfreeze the last 2 layers."
1163,9_nlm.ipynb,48,code,-,2,"learn.fit_one_cycle(2, min_grad_lr)	learn.recorder.plot_losses()"
1164,9_nlm.ipynb,49,code,-,4,"# unfreeze the last 2 layers	learn.freeze_to(-2)	learn.fit_one_cycle(4, slice(5e-3, 2e-3), moms=(0.8,0.7))	learn.recorder.plot_losses()"
1165,9_nlm.ipynb,50,code,-,3,"# unfreeze all layers and train the model at a low learning rate.	learn.unfreeze()	learn.fit_one_cycle(4, slice(2e-3/100, 2e-3), moms=(0.8,0.7))"
1166,9_nlm.ipynb,51,code,-,2,"test_predictions = [learn.predict(t)[1].item() for t in tqdm(texts_test)]	accuracy_score(test_predictions, y_test)"
1167,9_nlm.ipynb,52,markdown,slide,1,# Coding session
1168,9_nlm.ipynb,53,markdown,-,5,"## Exercise 1		Implement the function `document_embedding(s, elmo)` which accepts a string `s` and return the average vector of word embeddings from the last layer of ELMo embeddings. `ElmoEmbedder` is passed as the second argument. Use spaCy tokenizer (the function `tokens` implemented above).		**NB**: for empty texts, return zero vector with 1024 elements."
1169,9_nlm.ipynb,54,code,-,3,"import numpy as np	def document_embedding(s, elmo):	# YOUR CODE HERE"
1170,9_nlm.ipynb,55,markdown,-,3,"## Exercise 2	Using the function `document_embedding()`, compute clusters in the testing data for 3 categories of 20newsgroup dataset (see code below).	Estimate clustering quality and plot confusion matrix."
1171,9_nlm.ipynb,56,code,-,29,"# function for clustering quality estimation	def estimate_clustering_quality(target, prediction):	from itertools import combinations	N = len(target)	total_pairs = 0	# we will count the ratio of ""good"" pairs of points	good_pairs = 0	for pair in combinations(range(N), 2):	# NB: O(n**2) complexity!	total_pairs += 1	i, j = pair	if (target[i] == target[j]) == (prediction[i] == prediction[j]):	good_pairs += 1		return good_pairs / total_pairs		# get the data: train and test data from 20newsgroups dataset	from sklearn.feature_extraction.text import TfidfVectorizer	from sklearn.datasets import fetch_20newsgroups	import numpy as np		categories = (""sci.space"", ""rec.sport.hockey"", ""comp.graphics"")	fetch_params = dict(	shuffle=True, random_state=1,	remove=('headers', 'footers', 'quotes'),	categories=categories	)	train_dataset = fetch_20newsgroups(subset=""train"", **fetch_params)	test_dataset = fetch_20newsgroups(subset=""test"", **fetch_params)"
1172,9_nlm.ipynb,57,markdown,-,1,"For your reference, this is how to find vectors and clusters with sklearn."
1173,9_nlm.ipynb,58,code,-,15,"vec = TfidfVectorizer(stop_words=""english"")	vec.fit(train_dataset.data)	X_train = vec.transform(train_dataset.data)	X_test = vec.transform(test_dataset.data)		# for this dataset we know the true number of classes	n_clusters = len(categories)		# KMeans has standard fit/predict interface	from sklearn.cluster import KMeans	kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=20)	kmeans.fit(X_train)	predictions = kmeans.predict(X_test)	plot_confusion_matrix(test_dataset.target, predictions)	estimate_clustering_quality(test_dataset.target, predictions)"
1174,9_nlm.ipynb,59,markdown,-,1,**Your code** below:
1175,9_nlm.ipynb,60,code,-,1,X_elmo = # YOUR CODE HERE: compute matrix of vectors on test_dataset
1176,9_nlm.ipynb,61,code,-,2,"# YOUR CODE HERE:	# fit KMeans on X_elmo, predict clusters, plot confusion matrix and estimate clustering quality"
1177,9_nlm.ipynb,62,markdown,-,3,"## Exercise 3		In your solution of Exercise 2 from the coding session `8_rnn_2.ipynb`, use bidirectional LSTM and compare the result with the previous implementation."
1178,9_nlm.ipynb,63,markdown,slide,7,# Summary	1. RNN Language Models for contextual word embeddings	- Stacked RNN	- Bidirectional RNN	- ELMo	1. RNN Language Models for classification	- Universal Language Model Fine-tuning (ULMfit)
1179,9_nlm.ipynb,64,markdown,slide,6,"# Recommended resources	- [CS224n Lecture 13: Contextual Word Representations	and Pretraining](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture13-contextual-representations.pdf)	- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)	- [Deep contextualized word representations. NAACL 2018.](https://arxiv.org/abs/1802.05365)	- [Using FastAI’s ULMFiT to make a state-of-the-art multi-class text classifier](https://medium.com/technonerds/using-fastais-ulmfit-to-make-a-state-of-the-art-multi-label-text-classifier-bf54e2943e83)"
